{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# CS-GY 9223-E: Deep Learning Homework 1\n",
    "Due on Sunday, 11th February 2018, 11:55 PM\n",
    "\n",
    "This homework can be done in pairs. Everyone must submit on NYU Classes individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Write down the UNIs (NetIDs) of your group (if applicable)\n",
    "\n",
    "Member 1: Name, NetID\n",
    "\n",
    "Member 2: Name, NetID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import glob\n",
    "import sys\n",
    "# you shouldn't need to make any more imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    Abstraction of neural network.\n",
    "    Stores parameters, activations, cached values. \n",
    "    Provides necessary functions for training and prediction. \n",
    "    \"\"\"\n",
    "    def __init__(self, layer_dimensions, drop_prob=0.0, reg_lambda=0.0):\n",
    "        \"\"\"\n",
    "        Initializes the weights and biases for each layer\n",
    "        :param layer_dimensions: (list) number of nodes in each layer\n",
    "        :param drop_prob: drop probability for dropout layers. Only required in part 2 of the assignment\n",
    "        :param reg_lambda: regularization parameter. Only required in part 2 of the assignment\n",
    "        \"\"\"\n",
    "#         np.random.seed(1)\n",
    "        \n",
    "        self.parameters = {}\n",
    "        self.num_layers = len(layer_dimensions)\n",
    "        self.drop_prob = drop_prob\n",
    "        self.reg_lambda = []\n",
    "        \n",
    "        # init parameters\n",
    "        for i in range(self.num_layers - 1):\n",
    "            d_in  = layer_dimensions[i]\n",
    "            d_out = layer_dimensions[i+1]\n",
    "            self.parameters[\"w\"+str(i)] = np.random.randn(d_in, d_out) * 2/(d_in+d_out)\n",
    "            self.parameters[\"b\"+str(i)] = np.random.randn(d_out)       * 1e-9\n",
    "        \n",
    "\n",
    "    def affineForward(self, A, W, b):\n",
    "        \"\"\"\n",
    "        Forward pass for the affine layer.\n",
    "        :param A: input matrix, shape (L, S), where L is the number of hidden units in the previous layer and S is\n",
    "        the number of samples\n",
    "        :returns: the affine product WA + b, along with the cache required for the backward pass\n",
    "        \"\"\"\n",
    "        lin = np.matmul(A.T, W)\n",
    "        print(\"\\t\", str(A.shape)+\".T\", \"â€¢\", W.shape, \"=\", lin.shape)\n",
    "        \n",
    "        aff = lin + b\n",
    "        print(\"\\t(\", lin.shape, \"+\", b.shape, \").T =\", aff.T.shape)\n",
    "\n",
    "        return aff.T\n",
    "    \n",
    "\n",
    "    def activationForward(self, A, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Common interface to access all activation functions.\n",
    "        :param A: input to the activation function\n",
    "        :param prob: activation funciton to apply to A. Just \"relu\" for this assignment.\n",
    "        :returns: activation(A)\n",
    "        \"\"\" \n",
    "        return getattr(self,activation)(A)\n",
    "\n",
    "\n",
    "    def relu(self, X):\n",
    "        return np.maximum(X, 0)\n",
    "    \n",
    "    def tanh(self, X):\n",
    "        return (np.exp(X)-np.exp(-X)) / (np.exp(X)+np.exp(-X))\n",
    "    \n",
    "    def dropout(self, A, prob):\n",
    "        \"\"\"\n",
    "        :param A: Activation\n",
    "        :param prob: drop prob\n",
    "        :returns: tuple (A, M) \n",
    "            WHERE\n",
    "            A is matrix after applying dropout\n",
    "            M is dropout mask, used in the backward pass\n",
    "        \"\"\"\n",
    "        \n",
    "        return A, M\n",
    "\n",
    "    def forwardPropagation(self, X):\n",
    "        \"\"\"\n",
    "        Runs an input X through the neural network to compute activations\n",
    "        for all layers. Returns the output computed at the last layer along\n",
    "        with the cache required for backpropagation.\n",
    "        :returns: (tuple) AL, cache\n",
    "            WHERE \n",
    "            AL is activation of last layer\n",
    "            cache is cached values for each layer that\n",
    "                     are needed in further steps\n",
    "        \"\"\"\n",
    "        \n",
    "        cache = []\n",
    "        A = X #A0 is the input\n",
    "        for i in range(self.num_layers-1):\n",
    "            print(\"compute:\", i)\n",
    "            W = self.parameters[\"w\"+str(i)]\n",
    "            b = self.parameters[\"b\"+str(i)]\n",
    "            Z = self.affineForward(A, W, b)\n",
    "            A = self.activationForward(Z, 'relu')\n",
    "            cache.append(A)\n",
    "            \n",
    "        AL = A # maybe this should be softmax(A)?\n",
    "        \n",
    "        return AL, cache\n",
    "    \n",
    "    def costFunction(self, AL, y):\n",
    "        \"\"\"\n",
    "        :param AL: Activation of last layer, shape (num_classes, S)\n",
    "        :param y: labels, shape (S)\n",
    "        :param alpha: regularization parameter\n",
    "        :returns cost, dAL: A scalar denoting cost and the gradient of cost\n",
    "        \"\"\"\n",
    "        # compute loss\n",
    "        print(\"y.sh, AL.sh\", y.shape, AL.shape)\n",
    "#         print(AL)\n",
    "\n",
    "        Y = np.zeros_like(AL)\n",
    "        print(\"YYY:\", Y.shape, y.shape, len(y))\n",
    "        Y[y, np.arange(len(y))] = 1\n",
    "        print(Y)\n",
    "        \n",
    "        y_pred = np.exp(AL) / np.sum(np.exp(AL), axis=0)\n",
    "\n",
    "        cost = np.sum(Y * -np.log(y_pred)) / len(y)\n",
    "#         cost = np.dot(AL, y)\n",
    "                                   # if AL is un-softmaxed we can interpret AL as \n",
    "                                   # -log(P(y | x))\n",
    "\n",
    "#         p = np.exp(AL)/np.sum(np.exp(AL))\n",
    "#         cost = np.sum(-y * np.log(p))\n",
    "        \n",
    "#         if self.reg_lambda > 0:\n",
    "#             # add regularization\n",
    "#             cost += self.reg_lambda * np.sum(paramiters ** 2)\n",
    "            \n",
    "        # gradient of cost\n",
    "        dAL = 0\n",
    "        return cost, dAL\n",
    "\n",
    "    def affineBackward(self, dA_prev, cache):\n",
    "        \"\"\"\n",
    "        Backward pass for the affine layer.\n",
    "        :param dA_prev: gradient from the next layer.\n",
    "        :param cache: cache returned in affineForward\n",
    "        :returns dA: gradient on the input to this layer\n",
    "                 dW: gradient on the weights\n",
    "                 db: gradient on the bias\n",
    "        \"\"\"\n",
    "\n",
    "        return dA, dW, db\n",
    "\n",
    "    def activationBackward(self, dA, cache, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Interface to call backward on activation functions.\n",
    "        In this case, it's just relu. \n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "        \n",
    "    def relu_derivative(self, dx, cached_x):\n",
    "\n",
    "        return dx\n",
    "\n",
    "    def dropout_backward(self, dA, cache):\n",
    "\n",
    "        return dA\n",
    "\n",
    "    def backPropagation(self, dAL, Y, cache):\n",
    "        \"\"\"\n",
    "        Run backpropagation to compute gradients on all paramters in the model\n",
    "        :param dAL: gradient on the last layer of the network. Returned by the cost function.\n",
    "        :param Y: labels\n",
    "        :param cache: cached values during forwardprop\n",
    "        :returns gradients: dW and db for each weight/bias\n",
    "        \"\"\"\n",
    "        gradients = {}\n",
    "        \n",
    "#         for i in range(0): # fill in\n",
    "            \n",
    "            \n",
    "#             if self.drop_prob > 0:\n",
    "                #call dropout_backward\n",
    "           \n",
    "            \n",
    "#         if self.reg_lambda > 0:\n",
    "            # add gradients from L2 regularization to each dW\n",
    "        \n",
    "        return gradients\n",
    "\n",
    "\n",
    "    def updateParameters(self, gradients, alpha):\n",
    "        \"\"\"\n",
    "        :param gradients: gradients for each weight/bias\n",
    "        :param alpha: step size for gradient descent \n",
    "        \"\"\"\n",
    "\n",
    "    def train(self, X, y, iters=1000, alpha=0.0001, batch_size=100, print_every=100):\n",
    "        \"\"\"\n",
    "        :param X: input samples, each column is a sample\n",
    "        :param y: labels for input samples, y.shape[0] must equal X.shape[1]\n",
    "        :param iters: number of training iterations\n",
    "        :param alpha: step size for gradient descent\n",
    "        :param batch_size: number of samples in a minibatch\n",
    "        :param print_every: no. of iterations to print debug info after\n",
    "        \"\"\"\n",
    "        \n",
    "#         for i in range(0, iters):\n",
    "            # get minibatch\n",
    "            \n",
    "            # forward prop\n",
    "\n",
    "            # compute loss\n",
    "\n",
    "            # compute gradients\n",
    "\n",
    "            # update weights and biases based on gradient\n",
    "\n",
    "#             if i % print_every == 0:\n",
    "                # print cost, train and validation set accuracies\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions for each sample\n",
    "        \"\"\"\n",
    "        AL, cache = NN.forwardPropagation(X)\n",
    "        \n",
    "        y_pred = np.exp(AL) / np.sum(np.exp(AL), axis=0)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def get_batch(self, X, y, batch_size):\n",
    "        \"\"\"\n",
    "        Return minibatch of samples and labels\n",
    "        \n",
    "        :param X, y: samples and corresponding labels\n",
    "        :parma batch_size: minibatch size\n",
    "        :returns: (tuple) X_batch, y_batch\n",
    "        \"\"\"\n",
    "\n",
    "        return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 0]\n",
      "[[ 1.  1.  1.  0.]\n",
      " [ 1.  1.  1.  0.]\n",
      " [ 1.  1.  1.  0.]]\n",
      "[6 1 6 6 8 8 3 4 6 0]\n",
      "[[ 1.  1.  0.  1.  1.  0.  1.  0.  1.  0.]\n",
      " [ 1.  1.  0.  1.  1.  0.  1.  0.  1.  0.]\n",
      " [ 1.  1.  0.  1.  1.  0.  1.  0.  1.  0.]\n",
      " [ 1.  1.  0.  1.  1.  0.  1.  0.  1.  0.]\n",
      " [ 1.  1.  0.  1.  1.  0.  1.  0.  1.  0.]\n",
      " [ 1.  1.  0.  1.  1.  0.  1.  0.  1.  0.]\n",
      " [ 1.  1.  0.  1.  1.  0.  1.  0.  1.  0.]\n",
      " [ 1.  1.  0.  1.  1.  0.  1.  0.  1.  0.]\n",
      " [ 1.  1.  0.  1.  1.  0.  1.  0.  1.  0.]\n",
      " [ 1.  1.  0.  1.  1.  0.  1.  0.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "Z = np.zeros((3,3))\n",
    "z = np.array([0, 1, 2, 0])\n",
    "# Z[z, :] = 1\n",
    "# Z[np.array([0, 0])]\n",
    "\n",
    "print(z)\n",
    "print(one_hot(z, 3))\n",
    "\n",
    "print(y_train[:10])\n",
    "print(one_hot(y_train[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### run some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['w0', (3072, 100)], ['b0', (100,)], ['w1', (100, 10)], ['b1', (10,)]]\n",
      "\t (2, 2).T â€¢ (2, 2) = (2, 2)\n",
      "\t( (2, 2) + (2,) ).T = (2, 2)\n",
      "[[4 2]\n",
      " [1 2]]\n",
      "(2,)\n",
      "\t (2,).T â€¢ (2, 2) = (2,)\n",
      "\t( (2,) + (2,) ).T = (2,)\n",
      "[6 3]\n",
      "\n",
      " [ 0.83456329 -0.77637467 -0.97012434 -0.92902622 -0.90605198]\n",
      "[ 0.83456329  0.          0.          0.          0.        ]\n",
      "[ 0.83456329  0.          0.          0.          0.        ]\n",
      "\n",
      "\n",
      "compute: 0\n",
      "\t (3072, 10000).T â€¢ (3072, 100) = (10000, 100)\n",
      "\t( (10000, 100) + (100,) ).T = (100, 10000)\n",
      "compute: 1\n",
      "\t (100, 10000).T â€¢ (100, 10) = (10000, 10)\n",
      "\t( (10000, 10) + (10,) ).T = (10, 10000)\n",
      "AL: (10, 10000)\n",
      "\n",
      "\n",
      "\n",
      "y.sh, AL.sh (10000,) (10, 10000)\n",
      "YYY: (10, 10000) (10000,) 10000\n",
      "[[ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "cost: ( 2.30258425528 ?= 2.3 )  ( 0.100000083771 ?= 0.1 )\n",
      "compute: 0\n",
      "\t (3072, 10000).T â€¢ (3072, 100) = (10000, 100)\n",
      "\t( (10000, 100) + (100,) ).T = (100, 10000)\n",
      "compute: 1\n",
      "\t (100, 10000).T â€¢ (100, 10) = (10000, 10)\n",
      "\t( (10000, 10) + (10,) ).T = (10, 10000)\n",
      "\n",
      "\n",
      "\n",
      "predict: \n",
      " (10, 10000)\n",
      "sums: \n",
      " (10000,)\n",
      "1.0\n",
      "acc:  0.101\n",
      "\n",
      "\n",
      "\n",
      " [[ 0.10001331  0.1000929   0.10013067  0.10000738  0.09998673]\n",
      " [ 0.09994311  0.0999427   0.0999565   0.09994207  0.09994396]\n",
      " [ 0.09998551  0.10002182  0.0999565   0.10003127  0.09994396]\n",
      " [ 0.10014333  0.0999586   0.10016387  0.10013816  0.10024679]\n",
      " [ 0.10006546  0.09997051  0.09997063  0.09994207  0.10004057]\n",
      " [ 0.09994311  0.0999427   0.0999565   0.09994207  0.09994396]\n",
      " [ 0.09994311  0.0999427   0.09999582  0.09994207  0.09996343]\n",
      " [ 0.09994311  0.0999427   0.0999565   0.09994207  0.09994396]\n",
      " [ 0.09994311  0.10012887  0.0999565   0.09994207  0.09994396]\n",
      " [ 0.10007683  0.1000565   0.0999565   0.10017077  0.10004265]\n",
      " [ 1.          1.          1.          1.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "layer_dimensions = [X_train.shape[0], 100, 10]  # including the input and output layers\n",
    "NN = NeuralNetwork(layer_dimensions)\n",
    "NN.train(X_train, y_train, iters=10, alpha=0, batch_size=128, print_every=10)\n",
    "\n",
    "# print([k for k in NN.parameters.keys()])\n",
    "# print([v.shape for v in NN.parameters.values()])\n",
    "\n",
    "print([[k, v.shape] for k,v in NN.parameters.items()])\n",
    "\n",
    "A = np.array([[1, 0], [0, 1]])\n",
    "W = np.array([[4, 1], \n",
    "     [2, 2]])\n",
    "print(NN.affineForward(A, W, np.array([0,0]).T))\n",
    "A = np.array([1,1])\n",
    "print(A.shape)\n",
    "print(NN.affineForward(A, W, np.array([0,0])))\n",
    "\n",
    "z = np.random.randn(5)\n",
    "print(\"\\n\",z)\n",
    "print(NN.relu(z))\n",
    "print(NN.activationForward(z))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_train[:, 0].shape)\n",
    "# print(X_train[1].shape)\n",
    "\n",
    "\n",
    "\n",
    "AL, cache = NN.forwardPropagation(X_train[:, :10000])\n",
    "print(\"AL:\", AL.shape)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "cost, dAL = NN.costFunction(AL, y_train[:10000])\n",
    "print(\"cost: (\", cost, \"?=\", 2.3, \")  (\", np.exp(-cost), \"?=\", 0.1,\")\") # cost \\approx    2.3  \\approx    -log(1/10)     => :-)\n",
    "\n",
    "\n",
    "y_hat = NN.predict(X_train[:, :10000])\n",
    "sums = np.sum(y_hat, axis=0)\n",
    "print(\"\\n\\n\")\n",
    "print(\"predict: \\n\", y_hat.shape)\n",
    "print(\"sums: \\n\", sums.shape)\n",
    "print(np.mean(sums))\n",
    "print(\"acc: \", np.mean(np.argmax(y_hat, axis=0) == y_train[:10000]))\n",
    "\n",
    "print(\"\\n\\n\\n\", np.row_stack( (y_hat[:, :5], np.sum( y_hat[:, :5], axis=0)) ) )  # last row = 1  => :-)\n",
    "# print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0984  0.0995  0.1013  0.0997  0.1016  0.0982  0.1021  0.1017  0.0989\n",
      "  0.0986]\n",
      "\n",
      "[ 2.31871447  2.30759763  2.28966887  2.3055896   2.28671174  2.32074906\n",
      "  2.28180255  2.28572798  2.31364604  2.31668402]\n",
      "\n",
      "2.30268919744\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAyBJnTIAktQpAyBJnfo/ccty9qDdvE8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ecb55c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train[:10000]\n",
    "y_train[:10]\n",
    "\n",
    "plt.hist(y_train[:10000])\n",
    "\n",
    "counts = np.array([  984.,   995.,  1013.,   997.,  1016.,   982.,  1021.,  1017.,\n",
    "          989.,   986.])\n",
    "\n",
    "counts /= 10000\n",
    "\n",
    "print(counts)\n",
    "print()\n",
    "\n",
    "counts = -np.log(counts)\n",
    "print(counts)\n",
    "print()\n",
    "\n",
    "print(np.mean(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "y_ [ 0.09994044  0.10014091  0.09999489  0.09994044  0.09998973  0.09994044\n",
      "  0.09994044  0.09998702  0.10018524  0.09994044]\n",
      "8\n",
      "0.100185240185\n",
      "p: 8\n",
      "mean: [ 0.09992921  0.10025684  0.09993354  0.09990395  0.09999145  0.099929\n",
      "  0.09991847  0.09997355  0.10026267  0.09990132]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkq8mAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e49f780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_hat.shape\n",
    "p = np.argmax(y_hat, axis = 0)\n",
    "print(p.shape)\n",
    "print(\"y_\", y_hat[:, 1000])\n",
    "\n",
    "print(np.argmax(y_hat[:, 1000]))\n",
    "print(np.max(y_hat[:, 1000]))\n",
    "\n",
    "print(\"p:\", p[1000])\n",
    "plt.hist(p)\n",
    "\n",
    "print(\"mean:\", np.mean(y_hat, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[200]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.30258509299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10000000000000002"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = -np.log(.1)\n",
    "print(c)\n",
    "np.exp(-c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Helper functions, DO NOT modify this\n",
    "\n",
    "def get_img_array(path):\n",
    "    \"\"\"\n",
    "    Given path of image, returns it's numpy array\n",
    "    \"\"\"\n",
    "    return scipy.misc.imread(path)\n",
    "\n",
    "def get_files(folder):\n",
    "    \"\"\"\n",
    "    Given path to folder, returns list of files in it\n",
    "    \"\"\"\n",
    "    filenames = [file for file in glob.glob(folder+'*/*')]\n",
    "    filenames.sort()\n",
    "    return filenames\n",
    "\n",
    "def get_label(filepath, label2id):\n",
    "    \"\"\"\n",
    "    Files are assumed to be labeled as: /path/to/file/999_frog.png\n",
    "    Returns label for a filepath\n",
    "    \"\"\"\n",
    "    tokens = filepath.split('/')\n",
    "    label = tokens[-1].split('_')[1][:-4]\n",
    "    if label in label2id:\n",
    "        return label2id[label]\n",
    "    else:\n",
    "        sys.exit(\"Invalid label: \" + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Functions to load data, DO NOT change these\n",
    "\n",
    "def get_labels(folder, label2id):\n",
    "    \"\"\"\n",
    "    Returns vector of labels extracted from filenames of all files in folder\n",
    "    :param folder: path to data folder\n",
    "    :param label2id: mapping of text labels to numeric ids. (Eg: automobile -> 0)\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    y = []\n",
    "    for f in files:\n",
    "        y.append(get_label(f,label2id))\n",
    "    return np.array(y)\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts each label index in y to vector with one_hot encoding\n",
    "    One-hot encoding converts categorical labels to binary values\n",
    "    \"\"\"\n",
    "    y_one_hot = np.zeros((y.shape[0], num_classes))\n",
    "    y_one_hot[y] = 1\n",
    "    return y_one_hot.T\n",
    "\n",
    "def get_label_mapping(label_file):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        id2label = f.readlines()\n",
    "        id2label = [l.strip() for l in id2label]\n",
    "    label2id = {}\n",
    "    count = 0\n",
    "    for label in id2label:\n",
    "        label2id[label] = count\n",
    "        count += 1\n",
    "    return id2label, label2id\n",
    "\n",
    "def get_images(folder):\n",
    "    \"\"\"\n",
    "    returns numpy array of all samples in folder\n",
    "    each column is a sample resized to 30x30 and flattened\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    images = []\n",
    "    count = 0\n",
    "    \n",
    "    for f in files:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "        img_arr = get_img_array(f)\n",
    "        img_arr = img_arr.flatten() / 255.0\n",
    "        images.append(img_arr)\n",
    "    X = np.column_stack(images)\n",
    "\n",
    "    return X\n",
    "\n",
    "def get_train_data(data_root_path):\n",
    "    \"\"\"\n",
    "    Return X and y\n",
    "    \"\"\"\n",
    "    train_data_path = data_root_path + 'train'\n",
    "    id2label, label2id = get_label_mapping(data_root_path+'labels.txt')\n",
    "    print(label2id)\n",
    "    X = get_images(train_data_path)\n",
    "    y = get_labels(train_data_path, label2id)\n",
    "    return X, y\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    \"\"\"\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel/__main__.py:7: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000/50000\n",
      "Loaded 20000/50000\n",
      "Loaded 30000/50000\n",
      "Loaded 40000/50000\n",
      "Loaded 50000/50000\n",
      "Loaded 10000/10000\n",
      "Data loading done\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_root_path = 'cifar10-hw1/'\n",
    "X_train, y_train = get_train_data(data_root_path) # this may take a few minutes\n",
    "X_test = get_images(data_root_path + 'test')\n",
    "print('Data loading done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,)\n",
      "(5, 1)\n",
      "(5, 10)\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "[[6]\n",
      " [1]\n",
      " [6]\n",
      " [6]\n",
      " [8]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 6 is out of bounds for axis 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-481-3cc008564ade>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# y_train[:10]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-444-d8e11617571d>\u001b[0m in \u001b[0;36mone_hot\u001b[0;34m(y, num_classes)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \"\"\"\n\u001b[1;32m     20\u001b[0m     \u001b[0my_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0my_one_hot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my_one_hot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 6 is out of bounds for axis 0 with size 5"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_train[None,:5].T.shape)\n",
    "\n",
    "Y = np.zeros((y_train[None,:5].T.shape[0], 10))\n",
    "print(Y.shape)\n",
    "print(Y)\n",
    "print(y_train[None, :5].T)\n",
    "Y[np.arange(5), y_train[:5]] = 1\n",
    "print(Y)\n",
    "\n",
    "\n",
    "print(one_hot(y_train[None,:5].T))\n",
    "\n",
    "# y_train[:10]\n",
    "# one_hot(y_train[:4])\n",
    "# one_hot(y_train[:4], 10)\n",
    "# y_train[4]\n",
    "# one_hot(np.array([4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3  4  5  6]\n",
      " [ 7  8  9 10 11 12 13]\n",
      " [14 15 16 17 18 19 20]\n",
      " [21 22 23 24 25 26 27]\n",
      " [28 29 30 31 32 33 34]]\n",
      "[ 0  7 14 21 28]\n"
     ]
    }
   ],
   "source": [
    "y = np.arange(35).reshape(5,7)\n",
    "print(y)\n",
    "print(y[np.arange(5), 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Split train into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-418-9e7eb40c18ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_val' is not defined"
     ]
    }
   ],
   "source": [
    "X_train\n",
    "y_train\n",
    "\n",
    "X_val\n",
    "y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### display some samples from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 1\n",
    "\n",
    "#### Simple fully-connected deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layer_dimensions = [X_train.shape[0], 1, 10]  # including the input and output layers\n",
    "NN = NeuralNetwork(layer_dimensions)\n",
    "NN.train(X_train, y_train, iters=10, alpha=0, batch_size=128, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-d344c99731de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msave_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ans1-uni'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-c1d23d2687ea>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "y_predicted = NN.predict(X_test)\n",
    "save_predictions('ans1-uni', y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# test if your numpy file has been saved correctly\n",
    "loaded_y = np.load('ans1-uni.npy')\n",
    "print(loaded_y.shape)\n",
    "loaded_y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 2: Regularizing the neural network\n",
    "#### Add dropout and L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "NN2 = NeuralNetwork(layer_dimensions, drop_prob=0, reg_lambda=0)\n",
    "NN2.train(X_train, y_train, iters=1000, alpha=0.00001, batch_size=1000, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_predicted2 = NN2.predict(X)\n",
    "save_predictions(y_predicted, 'ans2-uni')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

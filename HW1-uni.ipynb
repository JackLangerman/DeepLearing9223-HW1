{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# CS-GY 9223-E: Deep Learning Homework 1\n",
    "Due on Sunday, 11th February 2018, 11:55 PM\n",
    "\n",
    "This homework can be done in pairs. Everyone must submit on NYU Classes individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Write down the UNIs (NetIDs) of your group (if applicable)\n",
    "\n",
    "Member 1: Name, NetID\n",
    "\n",
    "Member 2: Name, NetID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import glob\n",
    "import sys\n",
    "# you shouldn't need to make any more imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    Abstraction of neural network.\n",
    "    Stores parameters, activations, cached values. \n",
    "    Provides necessary functions for training and prediction. \n",
    "    \"\"\"\n",
    "    def __init__(self, layer_dimensions, drop_prob=0.0, reg_lambda=0.0):\n",
    "        \"\"\"\n",
    "        Initializes the weights and biases for each layer\n",
    "        :param layer_dimensions: (list) number of nodes in each layer\n",
    "        :param drop_prob: drop probability for dropout layers. Only required in part 2 of the assignment\n",
    "        :param reg_lambda: regularization parameter. Only required in part 2 of the assignment\n",
    "        \"\"\"\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        self.parameters = {}\n",
    "        self.num_layers = len(layer_dimensions)\n",
    "        self.drop_prob = drop_prob\n",
    "        self.reg_lambda = reg_lambda\n",
    "        \n",
    "        # init parameters\n",
    "        for i in range(self.num_layers - 1):\n",
    "            d_in  = layer_dimensions[i]\n",
    "            d_out = layer_dimensions[i+1]\n",
    "            self.parameters[\"layer\"+str(i)] = ( (np.random.randn(d_out, d_in) * np.sqrt(2/(d_in+d_out))), np.zeros(d_out) )\n",
    "#             self.parameters[\"b\"+str(i)] = np.zeros(d_out)\n",
    "        \n",
    "\n",
    "    def affineForward(self, A, W, b):\n",
    "        \"\"\"\n",
    "        Forward pass for the affine layer.\n",
    "        :param A: input matrix, shape (L, S), where L is the number of hidden units in the previous layer and S is\n",
    "        the number of samples\n",
    "        :returns: the affine product WA + b, along with the cache required for the backward pass\n",
    "        \"\"\"\n",
    "#         lin = np.matmul(A.T, W)\n",
    "#         print(\"\\t\", str(A.shape)+\".T\", \"•\", W.shape, \"=\", lin.shape)\n",
    "        \n",
    "\n",
    "#         return aff.T\n",
    "        \n",
    "        lin = np.matmul(W, A)\n",
    "        print(\"\\t\", str(W.shape), \"•\", A.shape, \"=\", lin.shape)\n",
    "        \n",
    "        aff = lin + b[:, None]\n",
    "        print(\"\\t(\", lin.shape, \"+\", b[:, None].shape, \").T =\", aff.T.shape)\n",
    "\n",
    "        return aff, A  # A is returned as the cache\n",
    "    \n",
    "\n",
    "    def activationForward(self, A, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Common interface to access all activation functions.\n",
    "        :param A: input to the activation function\n",
    "        :param prob: activation funciton to apply to A. Just \"relu\" for this assignment.\n",
    "        :returns: activation(A)\n",
    "        \"\"\" \n",
    "        return getattr(self,activation)(A)\n",
    "\n",
    "\n",
    "    def relu(self, X):\n",
    "        return np.maximum(X, 0)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def softmax(self, X):\n",
    "        ex = np.exp(X - np.max(X, axis=0))\n",
    "        return ex / np.sum(ex, axis=0)\n",
    "    \n",
    "#     def softmax_derivative(self, dx, cached_x):\n",
    "#         sm = softmax(cached_x)\n",
    "#         sigma_prime = sm * (np.ones_like(cached_x) - sm)\n",
    "#         return dx * sigma_prime\n",
    "        \n",
    "    def dropout(self, A, prob):\n",
    "        \"\"\"\n",
    "        :param A: Activation\n",
    "        :param prob: drop prob\n",
    "        :returns: tuple (A, M) \n",
    "            WHERE\n",
    "            A is matrix after applying dropout\n",
    "            M is dropout mask, used in the backward pass\n",
    "        \"\"\"\n",
    "        \n",
    "        M = np.random.rand(A.shape[0], A.shape[1])\n",
    "        M = (M > prob) * 1.0\n",
    "        scale_up_prob = 1 / (1 - prob)\n",
    "        M *= scale_up_prob\n",
    "        A *= M\n",
    "        \n",
    "        return A, M\n",
    "\n",
    "    def forwardPropagation(self, X):\n",
    "        \"\"\"\n",
    "        Runs an input X through the neural network to compute activations\n",
    "        for all layers. Returns the output computed at the last layer along\n",
    "        with the cache required for backpropagation.\n",
    "        :returns: (tuple) AL, cache\n",
    "            WHERE \n",
    "            AL is activation of last layer\n",
    "            cache is cached values for each layer that\n",
    "                     are needed in further steps\n",
    "        \"\"\"\n",
    "        \n",
    "        cache = []\n",
    "        A = X #A0 is the input\n",
    "        for i in range(self.num_layers-1):\n",
    "            print(\"compute:\", i)\n",
    "            cache.append(A)\n",
    "            \n",
    "#             W = self.parameters[\"w\"+str(i)]\n",
    "#             b = self.parameters[\"b\"+str(i)]\n",
    "            W, b = self.parameters[\"layer\"+str(i)]\n",
    "            Z, _ = self.affineForward(A, W, b)\n",
    "            A = self.activationForward(Z, 'relu')\n",
    "            \n",
    "            \n",
    "        AL = self.softmax(A)\n",
    "        \n",
    "        return AL, cache\n",
    "    \n",
    "    def costFunction(self, AL, y):\n",
    "        \"\"\"\n",
    "        :param AL: Activation of last layer, shape (num_classes, S)\n",
    "        :param y: labels, shape (S)\n",
    "        :param alpha: regularization parameter\n",
    "        :returns cost, dAL: A scalar denoting cost and the gradient of cost\n",
    "        \"\"\"\n",
    "        # compute loss\n",
    "        m = y.shape[0]\n",
    "        AL_softmax = self.softmax(AL)\n",
    "        correct_label_prob = AL_softmax[y, range(m)]\n",
    "        cost = -np.sum(np.log(correct_label_prob)) / m\n",
    "        \n",
    "#         if self.reg_lambda > 0:\n",
    "#             # add regularization\n",
    "#             cost += self.reg_lambda * np.sum(paramiters ** 2)\n",
    "            \n",
    "        # gradient of cost\n",
    "#         dAL = np.sum(-Y / y_pred) / len(y)\n",
    "        \n",
    "        return cost, None\n",
    "\n",
    "    def affineBackward(self, dA_prev, cache):\n",
    "        \"\"\"\n",
    "        Backward pass for the affine layer.\n",
    "        :param dA_prev: gradient from the next layer.\n",
    "        :param cache: cache returned in affineForward\n",
    "        :returns dA: gradient on the input to this layer\n",
    "                 dW: gradient on the weights\n",
    "                 db: gradient on the bias\n",
    "        \"\"\"\n",
    "        \n",
    "        dA = np.sum(A.T, axis=0)\n",
    "        \n",
    "        return dA, dW, db\n",
    "\n",
    "    def activationBackward(self, dA, cache, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Interface to call backward on activation functions.\n",
    "        In this case, it's just relu. \n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "        \n",
    "    def relu_derivative(self, dx, cached_x):\n",
    "        sigma_prime = np.zeros_like(dx)\n",
    "        sigma_prime[ cached_x > 0] = 1\n",
    "        \n",
    "        return dx * sigma_prime\n",
    "\n",
    "    def dropout_backward(self, dA, cache):\n",
    "\n",
    "        return dA\n",
    "\n",
    "    def backPropagation(self, dAL, Y, cache):\n",
    "        \"\"\"\n",
    "        Run backpropagation to compute gradients on all paramters in the model\n",
    "        :param dAL: gradient on the last layer of the network. Returned by the cost function.\n",
    "        :param Y: labels\n",
    "        :param cache: cached values during forwardprop\n",
    "        :returns gradients: dW and db for each weight/bias\n",
    "        \"\"\"\n",
    "        gradients = {}\n",
    "        \n",
    "#         for i in range(0): # fill in\n",
    "            \n",
    "            \n",
    "#             if self.drop_prob > 0:\n",
    "                #call dropout_backward\n",
    "           \n",
    "            \n",
    "#         if self.reg_lambda > 0:\n",
    "            # add gradients from L2 regularization to each dW\n",
    "        \n",
    "        return gradients\n",
    "\n",
    "\n",
    "    def updateParameters(self, gradients, alpha):\n",
    "        \"\"\"\n",
    "        :param gradients: gradients for each weight/bias\n",
    "        :param alpha: step size for gradient descent \n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "    def train(self, X, y, iters=1000, alpha=0.0001, batch_size=100, print_every=100):\n",
    "        \"\"\"\n",
    "        :param X: input samples, each column is a sample\n",
    "        :param y: labels for input samples, y.shape[0] must equal X.shape[1]\n",
    "        :param iters: number of training iterations\n",
    "        :param alpha: step size for gradient descent\n",
    "        :param batch_size: number of samples in a minibatch\n",
    "        :param print_every: no. of iterations to print debug info after\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(0, iters):\n",
    "            # get minibatch\n",
    "#                 X_mb, y_mb = get_batch(X, y, batch_size)\n",
    "            # forward prop\n",
    "            X_forward, cache = self.forwardPropagation(X)\n",
    "            # compute loss\n",
    "            cost, dAL = self.costFunction(X_forward, y)\n",
    "            # compute gradients\n",
    "            gradients = self.backPropagation(dAL, y, cache)    \n",
    "            # update weights and biases based on gradient\n",
    "            self.updateParameters(gradients, alpha)\n",
    "            \n",
    "            if i % print_every == 0:\n",
    "                # print cost, train and validation set accuracies\n",
    "                print(\"should print cost\")\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions for each sample\n",
    "        \"\"\"\n",
    "        AL, cache = NN.forwardPropagation(X)\n",
    "        \n",
    "        y_pred = self.softmax(AL)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def get_batch(self, X, y, batch_size):\n",
    "        \"\"\"\n",
    "        Return minibatch of samples and labels\n",
    "        \n",
    "        :param X, y: samples and corresponding labels\n",
    "        :parma batch_size: minibatch size\n",
    "        :returns: (tuple) X_batch, y_batch\n",
    "        \"\"\"\n",
    "\n",
    "        return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Functions to load data, DO NOT change these\n",
    "\n",
    "def get_labels(folder, label2id):\n",
    "    \"\"\"\n",
    "    Returns vector of labels extracted from filenames of all files in folder\n",
    "    :param folder: path to data folder\n",
    "    :param label2id: mapping of text labels to numeric ids. (Eg: automobile -> 0)\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    y = []\n",
    "    for f in files:\n",
    "        y.append(get_label(f,label2id))\n",
    "    return np.array(y)\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts each label index in y to vector with one_hot encoding\n",
    "    One-hot encoding converts categorical labels to binary values\n",
    "    \"\"\"\n",
    "    y_one_hot = np.zeros((y.shape[0], num_classes))\n",
    "    y_one_hot[y] = 1\n",
    "    return y_one_hot.T\n",
    "\n",
    "def get_label_mapping(label_file):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        id2label = f.readlines()\n",
    "        id2label = [l.strip() for l in id2label]\n",
    "    label2id = {}\n",
    "    count = 0\n",
    "    for label in id2label:\n",
    "        label2id[label] = count\n",
    "        count += 1\n",
    "    return id2label, label2id\n",
    "\n",
    "def get_images(folder):\n",
    "    \"\"\"\n",
    "    returns numpy array of all samples in folder\n",
    "    each column is a sample resized to 30x30 and flattened\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    images = []\n",
    "    count = 0\n",
    "    \n",
    "    for f in files:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "        img_arr = get_img_array(f)\n",
    "        img_arr = img_arr.flatten() / 255.0\n",
    "        images.append(img_arr)\n",
    "    X = np.column_stack(images)\n",
    "\n",
    "    return X\n",
    "\n",
    "def get_train_data(data_root_path):\n",
    "    \"\"\"\n",
    "    Return X and y\n",
    "    \"\"\"\n",
    "    train_data_path = data_root_path + 'train'\n",
    "    id2label, label2id = get_label_mapping(data_root_path+'labels.txt')\n",
    "    print(label2id)\n",
    "    X = get_images(train_data_path)\n",
    "    y = get_labels(train_data_path, label2id)\n",
    "    return X, y\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    \"\"\"\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Helper functions, DO NOT modify this\n",
    "\n",
    "def get_img_array(path):\n",
    "    \"\"\"\n",
    "    Given path of image, returns it's numpy array\n",
    "    \"\"\"\n",
    "    return scipy.misc.imread(path)\n",
    "\n",
    "def get_files(folder):\n",
    "    \"\"\"\n",
    "    Given path to folder, returns list of files in it\n",
    "    \"\"\"\n",
    "    filenames = [file for file in glob.glob(folder+'*/*')]\n",
    "    filenames.sort()\n",
    "    return filenames\n",
    "\n",
    "def get_label(filepath, label2id):\n",
    "    \"\"\"\n",
    "    Files are assumed to be labeled as: /path/to/file/999_frog.png\n",
    "    Returns label for a filepath\n",
    "    \"\"\"\n",
    "    tokens = filepath.split('/')\n",
    "    label = tokens[-1].split('_')[1][:-4]\n",
    "    if label in label2id:\n",
    "        return label2id[label]\n",
    "    else:\n",
    "        sys.exit(\"Invalid label: \" + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel/__main__.py:7: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000/50000\n",
      "Loaded 20000/50000\n",
      "Loaded 30000/50000\n",
      "Loaded 40000/50000\n",
      "Loaded 50000/50000\n",
      "Loaded 10000/10000\n",
      "Data loading done\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_root_path = 'cifar10-hw1/'\n",
    "X_train, y_train = get_train_data(data_root_path) # this may take a few minutes\n",
    "X_test = get_images(data_root_path + 'test')\n",
    "print('Data loading done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### run some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight and bias dimentions and norms\n",
      "[['layer0', (100, 3072), (100,)], ['layer1', (500, 100), (500,)], ['layer2', (100, 500), (100,)], ['layer3', (30, 100), (30,)], ['layer4', (10, 30), (10,)]]\n",
      "[(193.41081813638363, 0.0), (167.42608344452105, 0.0), (167.03653537541445, 0.0), (46.761635217691875, 0.0), (16.579385674361468, 0.0)]\n",
      "\n",
      "test affineForward\n",
      "\t (2, 2) • (2, 2) = (2, 2)\n",
      "\t( (2, 2) + (2, 1) ).T = (2, 2)\n",
      "(array([[4, 1],\n",
      "       [2, 2]]), array([[1, 0],\n",
      "       [0, 1]]))\n",
      "(2,)\n",
      "\t (2, 2) • (2,) = (2,)\n",
      "\t( (2,) + (2, 1) ).T = (2, 2)\n",
      "(array([[5, 4],\n",
      "       [5, 4]]), array([1, 1]))\n",
      "\n",
      "test activation\n",
      "[0.1, 0.2, -0.2, 1, -1, -1, -1, 1, 1, 1, 2]\n",
      "[ 0.1  0.2  0.   1.   0.   0.   0.   1.   1.   1.   2. ]\n",
      "[ 0.1  0.2  0.   1.   0.   0.   0.   1.   1.   1.   2. ]\n",
      "\n",
      "\n",
      "\n",
      "test forward prop\n",
      "compute: 0\n",
      "\t (100, 3072) • (3072, 10000) = (100, 10000)\n",
      "\t( (100, 10000) + (100, 1) ).T = (10000, 100)\n",
      "compute: 1\n",
      "\t (500, 100) • (100, 10000) = (500, 10000)\n",
      "\t( (500, 10000) + (500, 1) ).T = (10000, 500)\n",
      "compute: 2\n",
      "\t (100, 500) • (500, 10000) = (100, 10000)\n",
      "\t( (100, 10000) + (100, 1) ).T = (10000, 100)\n",
      "compute: 3\n",
      "\t (30, 100) • (100, 10000) = (30, 10000)\n",
      "\t( (30, 10000) + (30, 1) ).T = (10000, 30)\n",
      "compute: 4\n",
      "\t (10, 30) • (30, 10000) = (10, 10000)\n",
      "\t( (10, 10000) + (10, 1) ).T = (10000, 10)\n",
      "\n",
      "AL: (10, 10000)\n",
      "\n",
      "\n",
      "\n",
      "test cost\n",
      "cost: ( 2.30253064065 ?= 2.3 )  ( 0.100005445382 ?= 0.1 )\n",
      "\t/\\ should be about 2.3 and .1\n",
      "\n",
      "\n",
      "compute: 0\n",
      "\t (100, 3072) • (3072, 10000) = (100, 10000)\n",
      "\t( (100, 10000) + (100, 1) ).T = (10000, 100)\n",
      "compute: 1\n",
      "\t (500, 100) • (100, 10000) = (500, 10000)\n",
      "\t( (500, 10000) + (500, 1) ).T = (10000, 500)\n",
      "compute: 2\n",
      "\t (100, 500) • (500, 10000) = (100, 10000)\n",
      "\t( (100, 10000) + (100, 1) ).T = (10000, 100)\n",
      "compute: 3\n",
      "\t (30, 100) • (100, 10000) = (30, 10000)\n",
      "\t( (30, 10000) + (30, 1) ).T = (10000, 30)\n",
      "compute: 4\n",
      "\t (10, 30) • (30, 10000) = (10, 10000)\n",
      "\t( (10, 10000) + (10, 1) ).T = (10000, 10)\n",
      "\n",
      "\n",
      "test predict\n",
      "\n",
      "predict: \n",
      " (10, 10000)\n",
      "\n",
      "acc:  0.1036\n",
      "\t/\\ should be about .1\n",
      "\n",
      "\n",
      "\n",
      " [[ 0.09840573  0.09845216  0.09826705  0.09880251  0.09857056]\n",
      " [ 0.1026729   0.10151432  0.10303648  0.10148511  0.1022972 ]\n",
      " [ 0.09931819  0.10125042  0.10022606  0.10035377  0.10071786]\n",
      " [ 0.10236563  0.10194432  0.10203107  0.10135698  0.10146633]\n",
      " [ 0.10049644  0.10060077  0.10126818  0.10003889  0.10011624]\n",
      " [ 0.09840573  0.09845216  0.09826705  0.09880251  0.09857056]\n",
      " [ 0.09981537  0.09892665  0.09882124  0.10004732  0.09965888]\n",
      " [ 0.09840573  0.09845216  0.09826705  0.09880251  0.09857056]\n",
      " [ 0.09995108  0.10051745  0.09973405  0.10066414  0.10005911]\n",
      " [ 0.1001632   0.09988958  0.10008177  0.09964628  0.09997269]\n",
      " [ 1.          1.          1.          1.          1.        ]]\n",
      "\t/\\ should all add to 1\n"
     ]
    }
   ],
   "source": [
    "layer_dimensions = [X_train.shape[0], 100, 500, 100, 30, 10]  # including the input and output layers\n",
    "NN = NeuralNetwork(layer_dimensions)\n",
    "# NN.train(X_train, y_train, iters=10, alpha=0, batch_size=128, print_every=10)\n",
    "\n",
    "# print([k for k in NN.parameters.keys()])\n",
    "# print([v.shape for v in NN.parameters.values()])\n",
    "\n",
    "print(\"weight and bias dimentions and norms\")\n",
    "print([[k, v[0].shape, v[1].shape] for k,v in NN.parameters.items()])\n",
    "print([( np.sum(v[0] ** 2), np.sum(v[1] ** 2)) for k,v in NN.parameters.items()])\n",
    "print()\n",
    "\n",
    "print(\"test affineForward\")\n",
    "A = np.array([[1, 0], [0, 1]])\n",
    "W = np.array([[4, 1], \n",
    "     [2, 2]])\n",
    "print(NN.affineForward(A, W, np.array([0,0]).T))\n",
    "A = np.array([1,1])\n",
    "print(A.shape)\n",
    "print(NN.affineForward(A, W, np.array([0,0])))\n",
    "\n",
    "print(\"\\ntest activation\")\n",
    "z = [.1, .2, -.2, 1, -1, -1, -1, 1, 1, 1, 2]\n",
    "print(z)\n",
    "print(NN.relu(z))\n",
    "print(NN.activationForward(z))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_train[:, 0].shape)\n",
    "# print(X_train[1].shape)\n",
    "\n",
    "\n",
    "print(\"\\ntest forward prop\")\n",
    "AL, cache = NN.forwardPropagation(X_train[:, :10000])\n",
    "print(\"\\nAL:\", AL.shape)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"test cost\")\n",
    "cost, dAL = NN.costFunction(AL, y_train[:10000])\n",
    "print(\"cost: (\", cost, \"?=\", 2.3, \")  (\", np.exp(-cost), \"?=\", 0.1,\")\") # cost \\approx    2.3  \\approx    -log(1/10)     => :-)\n",
    "print(\"\\t/\\ should be about 2.3 and .1\\n\\n\")\n",
    "\n",
    "y_hat = NN.predict(X_train[:, :10000])\n",
    "\n",
    "print(\"\\n\\ntest predict\\n\")\n",
    "print(\"predict: \\n\", y_hat.shape)\n",
    "\n",
    "print(\"\\nacc: \", np.mean(np.argmax(y_hat, axis=0) == y_train[:10000]))\n",
    "print(\"\\t/\\ should be about .1\")\n",
    "\n",
    "print(\"\\n\\n\\n\", np.row_stack( (y_hat[:, :5], np.sum( y_hat[:, :5], axis=0)) ) )  # last row = 1  => :-)\n",
    "print(\"\\t/\\ should all add to 1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### check the distribution on the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "y_ [ 0.09865178  0.10162299  0.10141062  0.10123861  0.099907    0.09865178\n",
      "  0.09999661  0.09865178  0.10022558  0.09964326]\n",
      "1\n",
      "0.101622985025\n",
      "p: 1\n",
      "mean: [ 0.0983747   0.10184484  0.10089589  0.10133988  0.10028481  0.09837318\n",
      "  0.09938597  0.09840119  0.10097475  0.10012477]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 3.,  0.,  1.,  0.,  0.,  2.,  0.,  2.,  1.,  1.]),\n",
       " array([  983.73182352,   987.2034861 ,   990.67514868,   994.14681125,\n",
       "          997.61847383,  1001.0901364 ,  1004.56179898,  1008.03346155,\n",
       "         1011.50512413,  1014.97678671,  1018.44844928]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEMhJREFUeJzt3X+sX3V9x/HnS4oi+KMIV8JaWEls3NBEYE3B6YyDyU8j\nZFGD2bQhJN0StuC2xIH/EH+QQLKImkwSQjuLU7ADCUSI0ADO+Qc/WkB+6qgI0g5otYAy5g/wvT++\nn8uutd29X3rvPRc+z0dy8z3nfT7fc96nae/rns8539tUFZKk/rxq6AYkScMwACSpUwaAJHXKAJCk\nThkAktQpA0CSOmUASFKnDABJ6pQBIEmdWjR0A/+fAw88sJYtWzZ0G5L0srJp06afVNXEdOMWdAAs\nW7aMjRs3Dt2GJL2sJHl0JuOcApKkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQ\npE4t6E8C76ll51w3yHEfueCUQY4rSePwCkCSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4Z\nAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdWpGAZDkkST3Jrk7ycZWe1OSDUkeaq/7t3qS\nfDHJ5iT3JDlqyn5WtfEPJVk1N6ckSZqJca4A/rSqjqiqFW39HOCmqloO3NTWAU4Clrev1cDFMAoM\n4DzgaGAlcN5kaEiS5t+eTAGdCqxry+uA06bUL6uRW4HFSQ4GTgA2VNWOqnoK2ACcuAfHlyTtgZkG\nQAE3JtmUZHWrHVRVj7flJ4CD2vIS4LEp793SarurS5IGMNP/EvLdVbU1yZuBDUm+P3VjVVWSmo2G\nWsCsBjj00ENnY5eSpF2Y0RVAVW1tr9uAqxnN4T/ZpnZor9va8K3AIVPevrTVdlff+ViXVNWKqlox\nMTEx3tlIkmZs2gBIsl+S108uA8cD9wHXApNP8qwCrmnL1wIfa08DHQM806aKbgCOT7J/u/l7fKtJ\nkgYwkymgg4Crk0yO/1pVfSvJHcD6JGcCjwIfbuOvB04GNgPPAWcAVNWOJJ8B7mjjPl1VO2btTCRJ\nY5k2AKrqYeAdu6j/FDhuF/UCztrNvtYCa8dvU5I02/wksCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwA\nSeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCk\nThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSp2YcAEn2\nSnJXkm+29cOS3JZkc5KvJ3l1q7+mrW9u25dN2ce5rf6DJCfM9slIkmZunCuAs4EHp6xfCFxUVW8B\nngLObPUzgada/aI2jiSHA6cDbwNOBL6UZK89a1+S9FLNKACSLAVOAS5t6wGOBa5sQ9YBp7XlU9s6\nbftxbfypwBVV9cuq+hGwGVg5GychSRrfTK8APg98AvhNWz8AeLqqnm/rW4AlbXkJ8BhA2/5MG/9i\nfRfveVGS1Uk2Jtm4ffv2MU5FkjSOaQMgyfuBbVW1aR76oaouqaoVVbViYmJiPg4pSV1aNIMx7wI+\nkORkYB/gDcAXgMVJFrWf8pcCW9v4rcAhwJYki4A3Aj+dUp809T2SpHk27RVAVZ1bVUurahmjm7g3\nV9VfALcAH2zDVgHXtOVr2zpt+81VVa1+entK6DBgOXD7rJ2JJGksM7kC2J1/BK5I8lngLmBNq68B\nvpJkM7CDUWhQVfcnWQ88ADwPnFVVL+zB8SVJe2CsAKiqbwPfbssPs4uneKrqF8CHdvP+84Hzx21S\nkjT7/CSwJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNA\nkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSp\nUwaAJHXKAJCkThkAktQpA0CSOjVtACTZJ8ntSb6X5P4kn2r1w5LclmRzkq8neXWrv6atb27bl03Z\n17mt/oMkJ8zVSUmSpjeTK4BfAsdW1TuAI4ATkxwDXAhcVFVvAZ4CzmzjzwSeavWL2jiSHA6cDrwN\nOBH4UpK9ZvNkJEkzN20A1MizbXXv9lXAscCVrb4OOK0tn9rWaduPS5JWv6KqfllVPwI2Aytn5Swk\nSWOb0T2AJHsluRvYBmwAfgg8XVXPtyFbgCVteQnwGEDb/gxwwNT6Lt4jSZpnMwqAqnqhqo4AljL6\nqf0P5qqhJKuTbEyycfv27XN1GEnq3lhPAVXV08AtwDuBxUkWtU1Lga1teStwCEDb/kbgp1Pru3jP\n1GNcUlUrqmrFxMTEOO1JksYwk6eAJpIsbsuvBd4HPMgoCD7Yhq0CrmnL17Z12vabq6pa/fT2lNBh\nwHLg9tk6EUnSeBZNP4SDgXXtiZ1XAeur6ptJHgCuSPJZ4C5gTRu/BvhKks3ADkZP/lBV9ydZDzwA\nPA+cVVUvzO7pSJJmatoAqKp7gCN3UX+YXTzFU1W/AD60m32dD5w/fpuSpNnmJ4ElqVMGgCR1ygCQ\npE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnq\nlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4Z\nAJLUKQNAkjo1bQAkOSTJLUkeSHJ/krNb/U1JNiR5qL3u3+pJ8sUkm5Pck+SoKfta1cY/lGTV3J2W\nJGk6M7kCeB74h6o6HDgGOCvJ4cA5wE1VtRy4qa0DnAQsb1+rgYthFBjAecDRwErgvMnQkCTNv2kD\noKoer6o72/LPgQeBJcCpwLo2bB1wWls+FbisRm4FFic5GDgB2FBVO6rqKWADcOKsno0kacbGugeQ\nZBlwJHAbcFBVPd42PQEc1JaXAI9NeduWVttdXZI0gBkHQJLXAVcBH6+qn03dVlUF1Gw0lGR1ko1J\nNm7fvn02dilJ2oUZBUCSvRl98/9qVX2jlZ9sUzu0122tvhU4ZMrbl7ba7uq/paouqaoVVbViYmJi\nnHORJI1hJk8BBVgDPFhVn5uy6Vpg8kmeVcA1U+ofa08DHQM806aKbgCOT7J/u/l7fKtJkgawaAZj\n3gV8FLg3yd2t9kngAmB9kjOBR4EPt23XAycDm4HngDMAqmpHks8Ad7Rxn66qHbNyFpKksU0bAFX1\nXSC72XzcLsYXcNZu9rUWWDtOg5KkueEngSWpUwaAJHVqJvcA9DKx7JzrBjv2IxecMtixJb00XgFI\nUqcMAEnqlAEgSZ0yACSpUwaAJHXKp4AkLXhDPeH2Sn+6zSsASeqUASBJnTIAJKlTBoAkdcoAkKRO\nGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQB\nIEmdMgAkqVMGgCR1atoASLI2ybYk902pvSnJhiQPtdf9Wz1Jvphkc5J7khw15T2r2viHkqyam9OR\nJM3UTK4AvgycuFPtHOCmqloO3NTWAU4Clrev1cDFMAoM4DzgaGAlcN5kaEiShjFtAFTVd4AdO5VP\nBda15XXAaVPql9XIrcDiJAcDJwAbqmpHVT0FbOB3Q0WSNI9e6j2Ag6rq8bb8BHBQW14CPDZl3JZW\n2139dyRZnWRjko3bt29/ie1JkqazxzeBq6qAmoVeJvd3SVWtqKoVExMTs7VbSdJOXmoAPNmmdmiv\n21p9K3DIlHFLW213dUnSQF5qAFwLTD7Jswq4Zkr9Y+1poGOAZ9pU0Q3A8Un2bzd/j281SdJAFk03\nIMnlwHuBA5NsYfQ0zwXA+iRnAo8CH27DrwdOBjYDzwFnAFTVjiSfAe5o4z5dVTvfWJYkzaNpA6Cq\nPrKbTcftYmwBZ+1mP2uBtWN1J0maM34SWJI6Ne0VgMa37Jzrhm5BkqblFYAkdcoAkKROGQCS1CkD\nQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAk\nqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdWrR0A1I0kK17JzrBjv2\nIxecMufHmPcASHIi8AVgL+DSqrpgvnvQ7BvqH8p8/CORXqnmdQooyV7APwMnAYcDH0ly+Hz2IEka\nme97ACuBzVX1cFX9CrgCOHWee5AkMf8BsAR4bMr6llaTJM2zBXcTOMlqYHVbfTbJD/ZgdwcCP9nz\nrmadfY1nt33lwnnu5Le97P68BmZfY8iFe9TX789k0HwHwFbgkCnrS1vtRVV1CXDJbBwsycaqWjEb\n+5pN9jUe+xqPfY2n577mewroDmB5ksOSvBo4Hbh2nnuQJDHPVwBV9XySvwFuYPQY6Nqqun8+e5Ak\njcz7PYCquh64fp4ONytTSXPAvsZjX+Oxr/F021eqaq6PIUlagPxdQJLUqVdcACRZm2RbkvuG7mWq\nJIckuSXJA0nuT3L20D0BJNknye1Jvtf6+tTQPU2VZK8kdyX55tC9TErySJJ7k9ydZOPQ/UxKsjjJ\nlUm+n+TBJO9cAD29tf05TX79LMnHh+4LIMnftb/z9yW5PMk+Q/cEkOTs1tP9c/1n9YqbAkryHuBZ\n4LKqevvQ/UxKcjBwcFXdmeT1wCbgtKp6YOC+AuxXVc8m2Rv4LnB2Vd06ZF+Tkvw9sAJ4Q1W9f+h+\nYBQAwIqqWlDPjidZB/xHVV3anrLbt6qeHrqvSe1XwWwFjq6qRwfuZQmjv+uHV9X/JFkPXF9VXx64\nr7cz+g0JK4FfAd8C/rqqNs/F8V5xVwBV9R1gx9B97KyqHq+qO9vyz4EHWQCfgq6RZ9vq3u1rQfxU\nkGQpcApw6dC9LHRJ3gi8B1gDUFW/Wkjf/JvjgB8O/c1/ikXAa5MsAvYF/mvgfgD+ELitqp6rqueB\nfwf+fK4O9ooLgJeDJMuAI4Hbhu1kpE2z3A1sAzZU1YLoC/g88AngN0M3spMCbkyyqX1yfSE4DNgO\n/EubMrs0yX5DN7WT04HLh24CoKq2Av8E/Bh4HHimqm4ctisA7gP+JMkBSfYFTua3Pzw7qwyAeZbk\ndcBVwMer6mdD9wNQVS9U1RGMPpm9sl2GDirJ+4FtVbVp6F524d1VdRSj32p7Vpt2HNoi4Cjg4qo6\nEvhv4JxhW/o/bUrqA8C/Dd0LQJL9Gf0iysOA3wP2S/KXw3YFVfUgcCFwI6Ppn7uBF+bqeAbAPGpz\n7FcBX62qbwzdz87alMEtwIlD9wK8C/hAm2+/Ajg2yb8O29JI++mRqtoGXM1ovnZoW4AtU67ermQU\nCAvFScCdVfXk0I00fwb8qKq2V9WvgW8AfzxwTwBU1Zqq+qOqeg/wFPCfc3UsA2CetJuta4AHq+pz\nQ/czKclEksVt+bXA+4DvD9sVVNW5VbW0qpYxmjq4uaoG/wktyX7tJj5tiuV4Rpftg6qqJ4DHkry1\nlY4DBn3AYCcfYYFM/zQ/Bo5Jsm/7t3kco/tyg0vy5vZ6KKP5/6/N1bEW3G8D3VNJLgfeCxyYZAtw\nXlWtGbYrYPQT7UeBe9t8O8An2yejh3QwsK49ofEqYH1VLZhHLhegg4CrR98zWAR8raq+NWxLL/pb\n4KttuuVh4IyB+wFeDMr3AX81dC+Tquq2JFcCdwLPA3excD4RfFWSA4BfA2fN5c38V9xjoJKkmXEK\nSJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktSp/wUQBJ37CbSR/AAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1129fd8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSWqQ5S9JDfo/vURcwlAgp3UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112456400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_hat.shape\n",
    "p = np.argmax(y_hat, axis = 0)\n",
    "print(p.shape)\n",
    "print(\"y_\", y_hat[:, 1000])\n",
    "\n",
    "print(np.argmax(y_hat[:, 1000]))\n",
    "print(np.max(y_hat[:, 1000]))\n",
    "\n",
    "print(\"p:\", p[1000])\n",
    "plt.hist(p)\n",
    "\n",
    "print(\"mean:\", np.mean(y_hat, axis=1))\n",
    "prob_mass = np.sum(y_hat, axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(prob_mass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### tests for backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"test cost\")\n",
    "cost, dAL = NN.costFunction(AL, y_train[:10000])\n",
    "print(\"cost: {0:.2f}\".format(cost), \"dAL:\", dAL)\n",
    "\n",
    "\n",
    "# NN.affineBackward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Split train into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# X_train = ...\n",
    "# y_train = ...\n",
    "\n",
    "# X_val   \n",
    "# y_val1\n",
    "\n",
    "idx = np.random.permutation(len(y_train))\n",
    "X_all = X_train[:, idx]\n",
    "y_all = y_train[idx]\n",
    "\n",
    "m_train = int(len(y_all)*0.9);\n",
    "m_val   = len(y_all)-m_train\n",
    "\n",
    "X       = X_all[:, :m_train]\n",
    "y       = y_all[:m_train]\n",
    "\n",
    "X_val   = X_all[:, m_train:]\n",
    "y_val   = y_all[m_train:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(X_all.shape)\n",
    "print(y_all.shape)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(y)\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 1\n",
    "\n",
    "#### Simple fully-connected deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "layer_dimensions = [X_train.shape[0], 1, 10]  # including the input and output layers\n",
    "NN = NeuralNetwork(layer_dimensions)\n",
    "NN.train(X_train, y_train, iters=10, alpha=0, batch_size=128, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_predicted = NN.predict(X_test)\n",
    "save_predictions('ans1-uni', y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# test if your numpy file has been saved correctly\n",
    "loaded_y = np.load('ans1-uni.npy')\n",
    "print(loaded_y.shape)\n",
    "loaded_y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 2: Regularizing the neural network\n",
    "#### Add dropout and L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "NN2 = NeuralNetwork(layer_dimensions, drop_prob=0, reg_lambda=0)\n",
    "NN2.train(X_train, y_train, iters=1000, alpha=0.00001, batch_size=1000, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_predicted2 = NN2.predict(X)\n",
    "save_predictions(y_predicted, 'ans2-uni')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

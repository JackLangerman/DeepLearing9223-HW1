{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# CS-GY 9223-E: Deep Learning Homework 1\n",
    "Due on Sunday, 11th February 2018, 11:55 PM\n",
    "\n",
    "This homework can be done in pairs. Everyone must submit on NYU Classes individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Write down the UNIs (NetIDs) of your group (if applicable)\n",
    "\n",
    "Member 1: Name, NetID\n",
    "\n",
    "Member 2: Name, NetID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import glob\n",
    "import sys\n",
    "# you shouldn't need to make any more imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    Abstraction of neural network.\n",
    "    Stores parameters, activations, cached values. \n",
    "    Provides necessary functions for training and prediction. \n",
    "    \"\"\"\n",
    "    def __init__(self, layer_dimensions, drop_prob=0.0, reg_lambda=0.0):\n",
    "        \"\"\"\n",
    "        Initializes the weights and biases for each layer\n",
    "        :param layer_dimensions: (list) number of nodes in each layer\n",
    "        :param drop_prob: drop probability for dropout layers. Only required in part 2 of the assignment\n",
    "        :param reg_lambda: regularization parameter. Only required in part 2 of the assignment\n",
    "        \"\"\"\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        self.parameters = {}\n",
    "        self.num_layers = len(layer_dimensions)\n",
    "        self.drop_prob = drop_prob\n",
    "        self.reg_lambda = reg_lambda\n",
    "        \n",
    "        # init parameters\n",
    "        for i in range(1, self.num_layers):\n",
    "            d_in  = layer_dimensions[i-1]\n",
    "            d_out = layer_dimensions[i]\n",
    "            self.parameters[\"layer\"+str(i)] = \\\n",
    "                ( (np.random.randn(d_out, d_in) * np.sqrt(2/(d_in+d_out))), \n",
    "                     np.zeros(d_out) )\n",
    "                \n",
    "#         print(\"num weights:\", len(parameters))\n",
    "                #             self.parameters[\"b\"+str(i)] = np.zeros(d_out)\n",
    "        \n",
    "\n",
    "    def affineForward(self, A, W, b):\n",
    "        \"\"\"\n",
    "        Forward pass for the affine layer.\n",
    "        :param A: input matrix, shape (L, S), where L is the number of hidden units in the previous layer and S is\n",
    "        the number of samples\n",
    "        :returns: the affine product WA + b, along with the cache required for the backward pass\n",
    "        \"\"\"\n",
    "#         lin = np.matmul(A.T, W)\n",
    "#         print(\"\\t\", str(A.shape)+\".T\", \"•\", W.shape, \"=\", lin.shape)\n",
    "        \n",
    "\n",
    "#         return aff.T\n",
    "        \n",
    "        lin = np.matmul(W, A)\n",
    "        print(\"\\t\", str(W.shape), \"•\", A.shape, \"=\", lin.shape)\n",
    "        \n",
    "        aff = lin + b[:, None]\n",
    "        print(\"\\t(\", lin.shape, \"+\", b[:, None].shape, \").T =\", aff.T.shape)\n",
    "\n",
    "        \n",
    "        return aff, [A, W, aff]     # A which is the activations from the previous layer\n",
    "                                    # and W which are the weights from the current layer \n",
    "                                    # are returned as the cache for backprop\n",
    "    \n",
    "\n",
    "    def activationForward(self, A, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Common interface to access all activation functions.\n",
    "        :param A: input to the activation function\n",
    "        :param prob: activation funciton to apply to A. Just \"relu\" for this assignment.\n",
    "        :returns: activation(A)\n",
    "        \"\"\" \n",
    "        return getattr(self,activation)(A)\n",
    "\n",
    "\n",
    "    def relu(self, X):\n",
    "        return np.maximum(X, 0)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def softmax(self, X):\n",
    "        ex = np.exp(X - np.max(X, axis=0))\n",
    "        return ex / np.sum(ex, axis=0)\n",
    "    \n",
    "#     def softmax_derivative(self, dx, cached_x):\n",
    "#         sm = softmax(cached_x)\n",
    "#         sigma_prime = sm * (np.ones_like(cached_x) - sm)\n",
    "#         return dx * sigma_prime\n",
    "        \n",
    "    def dropout(self, A, prob):\n",
    "        \"\"\"\n",
    "        :param A: Activation\n",
    "        :param prob: drop prob\n",
    "        :returns: tuple (A, M) \n",
    "            WHERE\n",
    "            A is matrix after applying dropout\n",
    "            M is dropout mask, used in the backward pass\n",
    "        \"\"\"\n",
    "        \n",
    "        M = np.random.rand(A.shape[0], A.shape[1])\n",
    "        M = (M > prob) * 1.0\n",
    "        scale_up_prob = 1 / (1 - prob)\n",
    "        M *= scale_up_prob\n",
    "        A *= M\n",
    "        \n",
    "        return A, M\n",
    "\n",
    "    def forwardPropagation(self, X):\n",
    "        \"\"\"\n",
    "        Runs an input X through the neural network to compute activations\n",
    "        for all layers. Returns the output computed at the last layer along\n",
    "        with the cache required for backpropagation.\n",
    "        :returns: (tuple) AL, cache\n",
    "            WHERE \n",
    "            AL is activation of last layer\n",
    "            cache is cached values for each layer that\n",
    "                     are needed in further steps\n",
    "        \"\"\"\n",
    "        \n",
    "        cache = []\n",
    "        A = X #A0 is the input\n",
    "        for i in range(1, self.num_layers):\n",
    "            print(\"compute:\", i)\n",
    "#             cache.append(A)\n",
    "            \n",
    "#             W = self.parameters[\"w\"+str(i)]\n",
    "#             b = self.parameters[\"b\"+str(i)]\n",
    "            W, b = self.parameters[\"layer\"+str(i)]\n",
    "            Z, layer_cache = self.affineForward(A, W, b)\n",
    "            \n",
    "            cache.extend(layer_cache)\n",
    "            \n",
    "            A = self.activationForward(Z, 'relu')\n",
    "            \n",
    "            \n",
    "        AL = self.softmax(A)\n",
    "        \n",
    "        return AL, cache\n",
    "    \n",
    "    \n",
    "    def costFunction(self, AL, y):\n",
    "        \"\"\"\n",
    "        :param AL: Activation of last layer, shape (num_classes, S)\n",
    "        :param y: labels, shape (S)\n",
    "        :param alpha: regularization parameter\n",
    "        :returns cost, dAL: A scalar denoting cost and the gradient of cost\n",
    "        \"\"\"\n",
    "        # compute loss\n",
    "        N = y.shape[0]\n",
    "        AL_softmax = self.softmax(AL)\n",
    "        correct_label_prob = AL_softmax[y, range(N)]\n",
    "        cost = -np.sum(np.log(correct_label_prob)) / N\n",
    "        \n",
    "#         if self.reg_lambda > 0:\n",
    "#             # add regularization\n",
    "#             cost += self.reg_lambda * np.sum(paramiters ** 2)\n",
    "            \n",
    "        # gradient of cost\n",
    "#         dAL = np.sum(-Y / y_pred) / len(y)\n",
    "#         dAL = np.zeros_like(AL)\n",
    "        \n",
    "                            # y S\n",
    "        dAL = AL_softmax - 1\n",
    "        print(\"dAL:\", dAL.shape)\n",
    "    \n",
    "        return cost, dAL\n",
    "\n",
    "    def affineBackward(self, dA_prev, cache):\n",
    "        \"\"\"\n",
    "        Backward pass for the affine layer.\n",
    "        :param dA_prev: gradient from the next layer.\n",
    "        :param cache: cache returned in affineForward\n",
    "        :returns dA: gradient on the input to this layer\n",
    "                 dW: gradient on the weights\n",
    "                 db: gradient on the bias\n",
    "        \"\"\"\n",
    "#         print(dA_prev.shape, cache.shape)\n",
    "        W = cache.pop()\n",
    "        A = cache.pop()\n",
    "        print(\"W shapppppeeee\",W.shape)\n",
    "        print(\"A shapppppeeee\",A.shape)\n",
    "        # TILE IS WROOOOOOOONGGGGG\n",
    "        print(\"np.shape(np.sum(A.T, axis=0))\", np.shape(np.sum(A.T, axis=0)))\n",
    "        print(\"np.shape(np.sum(W.T, axis=0))\", np.shape(np.sum(W, axis=0)))\n",
    "        dW = np.tile( np.sum(A.T, axis=0), (W.shape[0], 1) )\n",
    "        print(\"dW shape\", np.shape(dW))\n",
    "        \n",
    "        \n",
    "        print(\"dA_prev shpe\", np.shape(dA_prev))\n",
    "        W_ = (np.tile( np.sum(W, axis=0), ( dA_prev.shape[0], 1 ) ))\n",
    "        print(\"W_ shape\", np.shape(W_))\n",
    "\n",
    "        dA = np.matmul(W_.T, dA_prev)\n",
    "        print(\"dA shape\", np.shape(dA))\n",
    "\n",
    "        \n",
    "        \n",
    "#         dA = np.tile( [1,2,3,4,5], ( A.shape[1], 1 ) )\n",
    "        \n",
    "        print(\"W rows\", W.shape[0])\n",
    "        db = np.ones((W.shape[0],1))\n",
    "        db = np.sum(dA_prev, axis=1)\n",
    "        \n",
    "        print(\"the shapes(w,a,b):\", dW.shape, dA.shape, db.shape, \"\\tdaprev\", dA_prev.shape)\n",
    "        \n",
    "        \n",
    "#         dW = np.sum(X.T, axis=0)\n",
    "#         dA = dA_prev.dot(dW)\n",
    "        return dA, dW, db\n",
    "    \n",
    "\n",
    "    def activationBackward(self, dA, cache, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Interface to call backward on activation functions.\n",
    "        In this case, it's just relu. \n",
    "        \"\"\"\n",
    "        return getattr(self,activation+\"_derivative\")(dA, cache.pop())\n",
    "\n",
    "        \n",
    "    def relu_derivative(self, dx, cached_x):\n",
    "        sigma_prime = np.zeros_like(cached_x)\n",
    "        \n",
    "        print(\"s':\", sigma_prime.shape, \"dx: \", dx.shape, \"cachedx:\", cached_x.shape)\n",
    "        \n",
    "        sigma_prime[ cached_x > 0] = 1\n",
    "        \n",
    "        return dx * sigma_prime\n",
    "\n",
    "    def dropout_backward(self, dA, cache):\n",
    "\n",
    "        return dA\n",
    "\n",
    "    def backPropagation(self, dAL, Y, cache):\n",
    "        \"\"\"\n",
    "        Run backpropagation to compute gradients on all paramters in the model\n",
    "        :param dAL: gradient on the last layer of the network. Returned by the cost function.\n",
    "        :param Y: labels\n",
    "        :param cache: cached values during forwardprop\n",
    "        :returns gradients: dW and db for each weight/bias\n",
    "        \"\"\"\n",
    "        print(\"cache shape: {:d}\".format(len(cache)))\n",
    "        gradients = {}\n",
    "        upstream_grad = Y * dAL\n",
    "        print(\"upstream grad:\", upstream_grad.shape)\n",
    "        cache.pop() # get rid of aff because the last layer doesn't use relu_backward\n",
    "        dA, dW, db = self.affineBackward(upstream_grad, cache)\n",
    "        gradients[\"layer\"+str(self.num_layers-1)] = (dW, db)\n",
    "        \n",
    "        for i in reversed(range(1, self.num_layers-1)): # loop through the layers backwards\n",
    "            print(\"backprop through layer \"+str(i))\n",
    "            [print(c.shape) for c in cache]\n",
    "            print(\"----\")\n",
    "            \n",
    "            dA = self.activationBackward(dA, cache)\n",
    "#             db = dA\n",
    "            dA, dW, db = self.affineBackward(dA, cache)\n",
    "            print(\"cache shape: {:d}\".format(len(cache)))\n",
    "\n",
    "            gradients[\"layer\"+str(i)] = (dW, db)\n",
    "#             gradients[\"dw\"+str(i)] = dW\n",
    "#             gradients[\"db\"+str(i)] = db\n",
    "#             if self.drop_prob > 0:\n",
    "                #call dropout_backward\n",
    "           \n",
    "            \n",
    "#         if self.reg_lambda > 0:\n",
    "            # add gradients from L2 regularization to each dW\n",
    "        \n",
    "        return gradients\n",
    "\n",
    "\n",
    "    def updateParameters(self, gradients, alpha):\n",
    "        \"\"\"\n",
    "        :param gradients: gradients for each weight/bias\n",
    "        :param alpha: step size for gradient descent \n",
    "        \"\"\"\n",
    "#         self.parameters -= alpha * gradients\n",
    "        for key in self.parameters:\n",
    "            print(\"updating \", key)\n",
    "            w, b = self.parameters[key]\n",
    "            \n",
    "            w -= alpha * gradients[key][0]\n",
    "            b -= alpha * gradients[key][1]\n",
    "\n",
    "            self.parameters[key] = (w, b)\n",
    "\n",
    "            \n",
    "    def train(self, X, y, iters=1000, alpha=0.0001, batch_size=100, print_every=100):\n",
    "        \"\"\"\n",
    "        :param X: input samples, each column is a sample\n",
    "        :param y: labels for input samples, y.shape[0] must equal X.shape[1]\n",
    "        :param iters: number of training iterations\n",
    "        :param alpha: step size for gradient descent\n",
    "        :param batch_size: number of samples in a minibatch\n",
    "        :param print_every: no. of iterations to print debug info after\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(0, iters):\n",
    "            # get minibatch\n",
    "#                 X_mb, y_mb = get_batch(X, y, batch_size)\n",
    "            # forward prop\n",
    "            AL, cache = self.forwardPropagation(X)\n",
    "            # compute loss\n",
    "            cost, dAL = self.costFunction(AL, y)\n",
    "            # compute gradients\n",
    "            gradients = self.backPropagation(dAL, y, cache)  \n",
    "#             print(gradients)\n",
    "            print(\"done backproping\")\n",
    "    \n",
    "            # update weights and biases based on gradient\n",
    "            self.updateParameters(gradients, alpha)\n",
    "            \n",
    "\n",
    "            if i % print_every == 0:\n",
    "                # print cost, train and validation set accuracies\n",
    "                print(\"should print cost \"+str(i))\n",
    "                print(\"\\n\\nCOST:\",cost,\"\\n\\n\")\n",
    "                print(gradients)\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions for each sample\n",
    "        \"\"\"\n",
    "        AL, cache = NN.forwardPropagation(X)\n",
    "        \n",
    "        y_pred = self.softmax(AL)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def get_batch(self, X, y, batch_size):\n",
    "        \"\"\"\n",
    "        Return minibatch of samples and labels\n",
    "        \n",
    "        :param X, y: samples and corresponding labels\n",
    "        :parma batch_size: minibatch size\n",
    "        :returns: (tuple) X_batch, y_batch\n",
    "        \"\"\"\n",
    "\n",
    "        return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1]\n",
      "[2, 2]\n",
      "\n",
      "[3 3]\n",
      "[2 4]\n",
      "\n",
      "[0 1 2 3 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3, 4, 0, 1, 2, 3, 4],\n",
       "       [0, 1, 2, 3, 4, 0, 1, 2, 3, 4],\n",
       "       [0, 1, 2, 3, 4, 0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [[1, 1], [2, 2]]\n",
    "\n",
    "[print(r) for r in x]\n",
    "print(\"\")\n",
    "print(np.sum(x, axis=0))\n",
    "print(np.sum(x, axis=1))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "x = np.arange(5)\n",
    "print(x)\n",
    "# np.repeat(x, [5, 1], axis=0)\n",
    "np.tile(x, (3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute: 1\n",
      "\t (72, 3072) • (3072, 50) = (72, 50)\n",
      "\t( (72, 50) + (72, 1) ).T = (50, 72)\n",
      "compute: 2\n",
      "\t (100, 72) • (72, 50) = (100, 50)\n",
      "\t( (100, 50) + (100, 1) ).T = (50, 100)\n",
      "compute: 3\n",
      "\t (20, 100) • (100, 50) = (20, 50)\n",
      "\t( (20, 50) + (20, 1) ).T = (50, 20)\n",
      "compute: 4\n",
      "\t (10, 20) • (20, 50) = (10, 50)\n",
      "\t( (10, 50) + (10, 1) ).T = (50, 10)\n",
      "dAL: (10, 50)\n",
      "cache shape: 12\n",
      "upstream grad: (10, 50)\n",
      "W shapppppeeee (10, 20)\n",
      "A shapppppeeee (20, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (20,)\n",
      "np.shape(np.sum(W.T, axis=0)) (20,)\n",
      "dW shape (10, 20)\n",
      "dA_prev shpe (10, 50)\n",
      "W_ shape (10, 20)\n",
      "dA shape (20, 50)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 20) (20, 50) (10,) \tdaprev (10, 50)\n",
      "backprop through layer 3\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "(72, 50)\n",
      "(100, 72)\n",
      "(100, 50)\n",
      "(100, 50)\n",
      "(20, 100)\n",
      "(20, 50)\n",
      "----\n",
      "s': (20, 50) dx:  (20, 50) cachedx: (20, 50)\n",
      "W shapppppeeee (20, 100)\n",
      "A shapppppeeee (100, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (20, 100)\n",
      "dA_prev shpe (20, 50)\n",
      "W_ shape (20, 100)\n",
      "dA shape (100, 50)\n",
      "W rows 20\n",
      "the shapes(w,a,b): (20, 100) (100, 50) (20,) \tdaprev (20, 50)\n",
      "cache shape: 6\n",
      "backprop through layer 2\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "(72, 50)\n",
      "(100, 72)\n",
      "(100, 50)\n",
      "----\n",
      "s': (100, 50) dx:  (100, 50) cachedx: (100, 50)\n",
      "W shapppppeeee (100, 72)\n",
      "A shapppppeeee (72, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (72,)\n",
      "np.shape(np.sum(W.T, axis=0)) (72,)\n",
      "dW shape (100, 72)\n",
      "dA_prev shpe (100, 50)\n",
      "W_ shape (100, 72)\n",
      "dA shape (72, 50)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 72) (72, 50) (100,) \tdaprev (100, 50)\n",
      "cache shape: 3\n",
      "backprop through layer 1\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "----\n",
      "s': (72, 50) dx:  (72, 50) cachedx: (72, 50)\n",
      "W shapppppeeee (72, 3072)\n",
      "A shapppppeeee (3072, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (72, 3072)\n",
      "dA_prev shpe (72, 50)\n",
      "W_ shape (72, 3072)\n",
      "dA shape (3072, 50)\n",
      "W rows 72\n",
      "the shapes(w,a,b): (72, 3072) (3072, 50) (72,) \tdaprev (72, 50)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "updating  layer3\n",
      "updating  layer4\n",
      "should print cost 0\n",
      "\n",
      "\n",
      "COST: 2.28600007638 \n",
      "\n",
      "\n",
      "{'layer4': (array([[  3.63331809e+00,   8.30239305e+00,   5.81664601e+00,\n",
      "          1.85695315e+01,   1.21476317e+01,   2.89861013e+01,\n",
      "          6.36013825e-01,   2.74990794e-02,   6.21692547e-01,\n",
      "          4.02020282e+01,   8.99067983e-01,   0.00000000e+00,\n",
      "          2.64214182e-01,   1.47837856e+01,   0.00000000e+00,\n",
      "          4.59408223e-01,   5.08221605e+01,   8.20801806e-03,\n",
      "          1.25080989e+01,   6.36134194e-03],\n",
      "       [  3.63331809e+00,   8.30239305e+00,   5.81664601e+00,\n",
      "          1.85695315e+01,   1.21476317e+01,   2.89861013e+01,\n",
      "          6.36013825e-01,   2.74990794e-02,   6.21692547e-01,\n",
      "          4.02020282e+01,   8.99067983e-01,   0.00000000e+00,\n",
      "          2.64214182e-01,   1.47837856e+01,   0.00000000e+00,\n",
      "          4.59408223e-01,   5.08221605e+01,   8.20801806e-03,\n",
      "          1.25080989e+01,   6.36134194e-03],\n",
      "       [  3.63331809e+00,   8.30239305e+00,   5.81664601e+00,\n",
      "          1.85695315e+01,   1.21476317e+01,   2.89861013e+01,\n",
      "          6.36013825e-01,   2.74990794e-02,   6.21692547e-01,\n",
      "          4.02020282e+01,   8.99067983e-01,   0.00000000e+00,\n",
      "          2.64214182e-01,   1.47837856e+01,   0.00000000e+00,\n",
      "          4.59408223e-01,   5.08221605e+01,   8.20801806e-03,\n",
      "          1.25080989e+01,   6.36134194e-03],\n",
      "       [  3.63331809e+00,   8.30239305e+00,   5.81664601e+00,\n",
      "          1.85695315e+01,   1.21476317e+01,   2.89861013e+01,\n",
      "          6.36013825e-01,   2.74990794e-02,   6.21692547e-01,\n",
      "          4.02020282e+01,   8.99067983e-01,   0.00000000e+00,\n",
      "          2.64214182e-01,   1.47837856e+01,   0.00000000e+00,\n",
      "          4.59408223e-01,   5.08221605e+01,   8.20801806e-03,\n",
      "          1.25080989e+01,   6.36134194e-03],\n",
      "       [  3.63331809e+00,   8.30239305e+00,   5.81664601e+00,\n",
      "          1.85695315e+01,   1.21476317e+01,   2.89861013e+01,\n",
      "          6.36013825e-01,   2.74990794e-02,   6.21692547e-01,\n",
      "          4.02020282e+01,   8.99067983e-01,   0.00000000e+00,\n",
      "          2.64214182e-01,   1.47837856e+01,   0.00000000e+00,\n",
      "          4.59408223e-01,   5.08221605e+01,   8.20801806e-03,\n",
      "          1.25080989e+01,   6.36134194e-03],\n",
      "       [  3.63331809e+00,   8.30239305e+00,   5.81664601e+00,\n",
      "          1.85695315e+01,   1.21476317e+01,   2.89861013e+01,\n",
      "          6.36013825e-01,   2.74990794e-02,   6.21692547e-01,\n",
      "          4.02020282e+01,   8.99067983e-01,   0.00000000e+00,\n",
      "          2.64214182e-01,   1.47837856e+01,   0.00000000e+00,\n",
      "          4.59408223e-01,   5.08221605e+01,   8.20801806e-03,\n",
      "          1.25080989e+01,   6.36134194e-03],\n",
      "       [  3.63331809e+00,   8.30239305e+00,   5.81664601e+00,\n",
      "          1.85695315e+01,   1.21476317e+01,   2.89861013e+01,\n",
      "          6.36013825e-01,   2.74990794e-02,   6.21692547e-01,\n",
      "          4.02020282e+01,   8.99067983e-01,   0.00000000e+00,\n",
      "          2.64214182e-01,   1.47837856e+01,   0.00000000e+00,\n",
      "          4.59408223e-01,   5.08221605e+01,   8.20801806e-03,\n",
      "          1.25080989e+01,   6.36134194e-03],\n",
      "       [  3.63331809e+00,   8.30239305e+00,   5.81664601e+00,\n",
      "          1.85695315e+01,   1.21476317e+01,   2.89861013e+01,\n",
      "          6.36013825e-01,   2.74990794e-02,   6.21692547e-01,\n",
      "          4.02020282e+01,   8.99067983e-01,   0.00000000e+00,\n",
      "          2.64214182e-01,   1.47837856e+01,   0.00000000e+00,\n",
      "          4.59408223e-01,   5.08221605e+01,   8.20801806e-03,\n",
      "          1.25080989e+01,   6.36134194e-03],\n",
      "       [  3.63331809e+00,   8.30239305e+00,   5.81664601e+00,\n",
      "          1.85695315e+01,   1.21476317e+01,   2.89861013e+01,\n",
      "          6.36013825e-01,   2.74990794e-02,   6.21692547e-01,\n",
      "          4.02020282e+01,   8.99067983e-01,   0.00000000e+00,\n",
      "          2.64214182e-01,   1.47837856e+01,   0.00000000e+00,\n",
      "          4.59408223e-01,   5.08221605e+01,   8.20801806e-03,\n",
      "          1.25080989e+01,   6.36134194e-03],\n",
      "       [  3.63331809e+00,   8.30239305e+00,   5.81664601e+00,\n",
      "          1.85695315e+01,   1.21476317e+01,   2.89861013e+01,\n",
      "          6.36013825e-01,   2.74990794e-02,   6.21692547e-01,\n",
      "          4.02020282e+01,   8.99067983e-01,   0.00000000e+00,\n",
      "          2.64214182e-01,   1.47837856e+01,   0.00000000e+00,\n",
      "          4.59408223e-01,   5.08221605e+01,   8.20801806e-03,\n",
      "          1.25080989e+01,   6.36134194e-03]]), array([-200.11187766, -202.02715145, -201.98721272, -201.59831604,\n",
      "       -202.01666665, -202.0051207 , -200.07683523, -202.0530064 ,\n",
      "       -202.05992431, -202.06388884])), 'layer3': (array([[  2.28117863,  10.95565183,   0.08120881, ...,  16.66981926,\n",
      "          0.08729611,   0.15132623],\n",
      "       [  2.28117863,  10.95565183,   0.08120881, ...,  16.66981926,\n",
      "          0.08729611,   0.15132623],\n",
      "       [  2.28117863,  10.95565183,   0.08120881, ...,  16.66981926,\n",
      "          0.08729611,   0.15132623],\n",
      "       ..., \n",
      "       [  2.28117863,  10.95565183,   0.08120881, ...,  16.66981926,\n",
      "          0.08729611,   0.15132623],\n",
      "       [  2.28117863,  10.95565183,   0.08120881, ...,  16.66981926,\n",
      "          0.08729611,   0.15132623],\n",
      "       [  2.28117863,  10.95565183,   0.08120881, ...,  16.66981926,\n",
      "          0.08729611,   0.15132623]]), array([  2.98733712e+02,  -1.30524804e+03,  -1.21109888e+03,\n",
      "        -2.24177234e+03,   9.35994548e+01,  -3.22382992e+02,\n",
      "         6.06206962e+02,   5.36908222e+01,   2.46763856e+02,\n",
      "         9.12545063e+02,   2.42848055e+02,   0.00000000e+00,\n",
      "         3.03055618e+02,  -2.27947110e+03,   0.00000000e+00,\n",
      "        -1.04436603e+02,  -9.96125535e+02,   1.86654090e+00,\n",
      "         1.52365066e+03,   3.02043631e+01])), 'layer2': (array([[ 49.0904395 ,   6.42282038,  16.74216402, ...,  57.72893456,\n",
      "          9.91822526,   0.        ],\n",
      "       [ 49.0904395 ,   6.42282038,  16.74216402, ...,  57.72893456,\n",
      "          9.91822526,   0.        ],\n",
      "       [ 49.0904395 ,   6.42282038,  16.74216402, ...,  57.72893456,\n",
      "          9.91822526,   0.        ],\n",
      "       ..., \n",
      "       [ 49.0904395 ,   6.42282038,  16.74216402, ...,  57.72893456,\n",
      "          9.91822526,   0.        ],\n",
      "       [ 49.0904395 ,   6.42282038,  16.74216402, ...,  57.72893456,\n",
      "          9.91822526,   0.        ],\n",
      "       [ 49.0904395 ,   6.42282038,  16.74216402, ...,  57.72893456,\n",
      "          9.91822526,   0.        ]]), array([ -5.87359906e+02,  -1.54715110e+02,   2.50160143e+01,\n",
      "        -1.51308923e+02,   1.72745171e+03,   5.16551314e+03,\n",
      "         2.68811179e+00,  -1.57927483e+03,  -1.88140915e+03,\n",
      "        -3.52586862e+02,  -2.62323489e+03,   0.00000000e+00,\n",
      "         2.60656475e+02,   7.87686493e+02,   0.00000000e+00,\n",
      "        -2.45297073e+01,   0.00000000e+00,  -9.71095260e+02,\n",
      "        -5.31555283e+02,   1.64203516e+03,  -8.41479713e+02,\n",
      "        -6.24259392e+00,  -4.43849813e+02,  -3.90632366e+03,\n",
      "        -4.94126721e+02,   2.07006229e+02,  -1.82008483e+03,\n",
      "         0.00000000e+00,  -1.47416611e+03,   0.00000000e+00,\n",
      "         0.00000000e+00,   1.49115920e+03,   2.36632449e+01,\n",
      "         1.31075091e+02,   0.00000000e+00,   2.15564473e+02,\n",
      "        -1.92523055e+02,   2.80560966e+03,  -2.24674616e+03,\n",
      "         3.85766891e+02,   7.97804552e+02,   0.00000000e+00,\n",
      "        -5.93378044e+02,   1.53022879e+03,  -1.89984655e+03,\n",
      "         3.00488923e+02,   0.00000000e+00,  -8.66139323e+02,\n",
      "         5.38028805e+01,  -1.07104744e+02,   4.55753525e+02,\n",
      "         0.00000000e+00,  -1.50587908e+03,  -1.00810279e+01,\n",
      "         9.76852696e+02,   0.00000000e+00,   0.00000000e+00,\n",
      "         1.62347575e+03,  -1.76164072e+02,   0.00000000e+00,\n",
      "         2.25071911e+03,   5.87473189e+00,  -5.28120839e+02,\n",
      "         7.55187515e+02,  -1.17130443e+03,  -5.08262629e+02,\n",
      "        -8.55749483e+01,   0.00000000e+00,   2.26150816e+03,\n",
      "         0.00000000e+00,   2.51820677e+03,   1.54834961e+01,\n",
      "        -1.88070182e+03,  -1.19133416e+02,   1.26042063e+01,\n",
      "         0.00000000e+00,   5.91786356e+02,   1.94234325e+03,\n",
      "        -2.77911213e+02,  -8.01991987e+01,  -2.41850549e+03,\n",
      "         0.00000000e+00,   1.49330444e+02,   1.44137839e+03,\n",
      "        -1.49606000e+02,   0.00000000e+00,   1.47484240e+02,\n",
      "        -4.38305116e+02,   2.34068762e+03,  -2.55255532e+03,\n",
      "         3.16917140e+02,  -4.42430704e+01,   1.26022941e+01,\n",
      "         5.04377341e+02,  -1.10759320e+03,  -4.07994609e+03,\n",
      "        -1.51340962e+02,  -1.22761434e+03,  -2.22391651e+01,\n",
      "         2.87603752e+01])), 'layer1': (array([[ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       ..., \n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314]]), array([  4.74480103e+03,  -4.24522556e+03,   9.96397340e+02,\n",
      "         7.37972093e+03,   6.08120274e+03,   4.92352718e+02,\n",
      "        -1.62807558e+02,  -3.23033379e+03,  -8.35112719e+02,\n",
      "         0.00000000e+00,  -2.61901984e+03,   3.90008252e+03,\n",
      "         1.26066193e+03,   2.50017885e+03,  -5.13465202e+02,\n",
      "         1.67794988e+01,   3.57146421e+03,  -4.68471890e+03,\n",
      "        -1.31498702e+03,  -2.58242020e+03,   0.00000000e+00,\n",
      "         1.30772338e+02,   5.09279451e+02,  -2.12058555e+03,\n",
      "         3.78190891e+03,  -7.97253993e+02,  -2.34391302e+03,\n",
      "         2.80341369e+03,   2.14585890e+03,  -1.95890854e+03,\n",
      "        -3.40470326e+03,   2.67897891e+03,  -7.28984805e+03,\n",
      "         3.64342388e+01,  -4.26018505e+00,  -2.14442475e+03,\n",
      "        -7.37458067e+01,   4.96360513e+03,  -1.22820074e+03,\n",
      "        -1.34658983e+03,   4.22373937e+03,   7.02105154e+02,\n",
      "         1.89287163e+02,  -6.33124671e+03,   4.81821671e+03,\n",
      "         0.00000000e+00,  -3.02893099e+03,   0.00000000e+00,\n",
      "         1.77612127e+00,   0.00000000e+00,   5.10549820e+03,\n",
      "        -1.56177834e+04,  -2.46721413e+01,   0.00000000e+00,\n",
      "         6.03295353e+02,  -1.75144830e+03,   4.06253156e+02,\n",
      "         3.60563199e+03,  -1.16362378e+04,  -4.95185024e+03,\n",
      "         6.75821362e+03,  -1.17562011e+02,   5.77766365e+00,\n",
      "         1.55110651e+03,   1.20192353e+03,   1.06327265e+03,\n",
      "         4.76013595e+03,   1.04765582e+03,   1.04239933e+03,\n",
      "         1.59950558e+04,  -2.91632542e+03,   0.00000000e+00]))}\n",
      "compute: 1\n",
      "\t (72, 3072) • (3072, 50) = (72, 50)\n",
      "\t( (72, 50) + (72, 1) ).T = (50, 72)\n",
      "compute: 2\n",
      "\t (100, 72) • (72, 50) = (100, 50)\n",
      "\t( (100, 50) + (100, 1) ).T = (50, 100)\n",
      "compute: 3\n",
      "\t (20, 100) • (100, 50) = (20, 50)\n",
      "\t( (20, 50) + (20, 1) ).T = (50, 20)\n",
      "compute: 4\n",
      "\t (10, 20) • (20, 50) = (10, 50)\n",
      "\t( (10, 50) + (10, 1) ).T = (50, 10)\n",
      "dAL: (10, 50)\n",
      "cache shape: 12\n",
      "upstream grad: (10, 50)\n",
      "W shapppppeeee (10, 20)\n",
      "A shapppppeeee (20, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (20,)\n",
      "np.shape(np.sum(W.T, axis=0)) (20,)\n",
      "dW shape (10, 20)\n",
      "dA_prev shpe (10, 50)\n",
      "W_ shape (10, 20)\n",
      "dA shape (20, 50)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 20) (20, 50) (10,) \tdaprev (10, 50)\n",
      "backprop through layer 3\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "(72, 50)\n",
      "(100, 72)\n",
      "(100, 50)\n",
      "(100, 50)\n",
      "(20, 100)\n",
      "(20, 50)\n",
      "----\n",
      "s': (20, 50) dx:  (20, 50) cachedx: (20, 50)\n",
      "W shapppppeeee (20, 100)\n",
      "A shapppppeeee (100, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (20, 100)\n",
      "dA_prev shpe (20, 50)\n",
      "W_ shape (20, 100)\n",
      "dA shape (100, 50)\n",
      "W rows 20\n",
      "the shapes(w,a,b): (20, 100) (100, 50) (20,) \tdaprev (20, 50)\n",
      "cache shape: 6\n",
      "backprop through layer 2\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "(72, 50)\n",
      "(100, 72)\n",
      "(100, 50)\n",
      "----\n",
      "s': (100, 50) dx:  (100, 50) cachedx: (100, 50)\n",
      "W shapppppeeee (100, 72)\n",
      "A shapppppeeee (72, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (72,)\n",
      "np.shape(np.sum(W.T, axis=0)) (72,)\n",
      "dW shape (100, 72)\n",
      "dA_prev shpe (100, 50)\n",
      "W_ shape (100, 72)\n",
      "dA shape (72, 50)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 72) (72, 50) (100,) \tdaprev (100, 50)\n",
      "cache shape: 3\n",
      "backprop through layer 1\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "----\n",
      "s': (72, 50) dx:  (72, 50) cachedx: (72, 50)\n",
      "W shapppppeeee (72, 3072)\n",
      "A shapppppeeee (3072, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (72, 3072)\n",
      "dA_prev shpe (72, 50)\n",
      "W_ shape (72, 3072)\n",
      "dA shape (3072, 50)\n",
      "W rows 72\n",
      "the shapes(w,a,b): (72, 3072) (3072, 50) (72,) \tdaprev (72, 50)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "updating  layer3\n",
      "updating  layer4\n",
      "should print cost 1\n",
      "\n",
      "\n",
      "COST: 2.30563480793 \n",
      "\n",
      "\n",
      "{'layer4': (array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.]]), array([-201.91351645, -201.50921516, -201.51854278, -201.60725373,\n",
      "       -201.51166785, -201.51436549, -201.92015456, -201.50315482,\n",
      "       -201.50153034, -201.50059882])), 'layer3': (array([[ 2878.06353734,   758.10404088,     0.        , ...,\n",
      "         6020.2739213 ,   108.97190897,     0.        ],\n",
      "       [ 2878.06353734,   758.10404088,     0.        , ...,\n",
      "         6020.2739213 ,   108.97190897,     0.        ],\n",
      "       [ 2878.06353734,   758.10404088,     0.        , ...,\n",
      "         6020.2739213 ,   108.97190897,     0.        ],\n",
      "       ..., \n",
      "       [ 2878.06353734,   758.10404088,     0.        , ...,\n",
      "         6020.2739213 ,   108.97190897,     0.        ],\n",
      "       [ 2878.06353734,   758.10404088,     0.        , ...,\n",
      "         6020.2739213 ,   108.97190897,     0.        ],\n",
      "       [ 2878.06353734,   758.10404088,     0.        , ...,\n",
      "         6020.2739213 ,   108.97190897,     0.        ]]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.])), 'layer2': (array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])), 'layer1': (array([[ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       ..., \n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314]]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.]))}\n",
      "compute: 1\n",
      "\t (72, 3072) • (3072, 50) = (72, 50)\n",
      "\t( (72, 50) + (72, 1) ).T = (50, 72)\n",
      "compute: 2\n",
      "\t (100, 72) • (72, 50) = (100, 50)\n",
      "\t( (100, 50) + (100, 1) ).T = (50, 100)\n",
      "compute: 3\n",
      "\t (20, 100) • (100, 50) = (20, 50)\n",
      "\t( (20, 50) + (20, 1) ).T = (50, 20)\n",
      "compute: 4\n",
      "\t (10, 20) • (20, 50) = (10, 50)\n",
      "\t( (10, 50) + (10, 1) ).T = (50, 10)\n",
      "dAL: (10, 50)\n",
      "cache shape: 12\n",
      "upstream grad: (10, 50)\n",
      "W shapppppeeee (10, 20)\n",
      "A shapppppeeee (20, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (20,)\n",
      "np.shape(np.sum(W.T, axis=0)) (20,)\n",
      "dW shape (10, 20)\n",
      "dA_prev shpe (10, 50)\n",
      "W_ shape (10, 20)\n",
      "dA shape (20, 50)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 20) (20, 50) (10,) \tdaprev (10, 50)\n",
      "backprop through layer 3\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "(72, 50)\n",
      "(100, 72)\n",
      "(100, 50)\n",
      "(100, 50)\n",
      "(20, 100)\n",
      "(20, 50)\n",
      "----\n",
      "s': (20, 50) dx:  (20, 50) cachedx: (20, 50)\n",
      "W shapppppeeee (20, 100)\n",
      "A shapppppeeee (100, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (20, 100)\n",
      "dA_prev shpe (20, 50)\n",
      "W_ shape (20, 100)\n",
      "dA shape (100, 50)\n",
      "W rows 20\n",
      "the shapes(w,a,b): (20, 100) (100, 50) (20,) \tdaprev (20, 50)\n",
      "cache shape: 6\n",
      "backprop through layer 2\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "(72, 50)\n",
      "(100, 72)\n",
      "(100, 50)\n",
      "----\n",
      "s': (100, 50) dx:  (100, 50) cachedx: (100, 50)\n",
      "W shapppppeeee (100, 72)\n",
      "A shapppppeeee (72, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (72,)\n",
      "np.shape(np.sum(W.T, axis=0)) (72,)\n",
      "dW shape (100, 72)\n",
      "dA_prev shpe (100, 50)\n",
      "W_ shape (100, 72)\n",
      "dA shape (72, 50)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 72) (72, 50) (100,) \tdaprev (100, 50)\n",
      "cache shape: 3\n",
      "backprop through layer 1\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "----\n",
      "s': (72, 50) dx:  (72, 50) cachedx: (72, 50)\n",
      "W shapppppeeee (72, 3072)\n",
      "A shapppppeeee (3072, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (72, 3072)\n",
      "dA_prev shpe (72, 50)\n",
      "W_ shape (72, 3072)\n",
      "dA shape (3072, 50)\n",
      "W rows 72\n",
      "the shapes(w,a,b): (72, 3072) (3072, 50) (72,) \tdaprev (72, 50)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "updating  layer3\n",
      "updating  layer4\n",
      "should print cost 2\n",
      "\n",
      "\n",
      "COST: 2.30501432895 \n",
      "\n",
      "\n",
      "{'layer4': (array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.]]), array([-201.85071887, -201.52771485, -201.53480466, -201.60307772,\n",
      "       -201.52957748, -201.53162747, -201.85629449, -201.52311748,\n",
      "       -201.52188635, -201.52118062])), 'layer3': (array([[ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       ..., \n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ]]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.])), 'layer2': (array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])), 'layer1': (array([[ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       ..., \n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314]]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.]))}\n",
      "compute: 1\n",
      "\t (72, 3072) • (3072, 50) = (72, 50)\n",
      "\t( (72, 50) + (72, 1) ).T = (50, 72)\n",
      "compute: 2\n",
      "\t (100, 72) • (72, 50) = (100, 50)\n",
      "\t( (100, 50) + (100, 1) ).T = (50, 100)\n",
      "compute: 3\n",
      "\t (20, 100) • (100, 50) = (20, 50)\n",
      "\t( (20, 50) + (20, 1) ).T = (50, 20)\n",
      "compute: 4\n",
      "\t (10, 20) • (20, 50) = (10, 50)\n",
      "\t( (10, 50) + (10, 1) ).T = (50, 10)\n",
      "dAL: (10, 50)\n",
      "cache shape: 12\n",
      "upstream grad: (10, 50)\n",
      "W shapppppeeee (10, 20)\n",
      "A shapppppeeee (20, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (20,)\n",
      "np.shape(np.sum(W.T, axis=0)) (20,)\n",
      "dW shape (10, 20)\n",
      "dA_prev shpe (10, 50)\n",
      "W_ shape (10, 20)\n",
      "dA shape (20, 50)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 20) (20, 50) (10,) \tdaprev (10, 50)\n",
      "backprop through layer 3\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "(72, 50)\n",
      "(100, 72)\n",
      "(100, 50)\n",
      "(100, 50)\n",
      "(20, 100)\n",
      "(20, 50)\n",
      "----\n",
      "s': (20, 50) dx:  (20, 50) cachedx: (20, 50)\n",
      "W shapppppeeee (20, 100)\n",
      "A shapppppeeee (100, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (20, 100)\n",
      "dA_prev shpe (20, 50)\n",
      "W_ shape (20, 100)\n",
      "dA shape (100, 50)\n",
      "W rows 20\n",
      "the shapes(w,a,b): (20, 100) (100, 50) (20,) \tdaprev (20, 50)\n",
      "cache shape: 6\n",
      "backprop through layer 2\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "(72, 50)\n",
      "(100, 72)\n",
      "(100, 50)\n",
      "----\n",
      "s': (100, 50) dx:  (100, 50) cachedx: (100, 50)\n",
      "W shapppppeeee (100, 72)\n",
      "A shapppppeeee (72, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (72,)\n",
      "np.shape(np.sum(W.T, axis=0)) (72,)\n",
      "dW shape (100, 72)\n",
      "dA_prev shpe (100, 50)\n",
      "W_ shape (100, 72)\n",
      "dA shape (72, 50)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 72) (72, 50) (100,) \tdaprev (100, 50)\n",
      "cache shape: 3\n",
      "backprop through layer 1\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "----\n",
      "s': (72, 50) dx:  (72, 50) cachedx: (72, 50)\n",
      "W shapppppeeee (72, 3072)\n",
      "A shapppppeeee (3072, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (72, 3072)\n",
      "dA_prev shpe (72, 50)\n",
      "W_ shape (72, 3072)\n",
      "dA shape (3072, 50)\n",
      "W rows 72\n",
      "the shapes(w,a,b): (72, 3072) (3072, 50) (72,) \tdaprev (72, 50)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "updating  layer3\n",
      "updating  layer4\n",
      "should print cost 3\n",
      "\n",
      "\n",
      "COST: 2.30450971014 \n",
      "\n",
      "\n",
      "{'layer4': (array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.]]), array([-201.79926372, -201.54274437, -201.54815498, -201.60075809,\n",
      "       -201.5441649 , -201.54572909, -201.80387378, -201.53924107,\n",
      "       -201.53830362, -201.53776637])), 'layer3': (array([[ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       ..., \n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ]]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.])), 'layer2': (array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])), 'layer1': (array([[ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       ..., \n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314]]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.]))}\n",
      "compute: 1\n",
      "\t (72, 3072) • (3072, 50) = (72, 50)\n",
      "\t( (72, 50) + (72, 1) ).T = (50, 72)\n",
      "compute: 2\n",
      "\t (100, 72) • (72, 50) = (100, 50)\n",
      "\t( (100, 50) + (100, 1) ).T = (50, 100)\n",
      "compute: 3\n",
      "\t (20, 100) • (100, 50) = (20, 50)\n",
      "\t( (20, 50) + (20, 1) ).T = (50, 20)\n",
      "compute: 4\n",
      "\t (10, 20) • (20, 50) = (10, 50)\n",
      "\t( (10, 50) + (10, 1) ).T = (50, 10)\n",
      "dAL: (10, 50)\n",
      "cache shape: 12\n",
      "upstream grad: (10, 50)\n",
      "W shapppppeeee (10, 20)\n",
      "A shapppppeeee (20, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (20,)\n",
      "np.shape(np.sum(W.T, axis=0)) (20,)\n",
      "dW shape (10, 20)\n",
      "dA_prev shpe (10, 50)\n",
      "W_ shape (10, 20)\n",
      "dA shape (20, 50)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 20) (20, 50) (10,) \tdaprev (10, 50)\n",
      "backprop through layer 3\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "(72, 50)\n",
      "(100, 72)\n",
      "(100, 50)\n",
      "(100, 50)\n",
      "(20, 100)\n",
      "(20, 50)\n",
      "----\n",
      "s': (20, 50) dx:  (20, 50) cachedx: (20, 50)\n",
      "W shapppppeeee (20, 100)\n",
      "A shapppppeeee (100, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (20, 100)\n",
      "dA_prev shpe (20, 50)\n",
      "W_ shape (20, 100)\n",
      "dA shape (100, 50)\n",
      "W rows 20\n",
      "the shapes(w,a,b): (20, 100) (100, 50) (20,) \tdaprev (20, 50)\n",
      "cache shape: 6\n",
      "backprop through layer 2\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "(72, 50)\n",
      "(100, 72)\n",
      "(100, 50)\n",
      "----\n",
      "s': (100, 50) dx:  (100, 50) cachedx: (100, 50)\n",
      "W shapppppeeee (100, 72)\n",
      "A shapppppeeee (72, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (72,)\n",
      "np.shape(np.sum(W.T, axis=0)) (72,)\n",
      "dW shape (100, 72)\n",
      "dA_prev shpe (100, 50)\n",
      "W_ shape (100, 72)\n",
      "dA shape (72, 50)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 72) (72, 50) (100,) \tdaprev (100, 50)\n",
      "cache shape: 3\n",
      "backprop through layer 1\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "----\n",
      "s': (72, 50) dx:  (72, 50) cachedx: (72, 50)\n",
      "W shapppppeeee (72, 3072)\n",
      "A shapppppeeee (3072, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (72, 3072)\n",
      "dA_prev shpe (72, 50)\n",
      "W_ shape (72, 3072)\n",
      "dA shape (3072, 50)\n",
      "W rows 72\n",
      "the shapes(w,a,b): (72, 3072) (3072, 50) (72,) \tdaprev (72, 50)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "updating  layer3\n",
      "updating  layer4\n",
      "should print cost 4\n",
      "\n",
      "\n",
      "COST: 2.30410329177 \n",
      "\n",
      "\n",
      "{'layer4': (array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.]]), array([-201.75757759, -201.55484135, -201.55898432, -201.59956004,\n",
      "       -201.55592851, -201.55712609, -201.7613403 , -201.55216187,\n",
      "       -201.55144527, -201.55103467])), 'layer3': (array([[ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       ..., \n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ]]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.])), 'layer2': (array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])), 'layer1': (array([[ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       ..., \n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314]]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.]))}\n",
      "compute: 1\n",
      "\t (72, 3072) • (3072, 50) = (72, 50)\n",
      "\t( (72, 50) + (72, 1) ).T = (50, 72)\n",
      "compute: 2\n",
      "\t (100, 72) • (72, 50) = (100, 50)\n",
      "\t( (100, 50) + (100, 1) ).T = (50, 100)\n",
      "compute: 3\n",
      "\t (20, 100) • (100, 50) = (20, 50)\n",
      "\t( (20, 50) + (20, 1) ).T = (50, 20)\n",
      "compute: 4\n",
      "\t (10, 20) • (20, 50) = (10, 50)\n",
      "\t( (10, 50) + (10, 1) ).T = (50, 10)\n",
      "dAL: (10, 50)\n",
      "cache shape: 12\n",
      "upstream grad: (10, 50)\n",
      "W shapppppeeee (10, 20)\n",
      "A shapppppeeee (20, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (20,)\n",
      "np.shape(np.sum(W.T, axis=0)) (20,)\n",
      "dW shape (10, 20)\n",
      "dA_prev shpe (10, 50)\n",
      "W_ shape (10, 20)\n",
      "dA shape (20, 50)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 20) (20, 50) (10,) \tdaprev (10, 50)\n",
      "backprop through layer 3\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "(72, 50)\n",
      "(100, 72)\n",
      "(100, 50)\n",
      "(100, 50)\n",
      "(20, 100)\n",
      "(20, 50)\n",
      "----\n",
      "s': (20, 50) dx:  (20, 50) cachedx: (20, 50)\n",
      "W shapppppeeee (20, 100)\n",
      "A shapppppeeee (100, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (20, 100)\n",
      "dA_prev shpe (20, 50)\n",
      "W_ shape (20, 100)\n",
      "dA shape (100, 50)\n",
      "W rows 20\n",
      "the shapes(w,a,b): (20, 100) (100, 50) (20,) \tdaprev (20, 50)\n",
      "cache shape: 6\n",
      "backprop through layer 2\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "(72, 50)\n",
      "(100, 72)\n",
      "(100, 50)\n",
      "----\n",
      "s': (100, 50) dx:  (100, 50) cachedx: (100, 50)\n",
      "W shapppppeeee (100, 72)\n",
      "A shapppppeeee (72, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (72,)\n",
      "np.shape(np.sum(W.T, axis=0)) (72,)\n",
      "dW shape (100, 72)\n",
      "dA_prev shpe (100, 50)\n",
      "W_ shape (100, 72)\n",
      "dA shape (72, 50)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 72) (72, 50) (100,) \tdaprev (100, 50)\n",
      "cache shape: 3\n",
      "backprop through layer 1\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "----\n",
      "s': (72, 50) dx:  (72, 50) cachedx: (72, 50)\n",
      "W shapppppeeee (72, 3072)\n",
      "A shapppppeeee (3072, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (72, 3072)\n",
      "dA_prev shpe (72, 50)\n",
      "W_ shape (72, 3072)\n",
      "dA shape (3072, 50)\n",
      "W rows 72\n",
      "the shapes(w,a,b): (72, 3072) (3072, 50) (72,) \tdaprev (72, 50)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "updating  layer3\n",
      "updating  layer4\n",
      "should print cost 5\n",
      "\n",
      "\n",
      "COST: 2.30377852293 \n",
      "\n",
      "\n",
      "{'layer4': (array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.]]), array([-201.72411249, -201.56450438, -201.5676854 , -201.59901662,\n",
      "       -201.56533879, -201.56625821, -201.72715148, -201.56244882,\n",
      "       -201.56189931, -201.5615845 ])), 'layer3': (array([[ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       ..., \n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ]]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.])), 'layer2': (array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])), 'layer1': (array([[ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       ..., \n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314]]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.]))}\n",
      "compute: 1\n",
      "\t (72, 3072) • (3072, 50) = (72, 50)\n",
      "\t( (72, 50) + (72, 1) ).T = (50, 72)\n",
      "compute: 2\n",
      "\t (100, 72) • (72, 50) = (100, 50)\n",
      "\t( (100, 50) + (100, 1) ).T = (50, 100)\n",
      "compute: 3\n",
      "\t (20, 100) • (100, 50) = (20, 50)\n",
      "\t( (20, 50) + (20, 1) ).T = (50, 20)\n",
      "compute: 4\n",
      "\t (10, 20) • (20, 50) = (10, 50)\n",
      "\t( (10, 50) + (10, 1) ).T = (50, 10)\n",
      "dAL: (10, 50)\n",
      "cache shape: 12\n",
      "upstream grad: (10, 50)\n",
      "W shapppppeeee (10, 20)\n",
      "A shapppppeeee (20, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (20,)\n",
      "np.shape(np.sum(W.T, axis=0)) (20,)\n",
      "dW shape (10, 20)\n",
      "dA_prev shpe (10, 50)\n",
      "W_ shape (10, 20)\n",
      "dA shape (20, 50)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 20) (20, 50) (10,) \tdaprev (10, 50)\n",
      "backprop through layer 3\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "(72, 50)\n",
      "(100, 72)\n",
      "(100, 50)\n",
      "(100, 50)\n",
      "(20, 100)\n",
      "(20, 50)\n",
      "----\n",
      "s': (20, 50) dx:  (20, 50) cachedx: (20, 50)\n",
      "W shapppppeeee (20, 100)\n",
      "A shapppppeeee (100, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (20, 100)\n",
      "dA_prev shpe (20, 50)\n",
      "W_ shape (20, 100)\n",
      "dA shape (100, 50)\n",
      "W rows 20\n",
      "the shapes(w,a,b): (20, 100) (100, 50) (20,) \tdaprev (20, 50)\n",
      "cache shape: 6\n",
      "backprop through layer 2\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "(72, 50)\n",
      "(100, 72)\n",
      "(100, 50)\n",
      "----\n",
      "s': (100, 50) dx:  (100, 50) cachedx: (100, 50)\n",
      "W shapppppeeee (100, 72)\n",
      "A shapppppeeee (72, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (72,)\n",
      "np.shape(np.sum(W.T, axis=0)) (72,)\n",
      "dW shape (100, 72)\n",
      "dA_prev shpe (100, 50)\n",
      "W_ shape (100, 72)\n",
      "dA shape (72, 50)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 72) (72, 50) (100,) \tdaprev (100, 50)\n",
      "cache shape: 3\n",
      "backprop through layer 1\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "----\n",
      "s': (72, 50) dx:  (72, 50) cachedx: (72, 50)\n",
      "W shapppppeeee (72, 3072)\n",
      "A shapppppeeee (3072, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (72, 3072)\n",
      "dA_prev shpe (72, 50)\n",
      "W_ shape (72, 3072)\n",
      "dA shape (3072, 50)\n",
      "W rows 72\n",
      "the shapes(w,a,b): (72, 3072) (3072, 50) (72,) \tdaprev (72, 50)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "updating  layer3\n",
      "updating  layer4\n",
      "should print cost 6\n",
      "\n",
      "\n",
      "COST: 2.30352062396 \n",
      "\n",
      "\n",
      "{'layer4': (array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.]]), array([-201.69744207, -201.57217611, -201.57462396, -201.59883928,\n",
      "       -201.57281801, -201.57352547, -201.69987587, -201.57059536,\n",
      "       -201.57017293, -201.56993094])), 'layer3': (array([[ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       ..., \n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ]]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.])), 'layer2': (array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])), 'layer1': (array([[ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       ..., \n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314]]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.]))}\n",
      "compute: 1\n",
      "\t (72, 3072) • (3072, 50) = (72, 50)\n",
      "\t( (72, 50) + (72, 1) ).T = (50, 72)\n",
      "compute: 2\n",
      "\t (100, 72) • (72, 50) = (100, 50)\n",
      "\t( (100, 50) + (100, 1) ).T = (50, 100)\n",
      "compute: 3\n",
      "\t (20, 100) • (100, 50) = (20, 50)\n",
      "\t( (20, 50) + (20, 1) ).T = (50, 20)\n",
      "compute: 4\n",
      "\t (10, 20) • (20, 50) = (10, 50)\n",
      "\t( (10, 50) + (10, 1) ).T = (50, 10)\n",
      "dAL: (10, 50)\n",
      "cache shape: 12\n",
      "upstream grad: (10, 50)\n",
      "W shapppppeeee (10, 20)\n",
      "A shapppppeeee (20, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (20,)\n",
      "np.shape(np.sum(W.T, axis=0)) (20,)\n",
      "dW shape (10, 20)\n",
      "dA_prev shpe (10, 50)\n",
      "W_ shape (10, 20)\n",
      "dA shape (20, 50)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 20) (20, 50) (10,) \tdaprev (10, 50)\n",
      "backprop through layer 3\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "(72, 50)\n",
      "(100, 72)\n",
      "(100, 50)\n",
      "(100, 50)\n",
      "(20, 100)\n",
      "(20, 50)\n",
      "----\n",
      "s': (20, 50) dx:  (20, 50) cachedx: (20, 50)\n",
      "W shapppppeeee (20, 100)\n",
      "A shapppppeeee (100, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (20, 100)\n",
      "dA_prev shpe (20, 50)\n",
      "W_ shape (20, 100)\n",
      "dA shape (100, 50)\n",
      "W rows 20\n",
      "the shapes(w,a,b): (20, 100) (100, 50) (20,) \tdaprev (20, 50)\n",
      "cache shape: 6\n",
      "backprop through layer 2\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "(72, 50)\n",
      "(100, 72)\n",
      "(100, 50)\n",
      "----\n",
      "s': (100, 50) dx:  (100, 50) cachedx: (100, 50)\n",
      "W shapppppeeee (100, 72)\n",
      "A shapppppeeee (72, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (72,)\n",
      "np.shape(np.sum(W.T, axis=0)) (72,)\n",
      "dW shape (100, 72)\n",
      "dA_prev shpe (100, 50)\n",
      "W_ shape (100, 72)\n",
      "dA shape (72, 50)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 72) (72, 50) (100,) \tdaprev (100, 50)\n",
      "cache shape: 3\n",
      "backprop through layer 1\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "----\n",
      "s': (72, 50) dx:  (72, 50) cachedx: (72, 50)\n",
      "W shapppppeeee (72, 3072)\n",
      "A shapppppeeee (3072, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (72, 3072)\n",
      "dA_prev shpe (72, 50)\n",
      "W_ shape (72, 3072)\n",
      "dA shape (3072, 50)\n",
      "W rows 72\n",
      "the shapes(w,a,b): (72, 3072) (3072, 50) (72,) \tdaprev (72, 50)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "updating  layer3\n",
      "updating  layer4\n",
      "should print cost 7\n",
      "\n",
      "\n",
      "COST: 2.30331684476 \n",
      "\n",
      "\n",
      "{'layer4': (array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.]]), array([-201.67630917, -201.57823719, -201.58012423, -201.59885454,\n",
      "       -201.57873192, -201.57927726, -201.67824518, -201.57701923,\n",
      "       -201.57669383, -201.57650744])), 'layer3': (array([[ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       ..., \n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ]]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.])), 'layer2': (array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])), 'layer1': (array([[ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       ..., \n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314]]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.]))}\n",
      "compute: 1\n",
      "\t (72, 3072) • (3072, 50) = (72, 50)\n",
      "\t( (72, 50) + (72, 1) ).T = (50, 72)\n",
      "compute: 2\n",
      "\t (100, 72) • (72, 50) = (100, 50)\n",
      "\t( (100, 50) + (100, 1) ).T = (50, 100)\n",
      "compute: 3\n",
      "\t (20, 100) • (100, 50) = (20, 50)\n",
      "\t( (20, 50) + (20, 1) ).T = (50, 20)\n",
      "compute: 4\n",
      "\t (10, 20) • (20, 50) = (10, 50)\n",
      "\t( (10, 50) + (10, 1) ).T = (50, 10)\n",
      "dAL: (10, 50)\n",
      "cache shape: 12\n",
      "upstream grad: (10, 50)\n",
      "W shapppppeeee (10, 20)\n",
      "A shapppppeeee (20, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (20,)\n",
      "np.shape(np.sum(W.T, axis=0)) (20,)\n",
      "dW shape (10, 20)\n",
      "dA_prev shpe (10, 50)\n",
      "W_ shape (10, 20)\n",
      "dA shape (20, 50)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 20) (20, 50) (10,) \tdaprev (10, 50)\n",
      "backprop through layer 3\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "(72, 50)\n",
      "(100, 72)\n",
      "(100, 50)\n",
      "(100, 50)\n",
      "(20, 100)\n",
      "(20, 50)\n",
      "----\n",
      "s': (20, 50) dx:  (20, 50) cachedx: (20, 50)\n",
      "W shapppppeeee (20, 100)\n",
      "A shapppppeeee (100, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (20, 100)\n",
      "dA_prev shpe (20, 50)\n",
      "W_ shape (20, 100)\n",
      "dA shape (100, 50)\n",
      "W rows 20\n",
      "the shapes(w,a,b): (20, 100) (100, 50) (20,) \tdaprev (20, 50)\n",
      "cache shape: 6\n",
      "backprop through layer 2\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "(72, 50)\n",
      "(100, 72)\n",
      "(100, 50)\n",
      "----\n",
      "s': (100, 50) dx:  (100, 50) cachedx: (100, 50)\n",
      "W shapppppeeee (100, 72)\n",
      "A shapppppeeee (72, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (72,)\n",
      "np.shape(np.sum(W.T, axis=0)) (72,)\n",
      "dW shape (100, 72)\n",
      "dA_prev shpe (100, 50)\n",
      "W_ shape (100, 72)\n",
      "dA shape (72, 50)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 72) (72, 50) (100,) \tdaprev (100, 50)\n",
      "cache shape: 3\n",
      "backprop through layer 1\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "----\n",
      "s': (72, 50) dx:  (72, 50) cachedx: (72, 50)\n",
      "W shapppppeeee (72, 3072)\n",
      "A shapppppeeee (3072, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (72, 3072)\n",
      "dA_prev shpe (72, 50)\n",
      "W_ shape (72, 3072)\n",
      "dA shape (3072, 50)\n",
      "W rows 72\n",
      "the shapes(w,a,b): (72, 3072) (3072, 50) (72,) \tdaprev (72, 50)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "updating  layer3\n",
      "updating  layer4\n",
      "should print cost 8\n",
      "\n",
      "\n",
      "COST: 2.30315646131 \n",
      "\n",
      "\n",
      "{'layer4': (array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.]]), array([-201.65964021, -201.58300724, -201.584464  , -201.59896122,\n",
      "       -201.5833891 , -201.58381007, -201.66117201, -201.58206736,\n",
      "       -201.5818163 , -201.5816725 ])), 'layer3': (array([[ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       ..., \n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ]]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.])), 'layer2': (array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])), 'layer1': (array([[ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       ..., \n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314]]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.]))}\n",
      "compute: 1\n",
      "\t (72, 3072) • (3072, 50) = (72, 50)\n",
      "\t( (72, 50) + (72, 1) ).T = (50, 72)\n",
      "compute: 2\n",
      "\t (100, 72) • (72, 50) = (100, 50)\n",
      "\t( (100, 50) + (100, 1) ).T = (50, 100)\n",
      "compute: 3\n",
      "\t (20, 100) • (100, 50) = (20, 50)\n",
      "\t( (20, 50) + (20, 1) ).T = (50, 20)\n",
      "compute: 4\n",
      "\t (10, 20) • (20, 50) = (10, 50)\n",
      "\t( (10, 50) + (10, 1) ).T = (50, 10)\n",
      "dAL: (10, 50)\n",
      "cache shape: 12\n",
      "upstream grad: (10, 50)\n",
      "W shapppppeeee (10, 20)\n",
      "A shapppppeeee (20, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (20,)\n",
      "np.shape(np.sum(W.T, axis=0)) (20,)\n",
      "dW shape (10, 20)\n",
      "dA_prev shpe (10, 50)\n",
      "W_ shape (10, 20)\n",
      "dA shape (20, 50)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 20) (20, 50) (10,) \tdaprev (10, 50)\n",
      "backprop through layer 3\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "(72, 50)\n",
      "(100, 72)\n",
      "(100, 50)\n",
      "(100, 50)\n",
      "(20, 100)\n",
      "(20, 50)\n",
      "----\n",
      "s': (20, 50) dx:  (20, 50) cachedx: (20, 50)\n",
      "W shapppppeeee (20, 100)\n",
      "A shapppppeeee (100, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (20, 100)\n",
      "dA_prev shpe (20, 50)\n",
      "W_ shape (20, 100)\n",
      "dA shape (100, 50)\n",
      "W rows 20\n",
      "the shapes(w,a,b): (20, 100) (100, 50) (20,) \tdaprev (20, 50)\n",
      "cache shape: 6\n",
      "backprop through layer 2\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "(72, 50)\n",
      "(100, 72)\n",
      "(100, 50)\n",
      "----\n",
      "s': (100, 50) dx:  (100, 50) cachedx: (100, 50)\n",
      "W shapppppeeee (100, 72)\n",
      "A shapppppeeee (72, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (72,)\n",
      "np.shape(np.sum(W.T, axis=0)) (72,)\n",
      "dW shape (100, 72)\n",
      "dA_prev shpe (100, 50)\n",
      "W_ shape (100, 72)\n",
      "dA shape (72, 50)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 72) (72, 50) (100,) \tdaprev (100, 50)\n",
      "cache shape: 3\n",
      "backprop through layer 1\n",
      "(3072, 50)\n",
      "(72, 3072)\n",
      "(72, 50)\n",
      "----\n",
      "s': (72, 50) dx:  (72, 50) cachedx: (72, 50)\n",
      "W shapppppeeee (72, 3072)\n",
      "A shapppppeeee (3072, 50)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (72, 3072)\n",
      "dA_prev shpe (72, 50)\n",
      "W_ shape (72, 3072)\n",
      "dA shape (3072, 50)\n",
      "W rows 72\n",
      "the shapes(w,a,b): (72, 3072) (3072, 50) (72,) \tdaprev (72, 50)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "updating  layer3\n",
      "updating  layer4\n",
      "should print cost 9\n",
      "\n",
      "\n",
      "COST: 2.30303062293 \n",
      "\n",
      "\n",
      "{'layer4': (array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.]]), array([-201.64653931, -201.58674975, -201.58787561, -201.59910243,\n",
      "       -201.58704482, -201.58737017, -201.64774617, -201.58602357,\n",
      "       -201.58582963, -201.58571855])), 'layer3': (array([[ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       ..., \n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ],\n",
      "       [ 2936.7995279 ,   773.57555192,     0.        , ...,\n",
      "         6138.07168886,   111.19582548,     0.        ]]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.])), 'layer2': (array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])), 'layer1': (array([[ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       ..., \n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314],\n",
      "       [ 23.85098039,  24.3254902 ,  23.82745098, ...,  25.50588235,\n",
      "         24.41176471,  22.00784314]]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.]))}\n"
     ]
    }
   ],
   "source": [
    "n = 50\n",
    "\n",
    "layer_dimensions = [X_train.shape[0], 72, 100, 20, 10]  # including the input and output layers\n",
    "NN = NeuralNetwork(layer_dimensions)\n",
    "NN.train(X_train[:, :n], y_train[:n], iters=10, alpha=0.1, batch_size=128, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Functions to load data, DO NOT change these\n",
    "\n",
    "def get_labels(folder, label2id):\n",
    "    \"\"\"\n",
    "    Returns vector of labels extracted from filenames of all files in folder\n",
    "    :param folder: path to data folder\n",
    "    :param label2id: mapping of text labels to numeric ids. (Eg: automobile -> 0)\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    y = []\n",
    "    for f in files:\n",
    "        y.append(get_label(f,label2id))\n",
    "    return np.array(y)\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts each label index in y to vector with one_hot encoding\n",
    "    One-hot encoding converts categorical labels to binary values\n",
    "    \"\"\"\n",
    "    y_one_hot = np.zeros((y.shape[0], num_classes))\n",
    "    y_one_hot[y] = 1\n",
    "    return y_one_hot.T\n",
    "\n",
    "def get_label_mapping(label_file):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        id2label = f.readlines()\n",
    "        id2label = [l.strip() for l in id2label]\n",
    "    label2id = {}\n",
    "    count = 0\n",
    "    for label in id2label:\n",
    "        label2id[label] = count\n",
    "        count += 1\n",
    "    return id2label, label2id\n",
    "\n",
    "def get_images(folder):\n",
    "    \"\"\"\n",
    "    returns numpy array of all samples in folder\n",
    "    each column is a sample resized to 30x30 and flattened\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    images = []\n",
    "    count = 0\n",
    "    \n",
    "    for f in files:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "        img_arr = get_img_array(f)\n",
    "        img_arr = img_arr.flatten() / 255.0\n",
    "        images.append(img_arr)\n",
    "    X = np.column_stack(images)\n",
    "\n",
    "    return X\n",
    "\n",
    "def get_train_data(data_root_path):\n",
    "    \"\"\"\n",
    "    Return X and y\n",
    "    \"\"\"\n",
    "    train_data_path = data_root_path + 'train'\n",
    "    id2label, label2id = get_label_mapping(data_root_path+'labels.txt')\n",
    "    print(label2id)\n",
    "    X = get_images(train_data_path)\n",
    "    y = get_labels(train_data_path, label2id)\n",
    "    return X, y\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    \"\"\"\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Helper functions, DO NOT modify this\n",
    "\n",
    "def get_img_array(path):\n",
    "    \"\"\"\n",
    "    Given path of image, returns it's numpy array\n",
    "    \"\"\"\n",
    "    return scipy.misc.imread(path)\n",
    "\n",
    "def get_files(folder):\n",
    "    \"\"\"\n",
    "    Given path to folder, returns list of files in it\n",
    "    \"\"\"\n",
    "    filenames = [file for file in glob.glob(folder+'*/*')]\n",
    "    filenames.sort()\n",
    "    return filenames\n",
    "\n",
    "def get_label(filepath, label2id):\n",
    "    \"\"\"\n",
    "    Files are assumed to be labeled as: /path/to/file/999_frog.png\n",
    "    Returns label for a filepath\n",
    "    \"\"\"\n",
    "    tokens = filepath.split('/')\n",
    "    label = tokens[-1].split('_')[1][:-4]\n",
    "    if label in label2id:\n",
    "        return label2id[label]\n",
    "    else:\n",
    "        sys.exit(\"Invalid label: \" + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel/__main__.py:7: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000/50000\n",
      "Loaded 20000/50000\n",
      "Loaded 30000/50000\n",
      "Loaded 40000/50000\n",
      "Loaded 50000/50000\n",
      "Loaded 10000/10000\n",
      "Data loading done\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_root_path = 'cifar10-hw1/'\n",
    "X_train, y_train = get_train_data(data_root_path) # this may take a few minutes\n",
    "X_test = get_images(data_root_path + 'test')\n",
    "print('Data loading done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### run some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight and bias dimentions and norms\n",
      "[['layer1', (500, 100), (500,)], ['layer2', (100, 500), (100,)], ['layer3', (30, 100), (30,)], ['layer4', (10, 30), (10,)]]\n",
      "[(166.89464867576024, 0.0), (165.33499556380119, 0.0), (47.252557570903775, 0.0), (16.241798557370469, 0.0)]\n",
      "\n",
      "test affineForward\n",
      "\t (2, 2) • (2, 2) = (2, 2)\n",
      "\t( (2, 2) + (2, 1) ).T = (2, 2)\n",
      "(array([[4, 1],\n",
      "       [2, 2]]), [array([[1, 0],\n",
      "       [0, 1]]), array([[4, 1],\n",
      "       [2, 2]]), array([[4, 1],\n",
      "       [2, 2]])])\n",
      "(2,)\n",
      "\t (2, 2) • (2,) = (2,)\n",
      "\t( (2,) + (2, 1) ).T = (2, 2)\n",
      "(array([[5, 4],\n",
      "       [5, 4]]), [array([1, 1]), array([[4, 1],\n",
      "       [2, 2]]), array([[5, 4],\n",
      "       [5, 4]])])\n",
      "\n",
      "test activation\n",
      "[0.1, 0.2, -0.2, 1, -1, -1, -1, 1, 1, 1, 2]\n",
      "[ 0.1  0.2  0.   1.   0.   0.   0.   1.   1.   1.   2. ]\n",
      "[ 0.1  0.2  0.   1.   0.   0.   0.   1.   1.   1.   2. ]\n",
      "\n",
      "\n",
      "\n",
      "test forward prop\n",
      "compute: 0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'layer0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6b8cf85c25fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\ntest forward prop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforwardPropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nAL:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-96a13901374f>\u001b[0m in \u001b[0;36mforwardPropagation\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;31m#             W = self.parameters[\"w\"+str(i)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;31m#             b = self.parameters[\"b\"+str(i)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"layer\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffineForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'layer0'"
     ]
    }
   ],
   "source": [
    "layer_dimensions = [X_train.shape[0], 100, 500, 100, 30, 10]  # including the input and output layers\n",
    "NN = NeuralNetwork(layer_dimensions)\n",
    "# NN.train(X_train, y_train, iters=10, alpha=0, batch_size=128, print_every=10)\n",
    "\n",
    "# print([k for k in NN.parameters.keys()])\n",
    "# print([v.shape for v in NN.parameters.values()])\n",
    "\n",
    "print(\"weight and bias dimentions and norms\")\n",
    "print([[k, v[0].shape, v[1].shape] for k,v in NN.parameters.items()])\n",
    "print([( np.sum(v[0] ** 2), np.sum(v[1] ** 2)) for k,v in NN.parameters.items()])\n",
    "print()\n",
    "\n",
    "print(\"test affineForward\")\n",
    "A = np.array([[1, 0], [0, 1]])\n",
    "W = np.array([[4, 1], \n",
    "     [2, 2]])\n",
    "print(NN.affineForward(A, W, np.array([0,0]).T))\n",
    "A = np.array([1,1])\n",
    "print(A.shape)\n",
    "print(NN.affineForward(A, W, np.array([0,0])))\n",
    "\n",
    "print(\"\\ntest activation\")\n",
    "z = [.1, .2, -.2, 1, -1, -1, -1, 1, 1, 1, 2]\n",
    "print(z)\n",
    "print(NN.relu(z))\n",
    "print(NN.activationForward(z))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_train[:, 0].shape)\n",
    "# print(X_train[1].shape)\n",
    "\n",
    "\n",
    "print(\"\\ntest forward prop\")\n",
    "AL, cache = NN.forwardPropagation(X_train[:, :10000])\n",
    "print(\"\\nAL:\", AL.shape)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"test cost\")\n",
    "cost, dAL = NN.costFunction(AL, y_train[:10000])\n",
    "print(\"cost: (\", cost, \"?=\", 2.3, \")  (\", np.exp(-cost), \"?=\", 0.1,\")\") # cost \\approx    2.3  \\approx    -log(1/10)     => :-)\n",
    "print(\"\\t/\\ should be about 2.3 and .1\\n\\n\")\n",
    "\n",
    "y_hat = NN.predict(X_train[:, :10000])\n",
    "\n",
    "print(\"\\n\\ntest predict\\n\")\n",
    "print(\"predict: \\n\", y_hat.shape)\n",
    "\n",
    "print(\"\\nacc: \", np.mean(np.argmax(y_hat, axis=0) == y_train[:10000]))\n",
    "print(\"\\t/\\ should be about .1\")\n",
    "\n",
    "print(\"\\n\\n\\n\", np.row_stack( (y_hat[:, :5], np.sum( y_hat[:, :5], axis=0)) ) )  # last row = 1  => :-)\n",
    "print(\"\\t/\\ should all add to 1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### check the distribution on the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_hat.shape\n",
    "p = np.argmax(y_hat, axis = 0)\n",
    "print(p.shape)\n",
    "print(\"y_\", y_hat[:, 1000])\n",
    "\n",
    "print(np.argmax(y_hat[:, 1000]))\n",
    "print(np.max(y_hat[:, 1000]))\n",
    "\n",
    "print(\"p:\", p[1000])\n",
    "plt.hist(p)\n",
    "\n",
    "print(\"mean:\", np.mean(y_hat, axis=1))\n",
    "prob_mass = np.sum(y_hat, axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(prob_mass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### tests for backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"test cost\")\n",
    "cost, dAL = NN.costFunction(AL, y_train[:10000])\n",
    "print(\"cost: {0:.2f}\".format(cost), \"dAL:\", dAL)\n",
    "\n",
    "\n",
    "# NN.affineBackward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 39\n",
    "n = 5\n",
    "print(y_train[i:i+n])\n",
    "print(dAL[:, i:i+n])\n",
    "print(np.argmax(y_hat[:, i:i+n], axis=0))\n",
    "print(1*(np.equal(y_train[i:i+n], np.argmax(y_hat[:, i:i+n], axis=0))))\n",
    "\n",
    "np.sum( np.equal(y_train[:10000], np.argmax(y_hat, axis=0) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Split train into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# X_train = ...\n",
    "# y_train = ...\n",
    "\n",
    "# X_val   \n",
    "# y_val1\n",
    "\n",
    "idx = np.random.permutation(len(y_train))\n",
    "X_all = X_train[:, idx]\n",
    "y_all = y_train[idx]\n",
    "\n",
    "m_train = int(len(y_all)*0.9);\n",
    "m_val   = len(y_all)-m_train\n",
    "\n",
    "X       = X_all[:, :m_train]\n",
    "y       = y_all[:m_train]\n",
    "\n",
    "X_val   = X_all[:, m_train:]\n",
    "y_val   = y_all[m_train:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(X_all.shape)\n",
    "print(y_all.shape)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(y)\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 1\n",
    "\n",
    "#### Simple fully-connected deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "layer_dimensions = [X_train.shape[0], 100, 10]  # including the input and output layers\n",
    "NN = NeuralNetwork(layer_dimensions)\n",
    "NN.train(X_train, y_train, iters=10, alpha=0, batch_size=128, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_predicted = NN.predict(X_test)\n",
    "save_predictions('ans1-uni', y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# test if your numpy file has been saved correctly\n",
    "loaded_y = np.load('ans1-uni.npy')\n",
    "print(loaded_y.shape)\n",
    "loaded_y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 2: Regularizing the neural network\n",
    "#### Add dropout and L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "NN2 = NeuralNetwork(layer_dimensions, drop_prob=0, reg_lambda=0)\n",
    "NN2.train(X_train, y_train, iters=1000, alpha=0.00001, batch_size=1000, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_predicted2 = NN2.predict(X)\n",
    "save_predictions(y_predicted, 'ans2-uni')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

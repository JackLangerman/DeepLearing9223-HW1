{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# CS-GY 9223-E: Deep Learning Homework 1\n",
    "Due on Sunday, 11th February 2018, 11:55 PM\n",
    "\n",
    "This homework can be done in pairs. Everyone must submit on NYU Classes individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Write down the UNIs (NetIDs) of your group (if applicable)\n",
    "\n",
    "Member 1: Name, NetID\n",
    "\n",
    "Member 2: Name, NetID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import glob\n",
    "import sys\n",
    "# you shouldn't need to make any more imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    Abstraction of neural network.\n",
    "    Stores parameters, activations, cached values. \n",
    "    Provides necessary functions for training and prediction. \n",
    "    \"\"\"\n",
    "    def __init__(self, layer_dimensions, drop_prob=0.0, reg_lambda=0.0):\n",
    "        \"\"\"\n",
    "        Initializes the weights and biases for each layer\n",
    "        :param layer_dimensions: (list) number of nodes in each layer\n",
    "        :param drop_prob: drop probability for dropout layers. Only required in part 2 of the assignment\n",
    "        :param reg_lambda: regularization parameter. Only required in part 2 of the assignment\n",
    "        \"\"\"\n",
    "#         np.random.seed(1)\n",
    "        \n",
    "        self.parameters = {}\n",
    "        self.num_layers = len(layer_dimensions)\n",
    "        self.drop_prob = drop_prob\n",
    "        self.reg_lambda = reg_lambda\n",
    "        \n",
    "        # init parameters\n",
    "        for i in range(1, self.num_layers):\n",
    "            d_in  = layer_dimensions[i-1]\n",
    "            d_out = layer_dimensions[i]\n",
    "            self.parameters[\"layer\"+str(i)] = \\\n",
    "                ( (np.random.randn(d_out, d_in) * np.sqrt(2/(d_in+d_out))), \n",
    "                     np.zeros(d_out) )\n",
    "                \n",
    "#         print(\"num weights:\", len(parameters))\n",
    "                #             self.parameters[\"b\"+str(i)] = np.zeros(d_out)\n",
    "        \n",
    "\n",
    "    def affineForward(self, A, W, b):\n",
    "        \"\"\"\n",
    "        Forward pass for the affine layer.\n",
    "        :param A: input matrix, shape (L, S), where L is the number of hidden units in the previous layer and S is\n",
    "        the number of samples\n",
    "        :returns: the affine product WA + b, along with the cache required for the backward pass\n",
    "        \"\"\"\n",
    "#         lin = np.matmul(A.T, W)\n",
    "#         print(\"\\t\", str(A.shape)+\".T\", \"•\", W.shape, \"=\", lin.shape)\n",
    "        \n",
    "\n",
    "#         return aff.T\n",
    "        \n",
    "        lin = np.matmul(W, A)\n",
    "        print(\"\\t\", str(W.shape), \"•\", A.shape, \"=\", lin.shape)\n",
    "        \n",
    "        aff = lin + b[:, None]\n",
    "        print(\"\\t(\", lin.shape, \"+\", b[:, None].shape, \").T =\", aff.T.shape)\n",
    "\n",
    "        \n",
    "        return aff, [A, W, aff]     # A which is the activations from the previous layer\n",
    "                                    # and W which are the weights from the current layer \n",
    "                                    # are returned as the cache for backprop\n",
    "    \n",
    "\n",
    "    def activationForward(self, A, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Common interface to access all activation functions.\n",
    "        :param A: input to the activation function\n",
    "        :param prob: activation funciton to apply to A. Just \"relu\" for this assignment.\n",
    "        :returns: activation(A)\n",
    "        \"\"\" \n",
    "        return getattr(self,activation)(A)\n",
    "\n",
    "\n",
    "    def relu(self, X):\n",
    "        return np.maximum(X, 0)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def softmax(self, X):\n",
    "        ex = np.exp(X - np.max(X, axis=0))\n",
    "        return ex / np.sum(ex, axis=0)\n",
    "    \n",
    "#     def softmax_derivative(self, dx, cached_x):\n",
    "#         sm = softmax(cached_x)\n",
    "#         sigma_prime = sm * (np.ones_like(cached_x) - sm)\n",
    "#         return dx * sigma_prime\n",
    "        \n",
    "    def dropout(self, A, prob):\n",
    "        \"\"\"\n",
    "        :param A: Activation\n",
    "        :param prob: drop prob\n",
    "        :returns: tuple (A, M) \n",
    "            WHERE\n",
    "            A is matrix after applying dropout\n",
    "            M is dropout mask, used in the backward pass\n",
    "        \"\"\"\n",
    "        \n",
    "        M = np.random.rand(A.shape[0], A.shape[1])\n",
    "        M = (M > prob) * 1.0\n",
    "        scale_up_prob = 1 / (1 - prob)\n",
    "        M *= scale_up_prob\n",
    "        A *= M\n",
    "        \n",
    "        return A, M\n",
    "\n",
    "    def forwardPropagation(self, X):\n",
    "        \"\"\"\n",
    "        Runs an input X through the neural network to compute activations\n",
    "        for all layers. Returns the output computed at the last layer along\n",
    "        with the cache required for backpropagation.\n",
    "        :returns: (tuple) AL, cache\n",
    "            WHERE \n",
    "            AL is activation of last layer\n",
    "            cache is cached values for each layer that\n",
    "                     are needed in further steps\n",
    "        \"\"\"\n",
    "        \n",
    "        cache = []\n",
    "        A = X #A0 is the input\n",
    "        for i in range(1, self.num_layers):\n",
    "            print(\"compute:\", i)\n",
    "#             cache.append(A)\n",
    "            \n",
    "#             W = self.parameters[\"w\"+str(i)]\n",
    "#             b = self.parameters[\"b\"+str(i)]\n",
    "            W, b = self.parameters[\"layer\"+str(i)]\n",
    "            Z, layer_cache = self.affineForward(A, W, b)\n",
    "            \n",
    "            cache.extend(layer_cache)\n",
    "            \n",
    "            A = self.activationForward(Z, 'relu')\n",
    "            \n",
    "            \n",
    "        AL = self.softmax(A)\n",
    "        \n",
    "        return AL, cache\n",
    "    \n",
    "    \n",
    "    def costFunction(self, AL, y):\n",
    "        \"\"\"\n",
    "        :param AL: Activation of last layer, shape (num_classes, S)\n",
    "        :param y: labels, shape (S)\n",
    "        :param alpha: regularization parameter\n",
    "        :returns cost, dAL: A scalar denoting cost and the gradient of cost\n",
    "        \"\"\"\n",
    "        # compute loss\n",
    "        N = y.shape[0]\n",
    "        AL_softmax = self.softmax(AL)\n",
    "        correct_label_prob = AL_softmax[y, range(N)]\n",
    "        cost = -np.sum(np.log(correct_label_prob)) / N\n",
    "        \n",
    "#         if self.reg_lambda > 0:\n",
    "#             # add regularization\n",
    "#             cost += self.reg_lambda * np.sum(paramiters ** 2)\n",
    "            \n",
    "        # gradient of cost\n",
    "#         dAL = np.sum(-Y / y_pred) / len(y)\n",
    "#         dAL = np.zeros_like(AL)\n",
    "        \n",
    "                            # y S\n",
    "        dAL = AL*(AL_softmax - 1)\n",
    "        print(\"dAL:\", dAL.shape)\n",
    "    \n",
    "        return cost, dAL\n",
    "\n",
    "    def affineBackward(self, dA_prev, cache):\n",
    "        \"\"\"\n",
    "        Backward pass for the affine layer.\n",
    "        :param dA_prev: gradient from the next layer.\n",
    "        :param cache: cache returned in affineForward\n",
    "        :returns dA: gradient on the input to this layer\n",
    "                 dW: gradient on the weights\n",
    "                 db: gradient on the bias\n",
    "        \"\"\"\n",
    "#         print(dA_prev.shape, cache.shape)\n",
    "        W = cache.pop()\n",
    "        A = cache.pop()\n",
    "        print(\"W shapppppeeee\",W.shape)\n",
    "        print(\"A shapppppeeee\",A.shape)\n",
    "        # TILE IS WROOOOOOOONGGGGG\n",
    "        print(\"np.shape(np.sum(A.T, axis=0))\", np.shape(np.sum(A.T, axis=0)))\n",
    "        print(\"np.shape(np.sum(W.T, axis=0))\", np.shape(np.sum(W, axis=0)))\n",
    "        dW = np.tile( np.mean(A.T, axis=0), (W.shape[0], 1) )\n",
    "        print(\"dW shape\", np.shape(dW))\n",
    "        \n",
    "        \n",
    "        print(\"dA_prev shpe\", np.shape(dA_prev))\n",
    "        W_ = (np.tile( np.mean(W, axis=0), ( dA_prev.shape[0], 1 ) ))\n",
    "        print(\"W_ shape\", np.shape(W_))\n",
    "\n",
    "        dA = np.matmul(W_.T, dA_prev)\n",
    "        print(\"dA shape\", np.shape(dA))\n",
    "\n",
    "        \n",
    "        \n",
    "#         dA = np.tile( [1,2,3,4,5], ( A.shape[1], 1 ) )\n",
    "        \n",
    "        print(\"W rows\", W.shape[0])\n",
    "        db = np.ones((W.shape[0],1))\n",
    "        db = np.mean(dA_prev, axis=1)\n",
    "        \n",
    "        print(\"the shapes(w,a,b):\", dW.shape, dA.shape, db.shape, \"\\tdaprev\", dA_prev.shape)\n",
    "        \n",
    "        \n",
    "#         dW = np.sum(X.T, axis=0)\n",
    "#         dA = dA_prev.dot(dW)\n",
    "        return dA, dW, db\n",
    "    \n",
    "\n",
    "    def activationBackward(self, dA, cache, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Interface to call backward on activation functions.\n",
    "        In this case, it's just relu. \n",
    "        \"\"\"\n",
    "        return getattr(self,activation+\"_derivative\")(dA, cache.pop())\n",
    "\n",
    "        \n",
    "    def relu_derivative(self, dx, cached_x):\n",
    "        sigma_prime = np.zeros_like(cached_x)\n",
    "        \n",
    "        print(\"s':\", sigma_prime.shape, \"dx: \", dx.shape, \"cachedx:\", cached_x.shape)\n",
    "        \n",
    "        sigma_prime[ cached_x > 0] = 1\n",
    "        \n",
    "        return dx * sigma_prime\n",
    "\n",
    "    def dropout_backward(self, dA, cache):\n",
    "\n",
    "        return dA\n",
    "\n",
    "    def backPropagation(self, dAL, Y, cache):\n",
    "        \"\"\"\n",
    "        Run backpropagation to compute gradients on all paramters in the model\n",
    "        :param dAL: gradient on the last layer of the network. Returned by the cost function.\n",
    "        :param Y: labels\n",
    "        :param cache: cached values during forwardprop\n",
    "        :returns gradients: dW and db for each weight/bias\n",
    "        \"\"\"\n",
    "        print(\"cache shape: {:d}\".format(len(cache)))\n",
    "        gradients = {}\n",
    "        \n",
    "        Y = one_hot(Y)\n",
    "        print(\"#$%#$%$#%$#$%#%$#\",Y)\n",
    "\n",
    "            \n",
    "        upstream_grad = Y * dAL\n",
    "        print(\"upstream grad:\", upstream_grad.shape)\n",
    "        cache.pop() # get rid of aff because the last layer doesn't use relu_backward\n",
    "        dA, dW, db = self.affineBackward(upstream_grad, cache)\n",
    "        gradients[\"layer\"+str(self.num_layers-1)] = (dW, db)\n",
    "        \n",
    "        for i in reversed(range(1, self.num_layers-1)): # loop through the layers backwards\n",
    "            print(\"backprop through layer \"+str(i))\n",
    "            [print(c.shape) for c in cache]\n",
    "            print(\"----\")\n",
    "            \n",
    "            dA = self.activationBackward(dA, cache)\n",
    "#             db = dA\n",
    "            dA, dW, db = self.affineBackward(dA, cache)\n",
    "            print(\"cache shape: {:d}\".format(len(cache)))\n",
    "\n",
    "            gradients[\"layer\"+str(i)] = (dW, db)\n",
    "#             gradients[\"dw\"+str(i)] = dW\n",
    "#             gradients[\"db\"+str(i)] = db\n",
    "#             if self.drop_prob > 0:\n",
    "                #call dropout_backward\n",
    "           \n",
    "            \n",
    "#         if self.reg_lambda > 0:\n",
    "            # add gradients from L2 regularization to each dW\n",
    "        \n",
    "        return gradients\n",
    "\n",
    "\n",
    "    def updateParameters(self, gradients, alpha):\n",
    "        \"\"\"\n",
    "        :param gradients: gradients for each weight/bias\n",
    "        :param alpha: step size for gradient descent \n",
    "        \"\"\"\n",
    "#         self.parameters -= alpha * gradients\n",
    "        for key in self.parameters:\n",
    "            print(\"updating \", key)\n",
    "            w, b = self.parameters[key]\n",
    "            \n",
    "            w -= alpha * gradients[key][0]\n",
    "            b -= alpha * gradients[key][1]\n",
    "\n",
    "            self.parameters[key] = (w, b)\n",
    "\n",
    "            \n",
    "    def train(self, X, y, iters=1000, alpha=0.0001, batch_size=100, print_every=100):\n",
    "        \"\"\"\n",
    "        :param X: input samples, each column is a sample\n",
    "        :param y: labels for input samples, y.shape[0] must equal X.shape[1]\n",
    "        :param iters: number of training iterations\n",
    "        :param alpha: step size for gradient descent\n",
    "        :param batch_size: number of samples in a minibatch\n",
    "        :param print_every: no. of iterations to print debug info after\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(0, iters):\n",
    "            # get minibatch\n",
    "#                 X_mb, y_mb = get_batch(X, y, batch_size)\n",
    "            # forward prop\n",
    "            AL, cache = self.forwardPropagation(X)\n",
    "            print(\"done forwardprop\")\n",
    "            \n",
    "            # compute loss\n",
    "            cost, dAL = self.costFunction(AL, y)\n",
    "            # compute gradients\n",
    "            \n",
    "            gradients = self.backPropagation(dAL, y, cache)  \n",
    "#             print(gradients)\n",
    "            print(\"done backproping\")\n",
    "    \n",
    "            # update weights and biases based on gradient\n",
    "            self.updateParameters(gradients, alpha)\n",
    "            \n",
    "            \n",
    "            if i % print_every == 0:\n",
    "                # print cost, train and validation set accuracies\n",
    "                print(\"should print cost \"+str(i))\n",
    "                print(\"\\n\\nCOST:\",cost,\"\\n\\n\")\n",
    "                print(\"COST:\",cost, file=sys.stderr)\n",
    "                for key in self.parameters:\n",
    "                    print(\"\\tweight:\", np.sum(self.parameters[key][0] ** 2), file=sys.stderr)\n",
    "                \n",
    "                for g in gradients:\n",
    "                    print(\"\\tgradient:\", np.sum(gradients[g][0] **2))\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions for each sample\n",
    "        \"\"\"\n",
    "        AL, cache = NN.forwardPropagation(X)\n",
    "        \n",
    "        y_pred = self.softmax(AL)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def get_batch(self, X, y, batch_size):\n",
    "        \"\"\"\n",
    "        Return minibatch of samples and labels\n",
    "        \n",
    "        :param X, y: samples and corresponding labels\n",
    "        :parma batch_size: minibatch size\n",
    "        :returns: (tuple) X_batch, y_batch\n",
    "        \"\"\"\n",
    "\n",
    "        return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1]\n",
      "[2, 2]\n",
      "\n",
      "[3 3]\n",
      "[2 4]\n",
      "\n",
      "[0 1 2 3 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3, 4, 0, 1, 2, 3, 4],\n",
       "       [0, 1, 2, 3, 4, 0, 1, 2, 3, 4],\n",
       "       [0, 1, 2, 3, 4, 0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [[1, 1], [2, 2]]\n",
    "\n",
    "[print(r) for r in x]\n",
    "print(\"\")\n",
    "print(np.sum(x, axis=0))\n",
    "print(np.sum(x, axis=1))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "x = np.arange(5)\n",
    "print(x)\n",
    "# np.repeat(x, [5, 1], axis=0)\n",
    "np.tile(x, (3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "should print cost 0\n",
      "\n",
      "\n",
      "COST: 2.33886977728 \n",
      "\n",
      "\n",
      "\tgradient: 315.889890543\n",
      "\tgradient: 72581.5820992\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COST: 2.33886977728\n",
      "\tweight: 201.195106347\n",
      "\tweight: 17.7908196681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "should print cost 10\n",
      "\n",
      "\n",
      "COST: 2.30250449906 \n",
      "\n",
      "\n",
      "\tgradient: 0.0\n",
      "\tgradient: 72581.5820992\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COST: 2.30250449906\n",
      "\tweight: 1071.11595697\n",
      "\tweight: 17.7908196681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "should print cost 20\n",
      "\n",
      "\n",
      "COST: 2.30241859351 \n",
      "\n",
      "\n",
      "\tgradient: 0.0\n",
      "\tgradient: 72581.5820992\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COST: 2.30241859351\n",
      "\tweight: 3392.66844958\n",
      "\tweight: 17.7908196681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "should print cost 30\n",
      "\n",
      "\n",
      "COST: 2.30233239858 \n",
      "\n",
      "\n",
      "\tgradient: 0.0\n",
      "\tgradient: 72581.5820992\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COST: 2.30233239858\n",
      "\tweight: 7165.85258418\n",
      "\tweight: 17.7908196681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "should print cost 40\n",
      "\n",
      "\n",
      "COST: 2.30224591241 \n",
      "\n",
      "\n",
      "\tgradient: 0.0\n",
      "\tgradient: 72581.5820992\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COST: 2.30224591241\n",
      "\tweight: 12390.6683608\n",
      "\tweight: 17.7908196681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "should print cost 50\n",
      "\n",
      "\n",
      "COST: 2.30215913312 \n",
      "\n",
      "\n",
      "\tgradient: 0.0\n",
      "\tgradient: 72581.5820992\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COST: 2.30215913312\n",
      "\tweight: 19067.1157793\n",
      "\tweight: 17.7908196681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "should print cost 60\n",
      "\n",
      "\n",
      "COST: 2.3020720588 \n",
      "\n",
      "\n",
      "\tgradient: 0.0\n",
      "\tgradient: 72581.5820992\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COST: 2.3020720588\n",
      "\tweight: 27195.1948399\n",
      "\tweight: 17.7908196681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "should print cost 70\n",
      "\n",
      "\n",
      "COST: 2.30198468754 \n",
      "\n",
      "\n",
      "\tgradient: 0.0\n",
      "\tgradient: 72581.5820992\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COST: 2.30198468754\n",
      "\tweight: 36774.9055424\n",
      "\tweight: 17.7908196681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "should print cost 80\n",
      "\n",
      "\n",
      "COST: 2.30189701742 \n",
      "\n",
      "\n",
      "\tgradient: 0.0\n",
      "\tgradient: 72581.5820992\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COST: 2.30189701742\n",
      "\tweight: 47806.2478869\n",
      "\tweight: 17.7908196681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "should print cost 90\n",
      "\n",
      "\n",
      "COST: 2.30180904648 \n",
      "\n",
      "\n",
      "\tgradient: 0.0\n",
      "\tgradient: 72581.5820992\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COST: 2.30180904648\n",
      "\tweight: 60289.2218734\n",
      "\tweight: 17.7908196681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "done forwardprop\n",
      "dAL: (10, 20)\n",
      "cache shape: 6\n",
      "#$%#$%$#%$#$%#%$# [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "upstream grad: (10, 20)\n",
      "W shapppppeeee (10, 100)\n",
      "A shapppppeeee (100, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (100,)\n",
      "np.shape(np.sum(W.T, axis=0)) (100,)\n",
      "dW shape (10, 100)\n",
      "dA_prev shpe (10, 20)\n",
      "W_ shape (10, 100)\n",
      "dA shape (100, 20)\n",
      "W rows 10\n",
      "the shapes(w,a,b): (10, 100) (100, 20) (10,) \tdaprev (10, 20)\n",
      "backprop through layer 1\n",
      "(3072, 20)\n",
      "(100, 3072)\n",
      "(100, 20)\n",
      "----\n",
      "s': (100, 20) dx:  (100, 20) cachedx: (100, 20)\n",
      "W shapppppeeee (100, 3072)\n",
      "A shapppppeeee (3072, 20)\n",
      "np.shape(np.sum(A.T, axis=0)) (3072,)\n",
      "np.shape(np.sum(W.T, axis=0)) (3072,)\n",
      "dW shape (100, 3072)\n",
      "dA_prev shpe (100, 20)\n",
      "W_ shape (100, 3072)\n",
      "dA shape (3072, 20)\n",
      "W rows 100\n",
      "the shapes(w,a,b): (100, 3072) (3072, 20) (100,) \tdaprev (100, 20)\n",
      "cache shape: 0\n",
      "done backproping\n",
      "updating  layer1\n",
      "updating  layer2\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 20) = (100, 20)\n",
      "\t( (100, 20) + (100, 1) ).T = (20, 100)\n",
      "compute: 2\n",
      "\t (10, 100) • (100, 20) = (10, 20)\n",
      "\t( (10, 20) + (10, 1) ).T = (20, 10)\n",
      "[[ 0.0999989   0.0999989   0.0999989   0.0999989   0.0999989   0.0999989\n",
      "   0.0999989   0.0999989   0.0999989   0.0999989   0.0999989   0.0999989\n",
      "   0.0999989   0.0999989   0.0999989   0.0999989   0.0999989   0.0999989\n",
      "   0.0999989   0.0999989 ]\n",
      " [ 0.0999548   0.0999548   0.0999548   0.0999548   0.0999548   0.0999548\n",
      "   0.0999548   0.0999548   0.0999548   0.0999548   0.0999548   0.0999548\n",
      "   0.0999548   0.0999548   0.0999548   0.0999548   0.0999548   0.0999548\n",
      "   0.0999548   0.0999548 ]\n",
      " [ 0.09990993  0.09990993  0.09990993  0.09990993  0.09990993  0.09990993\n",
      "   0.09990993  0.09990993  0.09990993  0.09990993  0.09990993  0.09990993\n",
      "   0.09990993  0.09990993  0.09990993  0.09990993  0.09990993  0.09990993\n",
      "   0.09990993  0.09990993]\n",
      " [ 0.10004409  0.10004409  0.10004409  0.10004409  0.10004409  0.10004409\n",
      "   0.10004409  0.10004409  0.10004409  0.10004409  0.10004409  0.10004409\n",
      "   0.10004409  0.10004409  0.10004409  0.10004409  0.10004409  0.10004409\n",
      "   0.10004409  0.10004409]\n",
      " [ 0.0999991   0.0999991   0.0999991   0.0999991   0.0999991   0.0999991\n",
      "   0.0999991   0.0999991   0.0999991   0.0999991   0.0999991   0.0999991\n",
      "   0.0999991   0.0999991   0.0999991   0.0999991   0.0999991   0.0999991\n",
      "   0.0999991   0.0999991 ]\n",
      " [ 0.09995431  0.09995431  0.09995431  0.09995431  0.09995431  0.09995431\n",
      "   0.09995431  0.09995431  0.09995431  0.09995431  0.09995431  0.09995431\n",
      "   0.09995431  0.09995431  0.09995431  0.09995431  0.09995431  0.09995431\n",
      "   0.09995431  0.09995431]\n",
      " [ 0.10022933  0.10022933  0.10022933  0.10022933  0.10022933  0.10022933\n",
      "   0.10022933  0.10022933  0.10022933  0.10022933  0.10022933  0.10022933\n",
      "   0.10022933  0.10022933  0.10022933  0.10022933  0.10022933  0.10022933\n",
      "   0.10022933  0.10022933]\n",
      " [ 0.09990993  0.09990993  0.09990993  0.09990993  0.09990993  0.09990993\n",
      "   0.09990993  0.09990993  0.09990993  0.09990993  0.09990993  0.09990993\n",
      "   0.09990993  0.09990993  0.09990993  0.09990993  0.09990993  0.09990993\n",
      "   0.09990993  0.09990993]\n",
      " [ 0.10004414  0.10004414  0.10004414  0.10004414  0.10004414  0.10004414\n",
      "   0.10004414  0.10004414  0.10004414  0.10004414  0.10004414  0.10004414\n",
      "   0.10004414  0.10004414  0.10004414  0.10004414  0.10004414  0.10004414\n",
      "   0.10004414  0.10004414]\n",
      " [ 0.09995547  0.09995547  0.09995547  0.09995547  0.09995547  0.09995547\n",
      "   0.09995547  0.09995547  0.09995547  0.09995547  0.09995547  0.09995547\n",
      "   0.09995547  0.09995547  0.09995547  0.09995547  0.09995547  0.09995547\n",
      "   0.09995547  0.09995547]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "class DummyFile(object):\n",
    "    def write(self, x): pass\n",
    "\n",
    "\n",
    "# save_stdout = sys.stdout\n",
    "# sys.stdout = DummyFile()\n",
    "\n",
    "n = 20\n",
    "\n",
    "layer_dimensions = [X_train.shape[0], 100, 10]  # including the input and output layers\n",
    "NN = NeuralNetwork(layer_dimensions)\n",
    "NN.train(X_train[:, :n], y_train[:n], iters=100, alpha=.01, batch_size=128, print_every=10)\n",
    "y_hat = NN.predict(X_train[:, :n])\n",
    "# sys.stdout = save_stdout\n",
    "\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b900ed9ed1f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mXs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.randn (numpy/random/mtrand/mtrand.c:19843)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.standard_normal (numpy/random/mtrand/mtrand.c:20368)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.cont0_array (numpy/random/mtrand/mtrand.c:6127)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "Xs = 2*np.random.randn(500)\n",
    "ys = np.ones_like(Xs) * np.random.randn(Xs.shape)\n",
    "\n",
    "plt.plot(Xs, ys, 'o')\n",
    "\n",
    "\n",
    "# print(Xs)\n",
    "\n",
    "# n = 20\n",
    "\n",
    "# layer_dimensions = [X_train.shape[0], 100, 10]  # including the input and output layers\n",
    "# NN = NeuralNetwork(layer_dimensions)\n",
    "# NN.train(X_train[:, :n], y_train[:n], iters=100, alpha=.001, batch_size=128, print_every=10)\n",
    "# y_hat = NN.predict(X_train[:, :n])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Functions to load data, DO NOT change these\n",
    "\n",
    "def get_labels(folder, label2id):\n",
    "    \"\"\"\n",
    "    Returns vector of labels extracted from filenames of all files in folder\n",
    "    :param folder: path to data folder\n",
    "    :param label2id: mapping of text labels to numeric ids. (Eg: automobile -> 0)\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    y = []\n",
    "    for f in files:\n",
    "        y.append(get_label(f,label2id))\n",
    "    return np.array(y)\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts each label index in y to vector with one_hot encoding\n",
    "    \"\"\"\n",
    "    y_one_hot = np.zeros((num_classes, y.shape[0]))\n",
    "    y_one_hot[y, range(y.shape[0])] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "def get_label_mapping(label_file):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        id2label = f.readlines()\n",
    "        id2label = [l.strip() for l in id2label]\n",
    "    label2id = {}\n",
    "    count = 0\n",
    "    for label in id2label:\n",
    "        label2id[label] = count\n",
    "        count += 1\n",
    "    return id2label, label2id\n",
    "\n",
    "def get_images(folder):\n",
    "    \"\"\"\n",
    "    returns numpy array of all samples in folder\n",
    "    each column is a sample resized to 30x30 and flattened\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    images = []\n",
    "    count = 0\n",
    "    \n",
    "    for f in files:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "        img_arr = get_img_array(f)\n",
    "        img_arr = img_arr.flatten() / 255.0\n",
    "        images.append(img_arr)\n",
    "    X = np.column_stack(images)\n",
    "\n",
    "    return X\n",
    "\n",
    "def get_train_data(data_root_path):\n",
    "    \"\"\"\n",
    "    Return X and y\n",
    "    \"\"\"\n",
    "    train_data_path = data_root_path + 'train'\n",
    "    id2label, label2id = get_label_mapping(data_root_path+'labels.txt')\n",
    "    print(label2id)\n",
    "    X = get_images(train_data_path)\n",
    "    y = get_labels(train_data_path, label2id)\n",
    "    return X, y\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    \"\"\"\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Helper functions, DO NOT modify this\n",
    "\n",
    "def get_img_array(path):\n",
    "    \"\"\"\n",
    "    Given path of image, returns it's numpy array\n",
    "    \"\"\"\n",
    "    return scipy.misc.imread(path)\n",
    "\n",
    "def get_files(folder):\n",
    "    \"\"\"\n",
    "    Given path to folder, returns list of files in it\n",
    "    \"\"\"\n",
    "    filenames = [file for file in glob.glob(folder+'*/*')]\n",
    "    filenames.sort()\n",
    "    return filenames\n",
    "\n",
    "def get_label(filepath, label2id):\n",
    "    \"\"\"\n",
    "    Files are assumed to be labeled as: /path/to/file/999_frog.png\n",
    "    Returns label for a filepath\n",
    "    \"\"\"\n",
    "    tokens = filepath.split('/')\n",
    "    label = tokens[-1].split('_')[1][:-4]\n",
    "    if label in label2id:\n",
    "        return label2id[label]\n",
    "    else:\n",
    "        sys.exit(\"Invalid label: \" + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel/__main__.py:7: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000/50000\n",
      "Loaded 20000/50000\n",
      "Loaded 30000/50000\n",
      "Loaded 40000/50000\n",
      "Loaded 50000/50000\n",
      "Loaded 10000/10000\n",
      "Data loading done\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_root_path = 'cifar10-hw1/'\n",
    "X_train, y_train = get_train_data(data_root_path) # this may take a few minutes\n",
    "X_test = get_images(data_root_path + 'test')\n",
    "print('Data loading done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### run some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight and bias dimentions and norms\n",
      "[['layer1', (100, 3072), (100,)], ['layer2', (500, 100), (500,)], ['layer3', (100, 500), (100,)], ['layer4', (30, 100), (30,)], ['layer5', (10, 30), (10,)]]\n",
      "[(194.24220745443395, 0.0), (167.78937148450697, 0.0), (167.52084421223711, 0.0), (45.863617818508075, 0.0), (16.934975386249203, 0.0)]\n",
      "\n",
      "test affineForward\n",
      "\t (2, 2) • (2, 2) = (2, 2)\n",
      "\t( (2, 2) + (2, 1) ).T = (2, 2)\n",
      "(array([[4, 1],\n",
      "       [2, 2]]), [array([[1, 0],\n",
      "       [0, 1]]), array([[4, 1],\n",
      "       [2, 2]]), array([[4, 1],\n",
      "       [2, 2]])])\n",
      "(2,)\n",
      "\t (2, 2) • (2,) = (2,)\n",
      "\t( (2,) + (2, 1) ).T = (2, 2)\n",
      "(array([[5, 4],\n",
      "       [5, 4]]), [array([1, 1]), array([[4, 1],\n",
      "       [2, 2]]), array([[5, 4],\n",
      "       [5, 4]])])\n",
      "\n",
      "test activation\n",
      "[0.1, 0.2, -0.2, 1, -1, -1, -1, 1, 1, 1, 2]\n",
      "[ 0.1  0.2  0.   1.   0.   0.   0.   1.   1.   1.   2. ]\n",
      "[ 0.1  0.2  0.   1.   0.   0.   0.   1.   1.   1.   2. ]\n",
      "\n",
      "\n",
      "\n",
      "test forward prop\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 10000) = (100, 10000)\n",
      "\t( (100, 10000) + (100, 1) ).T = (10000, 100)\n",
      "compute: 2\n",
      "\t (500, 100) • (100, 10000) = (500, 10000)\n",
      "\t( (500, 10000) + (500, 1) ).T = (10000, 500)\n",
      "compute: 3\n",
      "\t (100, 500) • (500, 10000) = (100, 10000)\n",
      "\t( (100, 10000) + (100, 1) ).T = (10000, 100)\n",
      "compute: 4\n",
      "\t (30, 100) • (100, 10000) = (30, 10000)\n",
      "\t( (30, 10000) + (30, 1) ).T = (10000, 30)\n",
      "compute: 5\n",
      "\t (10, 30) • (30, 10000) = (10, 10000)\n",
      "\t( (10, 10000) + (10, 1) ).T = (10000, 10)\n",
      "\n",
      "AL: (10, 10000)\n",
      "\n",
      "\n",
      "\n",
      "test cost\n",
      "dAL: (10, 10000)\n",
      "cost: ( 2.3029151505 ?= 2.3 )  ( 0.0999669996954 ?= 0.1 )\n",
      "\t/\\ should be about 2.3 and .1\n",
      "\n",
      "\n",
      "compute: 1\n",
      "\t (100, 3072) • (3072, 10000) = (100, 10000)\n",
      "\t( (100, 10000) + (100, 1) ).T = (10000, 100)\n",
      "compute: 2\n",
      "\t (500, 100) • (100, 10000) = (500, 10000)\n",
      "\t( (500, 10000) + (500, 1) ).T = (10000, 500)\n",
      "compute: 3\n",
      "\t (100, 500) • (500, 10000) = (100, 10000)\n",
      "\t( (100, 10000) + (100, 1) ).T = (10000, 100)\n",
      "compute: 4\n",
      "\t (30, 100) • (100, 10000) = (30, 10000)\n",
      "\t( (30, 10000) + (30, 1) ).T = (10000, 30)\n",
      "compute: 5\n",
      "\t (10, 30) • (30, 10000) = (10, 10000)\n",
      "\t( (10, 10000) + (10, 1) ).T = (10000, 10)\n",
      "\n",
      "\n",
      "test predict\n",
      "\n",
      "predict: \n",
      " (10, 10000)\n",
      "\n",
      "acc:  0.0785\n",
      "\t/\\ should be about .1\n",
      "\n",
      "\n",
      "\n",
      " [[ 0.10175962  0.1012145   0.10071834  0.10175552  0.10169481]\n",
      " [ 0.09897877  0.09877589  0.09930638  0.0992178   0.09886862]\n",
      " [ 0.10102004  0.1006994   0.10006574  0.10093127  0.10079007]\n",
      " [ 0.09897877  0.09877589  0.09930638  0.0992178   0.09886862]\n",
      " [ 0.09897877  0.09877589  0.09930638  0.0992178   0.09886862]\n",
      " [ 0.10126057  0.10329651  0.10143519  0.10091676  0.10171253]\n",
      " [ 0.10141452  0.10082203  0.10065897  0.10085096  0.10124959]\n",
      " [ 0.09897877  0.09877589  0.09930638  0.0992178   0.09886862]\n",
      " [ 0.09897877  0.09877589  0.09930638  0.0992178   0.09886862]\n",
      " [ 0.09965142  0.1000881   0.10058987  0.09945651  0.10020989]\n",
      " [ 1.          1.          1.          1.          1.        ]]\n",
      "\t/\\ should all add to 1\n"
     ]
    }
   ],
   "source": [
    "layer_dimensions = [X_train.shape[0], 100, 500, 100, 30, 10]  # including the input and output layers\n",
    "NN = NeuralNetwork(layer_dimensions)\n",
    "# NN.train(X_train, y_train, iters=10, alpha=0, batch_size=128, print_every=10)\n",
    "\n",
    "# print([k for k in NN.parameters.keys()])\n",
    "# print([v.shape for v in NN.parameters.values()])\n",
    "\n",
    "print(\"weight and bias dimentions and norms\")\n",
    "print([[k, v[0].shape, v[1].shape] for k,v in NN.parameters.items()])\n",
    "print([( np.sum(v[0] ** 2), np.sum(v[1] ** 2)) for k,v in NN.parameters.items()])\n",
    "print()\n",
    "\n",
    "print(\"test affineForward\")\n",
    "A = np.array([[1, 0], [0, 1]])\n",
    "W = np.array([[4, 1], \n",
    "     [2, 2]])\n",
    "print(NN.affineForward(A, W, np.array([0,0]).T))\n",
    "A = np.array([1,1])\n",
    "print(A.shape)\n",
    "print(NN.affineForward(A, W, np.array([0,0])))\n",
    "\n",
    "print(\"\\ntest activation\")\n",
    "z = [.1, .2, -.2, 1, -1, -1, -1, 1, 1, 1, 2]\n",
    "print(z)\n",
    "print(NN.relu(z))\n",
    "print(NN.activationForward(z))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_train[:, 0].shape)\n",
    "# print(X_train[1].shape)\n",
    "\n",
    "\n",
    "print(\"\\ntest forward prop\")\n",
    "AL, cache = NN.forwardPropagation(X_train[:, :10000])\n",
    "print(\"\\nAL:\", AL.shape)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"test cost\")\n",
    "cost, dAL = NN.costFunction(AL, y_train[:10000])\n",
    "print(\"cost: (\", cost, \"?=\", 2.3, \")  (\", np.exp(-cost), \"?=\", 0.1,\")\") # cost \\approx    2.3  \\approx    -log(1/10)     => :-)\n",
    "print(\"\\t/\\ should be about 2.3 and .1\\n\\n\")\n",
    "\n",
    "y_hat = NN.predict(X_train[:, :10000])\n",
    "\n",
    "print(\"\\n\\ntest predict\\n\")\n",
    "print(\"predict: \\n\", y_hat.shape)\n",
    "\n",
    "print(\"\\nacc: \", np.mean(np.argmax(y_hat, axis=0) == y_train[:10000]))\n",
    "print(\"\\t/\\ should be about .1\")\n",
    "\n",
    "print(\"\\n\\n\\n\", np.row_stack( (y_hat[:, :5], np.sum( y_hat[:, :5], axis=0)) ) )  # last row = 1  => :-)\n",
    "print(\"\\t/\\ should all add to 1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### check the distribution on the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "y_ [ 0.10132308  0.09899113  0.10047647  0.09899113  0.09899113  0.1023125\n",
      "  0.10038476  0.09902675  0.09899113  0.1005119 ]\n",
      "5\n",
      "0.10231249705\n",
      "p: 5\n",
      "mean: [ 0.10198603  0.09876352  0.10087686  0.0987415   0.0987392   0.10269873\n",
      "  0.10104106  0.09874136  0.09873977  0.09967196]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 5.,  0.,  1.,  0.,  0.,  2.,  0.,  0.,  1.,  1.]),\n",
       " array([  987.39202465,   991.35155683,   995.31108901,   999.27062118,\n",
       "         1003.23015336,  1007.18968554,  1011.14921772,  1015.10874989,\n",
       "         1019.06828207,  1023.02781425,  1026.98734643]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1085db208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1038ea400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_hat.shape\n",
    "p = np.argmax(y_hat, axis = 0)\n",
    "print(p.shape)\n",
    "print(\"y_\", y_hat[:, 1000])\n",
    "\n",
    "print(np.argmax(y_hat[:, 1000]))\n",
    "print(np.max(y_hat[:, 1000]))\n",
    "\n",
    "print(\"p:\", p[1000])\n",
    "plt.hist(p)\n",
    "\n",
    "print(\"mean:\", np.mean(y_hat, axis=1))\n",
    "prob_mass = np.sum(y_hat, axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(prob_mass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### tests for backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test cost\n",
      "dAL: (10, 10000)\n",
      "cost: 2.30 dAL: [[-0.10555073 -0.10082211 -0.09639087 ..., -0.10284407 -0.10825792\n",
      "  -0.10336148]\n",
      " [-0.08091195 -0.07911618 -0.0838261  ..., -0.07823628 -0.07826657\n",
      "  -0.07932893]\n",
      " [-0.09908007 -0.09629152 -0.09061072 ..., -0.10147881 -0.09962428\n",
      "  -0.10359163]\n",
      " ..., \n",
      " [-0.08091195 -0.07911618 -0.0838261  ..., -0.07823628 -0.07826657\n",
      "  -0.07932893]\n",
      " [-0.08091195 -0.07911618 -0.0838261  ..., -0.07823628 -0.07826657\n",
      "  -0.07932893]\n",
      " [-0.08694955 -0.0908773  -0.09525661 ..., -0.08608213 -0.08912776\n",
      "  -0.08642497]]\n"
     ]
    }
   ],
   "source": [
    "print(\"test cost\")\n",
    "cost, dAL = NN.costFunction(AL, y_train[:10000])\n",
    "print(\"cost: {0:.2f}\".format(cost), \"dAL:\", dAL)\n",
    "\n",
    "\n",
    "# NN.affineBackward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 6 7 9 1]\n",
      "[[-0.10586594 -0.10759139 -0.11114951 -0.10626543 -0.10523313]\n",
      " [-0.08535002 -0.07709121 -0.07741366 -0.07885555 -0.08307425]\n",
      " [-0.09384182 -0.09654573 -0.10121103 -0.09443045 -0.09923268]\n",
      " [-0.08535002 -0.07709121 -0.07741366 -0.07885555 -0.08307425]\n",
      " [-0.08535002 -0.07709121 -0.07741366 -0.07885555 -0.08307425]\n",
      " [-0.09221311 -0.11445323 -0.119745   -0.1208284  -0.1046973 ]\n",
      " [-0.09592356 -0.1035908  -0.09596845 -0.10129065 -0.09228647]\n",
      " [-0.08535002 -0.07709121 -0.07741366 -0.07885555 -0.08307425]\n",
      " [-0.08535002 -0.07709121 -0.07741366 -0.07885555 -0.08307425]\n",
      " [-0.08535002 -0.0921125  -0.08456254 -0.08264718 -0.08307425]]\n",
      "[0 5 5 5 0]\n",
      "[0 0 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "785"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 39\n",
    "n = 5\n",
    "print(y_train[i:i+n])\n",
    "print(dAL[:, i:i+n])\n",
    "print(np.argmax(y_hat[:, i:i+n], axis=0))\n",
    "print(1*(np.equal(y_train[i:i+n], np.argmax(y_hat[:, i:i+n], axis=0))))\n",
    "\n",
    "np.sum( np.equal(y_train[:10000], np.argmax(y_hat, axis=0) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Split train into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# X_train = ...\n",
    "# y_train = ...\n",
    "\n",
    "# X_val   \n",
    "# y_val1\n",
    "\n",
    "idx = np.random.permutation(len(y_train))\n",
    "X_all = X_train[:, idx]\n",
    "y_all = y_train[idx]\n",
    "\n",
    "m_train = int(len(y_all)*0.9);\n",
    "m_val   = len(y_all)-m_train\n",
    "\n",
    "X       = X_all[:, :m_train]\n",
    "y       = y_all[:m_train]\n",
    "\n",
    "X_val   = X_all[:, m_train:]\n",
    "y_val   = y_all[m_train:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(X_all.shape)\n",
    "print(y_all.shape)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(y)\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 1\n",
    "\n",
    "#### Simple fully-connected deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "layer_dimensions = [X_train.shape[0], 100, 10]  # including the input and output layers\n",
    "NN = NeuralNetwork(layer_dimensions)\n",
    "NN.train(X_train, y_train, iters=10, alpha=0, batch_size=128, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_predicted = NN.predict(X_test)\n",
    "save_predictions('ans1-uni', y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# test if your numpy file has been saved correctly\n",
    "loaded_y = np.load('ans1-uni.npy')\n",
    "print(loaded_y.shape)\n",
    "loaded_y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 2: Regularizing the neural network\n",
    "#### Add dropout and L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "NN2 = NeuralNetwork(layer_dimensions, drop_prob=0, reg_lambda=0)\n",
    "NN2.train(X_train, y_train, iters=1000, alpha=0.00001, batch_size=1000, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_predicted2 = NN2.predict(X)\n",
    "save_predictions(y_predicted, 'ans2-uni')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

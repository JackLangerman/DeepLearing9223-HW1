{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# CS-GY 9223-E: Deep Learning Homework 1\n",
    "Due on Sunday, 11th February 2018, 11:55 PM\n",
    "\n",
    "This homework can be done in pairs. Everyone must submit on NYU Classes individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Write down the UNIs (NetIDs) of your group (if applicable)\n",
    "\n",
    "Member 1: Name, NetID\n",
    "\n",
    "Member 2: Name, NetID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import glob\n",
    "import sys\n",
    "# you shouldn't need to make any more imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    Abstraction of neural network.\n",
    "    Stores parameters, activations, cached values. \n",
    "    Provides necessary functions for training and prediction. \n",
    "    \"\"\"\n",
    "    def __init__(self, layer_dimensions, drop_prob=0.0, reg_lambda=0.0):\n",
    "        \"\"\"\n",
    "        Initializes the weights and biases for each layer\n",
    "        :param layer_dimensions: (list) number of nodes in each layer\n",
    "        :param drop_prob: drop probability for dropout layers. Only required in part 2 of the assignment\n",
    "        :param reg_lambda: regularization parameter. Only required in part 2 of the assignment\n",
    "        \"\"\"\n",
    "#         np.random.seed(1)\n",
    "        \n",
    "        self.parameters = {}\n",
    "        self.num_layers = len(layer_dimensions)\n",
    "        self.drop_prob = drop_prob\n",
    "        self.reg_lambda = reg_lambda\n",
    "        \n",
    "        # init parameters\n",
    "        for i in range(1, self.num_layers):\n",
    "            d_in  = layer_dimensions[i-1]\n",
    "            d_out = layer_dimensions[i]\n",
    "            self.parameters[\"layer\"+str(i)] = \\\n",
    "                ( (np.random.randn(d_out, d_in) * np.sqrt(2/(d_in+d_out))), \n",
    "                     np.zeros(d_out) )\n",
    "                \n",
    "#         print(\"num weights:\", len(parameters))\n",
    "                #             self.parameters[\"b\"+str(i)] = np.zeros(d_out)\n",
    "        \n",
    "\n",
    "    def affineForward(self, A, W, b):\n",
    "        \"\"\"\n",
    "        Forward pass for the affine layer.\n",
    "        :param A: input matrix, shape (L, S), where L is the number of hidden units in the previous layer and S is\n",
    "        the number of samples\n",
    "        :returns: the affine product WA + b, along with the cache required for the backward pass\n",
    "        \"\"\"\n",
    "#         lin = np.matmul(A.T, W)\n",
    "#         print(\"\\t\", str(A.shape)+\".T\", \"•\", W.shape, \"=\", lin.shape)\n",
    "        \n",
    "\n",
    "#         return aff.T\n",
    "        \n",
    "        lin = np.matmul(W, A)\n",
    "#         print(\"\\t\", str(W.shape), \"•\", A.shape, \"=\", lin.shape)\n",
    "        \n",
    "        aff = lin + b[:, None]\n",
    "#         print(\"\\t(\", lin.shape, \"+\", b[:, None].shape, \").T =\", aff.T.shape)\n",
    "\n",
    "        \n",
    "        return aff, [A, W, aff]     # A which is the activations from the previous layer\n",
    "                                    # and W which are the weights from the current layer \n",
    "                                    # are returned as the cache for backprop\n",
    "    \n",
    "\n",
    "    def activationForward(self, A, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Common interface to access all activation functions.\n",
    "        :param A: input to the activation function\n",
    "        :param prob: activation funciton to apply to A. Just \"relu\" for this assignment.\n",
    "        :returns: activation(A)\n",
    "        \"\"\" \n",
    "        return getattr(self,activation)(A)\n",
    "\n",
    "\n",
    "    def relu(self, X):\n",
    "        return np.maximum(X, 0)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def softmax(self, X):\n",
    "        ex = np.exp(X - np.max(X, axis=0))\n",
    "        return ex / np.sum(ex, axis=0)\n",
    "    \n",
    "#     def softmax_derivative(self, dx, cached_x):\n",
    "#         sm = softmax(cached_x)\n",
    "#         sigma_prime = sm * (np.ones_like(cached_x) - sm)\n",
    "#         return dx * sigma_prime\n",
    "        \n",
    "    def dropout(self, A, prob):\n",
    "        \"\"\"\n",
    "        :param A: Activation\n",
    "        :param prob: drop prob\n",
    "        :returns: tuple (A, M) \n",
    "            WHERE\n",
    "            A is matrix after applying dropout\n",
    "            M is dropout mask, used in the backward pass\n",
    "        \"\"\"\n",
    "        \n",
    "        M = np.random.rand(A.shape[0], A.shape[1])\n",
    "        M = (M > prob) * 1.0\n",
    "        scale_up_prob = 1 / (1 - prob)\n",
    "        M *= scale_up_prob\n",
    "        A *= M\n",
    "        \n",
    "        return A, M\n",
    "\n",
    "    def forwardPropagation(self, X):\n",
    "        \"\"\"\n",
    "        Runs an input X through the neural network to compute activations\n",
    "        for all layers. Returns the output computed at the last layer along\n",
    "        with the cache required for backpropagation.\n",
    "        :returns: (tuple) AL, cache\n",
    "            WHERE \n",
    "            AL is activation of last layer\n",
    "            cache is cached values for each layer that\n",
    "                     are needed in further steps\n",
    "        \"\"\"\n",
    "        \n",
    "        cache = []\n",
    "        A = X #A0 is the input\n",
    "        for i in range(1, self.num_layers):\n",
    "#             print(\"compute:\", i)\n",
    "#             cache.append(A)\n",
    "            \n",
    "#             W = self.parameters[\"w\"+str(i)]\n",
    "#             b = self.parameters[\"b\"+str(i)]\n",
    "            W, b = self.parameters[\"layer\"+str(i)]\n",
    "            Z, layer_cache = self.affineForward(A, W, b)\n",
    "            \n",
    "            cache.extend(layer_cache)\n",
    "            \n",
    "            A = self.activationForward(Z, 'relu')\n",
    "            \n",
    "            \n",
    "        AL = self.softmax(A)\n",
    "        \n",
    "        return AL, cache\n",
    "    \n",
    "    \n",
    "    def costFunction(self, AL, y):\n",
    "        \"\"\"\n",
    "        :param AL: Activation of last layer, shape (num_classes, S)\n",
    "        :param y: labels, shape (S)\n",
    "        :param alpha: regularization parameter\n",
    "        :returns cost, dAL: A scalar denoting cost and the gradient of cost\n",
    "        \"\"\"\n",
    "        # compute loss\n",
    "        N = y.shape[0]\n",
    "        AL_softmax = self.softmax(AL)\n",
    "        correct_label_prob = AL_softmax[y, range(N)]\n",
    "        cost = -np.sum(np.log(correct_label_prob)) / N\n",
    "        \n",
    "#         if self.reg_lambda > 0:\n",
    "#             # add regularization\n",
    "#             cost += self.reg_lambda * np.sum(paramiters ** 2)\n",
    "            \n",
    "        # gradient of cost\n",
    "#         dAL = np.sum(-Y / y_pred) / len(y)\n",
    "#         dAL = np.zeros_like(AL)\n",
    "        \n",
    "                            # y S\n",
    "        dAL = AL_softmax - 1\n",
    "#         print(\"dAL:\", dAL.shape)\n",
    "    \n",
    "        return cost, dAL\n",
    "\n",
    "    def affineBackward(self, dA_prev, cache):\n",
    "        \"\"\"\n",
    "        Backward pass for the affine layer.\n",
    "        :param dA_prev: gradient from the next layer.\n",
    "        :param cache: cache returned in affineForward\n",
    "        :returns dA: gradient on the input to this layer\n",
    "                 dW: gradient on the weights\n",
    "                 db: gradient on the bias\n",
    "        \"\"\"\n",
    "#         print(dA_prev.shape, cache.shape)\n",
    "        W = cache.pop()\n",
    "        A = cache.pop()\n",
    "#         print(\"W shapppppeeee\",W.shape)\n",
    "#         print(\"A shapppppeeee\",A.shape)\n",
    "#         # TILE IS WROOOOOOOONGGGGG\n",
    "#         print(\"np.shape(np.sum(A.T, axis=0))\", np.shape(np.sum(A.T, axis=0)))\n",
    "#         print(\"np.shape(np.sum(W.T, axis=0))\", np.shape(np.sum(W, axis=0)))\n",
    "        dW = np.tile( np.sum(A.T, axis=0), (W.shape[0], 1) )\n",
    "#         print(\"dW shape\", np.shape(dW))\n",
    "        \n",
    "        \n",
    "#         print(\"dA_prev shpe\", np.shape(dA_prev))\n",
    "        W_ = (np.tile( np.sum(W, axis=0), ( dA_prev.shape[0], 1 ) ))\n",
    "#         print(\"W_ shape\", np.shape(W_))\n",
    "\n",
    "        dA = np.matmul(W_.T, dA_prev)\n",
    "#         print(\"dA shape\", np.shape(dA))\n",
    "\n",
    "        \n",
    "        \n",
    "#         dA = np.tile( [1,2,3,4,5], ( A.shape[1], 1 ) )\n",
    "        \n",
    "#         print(\"W rows\", W.shape[0])\n",
    "        db = np.ones((W.shape[0],1))\n",
    "        db = np.sum(dA_prev, axis=1)\n",
    "        \n",
    "#         print(\"the shapes(w,a,b):\", dW.shape, dA.shape, db.shape, \"\\tdaprev\", dA_prev.shape)\n",
    "        \n",
    "        \n",
    "#         dW = np.sum(X.T, axis=0)\n",
    "#         dA = dA_prev.dot(dW)\n",
    "        return dA, dW, db\n",
    "    \n",
    "\n",
    "    def activationBackward(self, dA, cache, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Interface to call backward on activation functions.\n",
    "        In this case, it's just relu. \n",
    "        \"\"\"\n",
    "        return getattr(self,activation+\"_derivative\")(dA, cache.pop())\n",
    "\n",
    "        \n",
    "    def relu_derivative(self, dx, cached_x):\n",
    "        sigma_prime = np.zeros_like(cached_x)\n",
    "        \n",
    "#         print(\"s':\", sigma_prime.shape, \"dx: \", dx.shape, \"cachedx:\", cached_x.shape)\n",
    "        \n",
    "        sigma_prime[ cached_x > 0] = 1\n",
    "        \n",
    "        return dx * sigma_prime\n",
    "\n",
    "    def dropout_backward(self, dA, cache):\n",
    "\n",
    "        return dA\n",
    "\n",
    "    def backPropagation(self, dAL, Y, cache):\n",
    "        \"\"\"\n",
    "        Run backpropagation to compute gradients on all paramters in the model\n",
    "        :param dAL: gradient on the last layer of the network. Returned by the cost function.\n",
    "        :param Y: labels\n",
    "        :param cache: cached values during forwardprop\n",
    "        :returns gradients: dW and db for each weight/bias\n",
    "        \"\"\"\n",
    "#         print(\"cache shape: {:d}\".format(len(cache)))\n",
    "        gradients = {}\n",
    "        upstream_grad = one_hot(Y, num_classes=2) * dAL\n",
    "#         print(\"upstream grad: \", upstream_grad.shape)\n",
    "        cache.pop() # get rid of aff because the last layer doesn't use relu_backward\n",
    "        dA, dW, db = self.affineBackward(upstream_grad, cache)\n",
    "        gradients[\"layer\"+str(self.num_layers-1)] = (dW, db)\n",
    "        \n",
    "        for i in reversed(range(1, self.num_layers-1)): # loop through the layers backwards\n",
    "#             print(\"backprop through layer \"+str(i))\n",
    "#             [print(c.shape) for c in cache]\n",
    "#             print(\"----\")\n",
    "            \n",
    "            dA = self.activationBackward(dA, cache)\n",
    "#             db = dA\n",
    "            dA, dW, db = self.affineBackward(dA, cache)\n",
    "#             print(\"cache shape: {:d}\".format(len(cache)))\n",
    "\n",
    "            gradients[\"layer\"+str(i)] = (dW, db)\n",
    "#             gradients[\"dw\"+str(i)] = dW\n",
    "#             gradients[\"db\"+str(i)] = db\n",
    "#             if self.drop_prob > 0:\n",
    "                #call dropout_backward\n",
    "           \n",
    "            \n",
    "#         if self.reg_lambda > 0:\n",
    "            # add gradients from L2 regularization to each dW\n",
    "        \n",
    "        return gradients\n",
    "\n",
    "\n",
    "    def updateParameters(self, gradients, alpha):\n",
    "        \"\"\"\n",
    "        :param gradients: gradients for each weight/bias\n",
    "        :param alpha: step size for gradient descent \n",
    "        \"\"\"\n",
    "#         self.parameters -= alpha * gradients\n",
    "        for key in self.parameters:\n",
    "#             print(\"-->\",key,\"<--\")\n",
    "#             if (key != \"layer2\"):\n",
    "#                 return \n",
    "            \n",
    "#             print(\"updating \", key)\n",
    "\n",
    "            w, b = self.parameters[key]\n",
    "            \n",
    "            w -= alpha * gradients[key][0]\n",
    "            b -= alpha * gradients[key][1]\n",
    "\n",
    "            self.parameters[key] = (w, b)\n",
    "\n",
    "            \n",
    "    def train(self, X, y, iters=1000, alpha=0.0001, batch_size=100, print_every=100):\n",
    "        \"\"\"\n",
    "        :param X: input samples, each column is a sample\n",
    "        :param y: labels for input samples, y.shape[0] must equal X.shape[1]\n",
    "        :param iters: number of training iterations\n",
    "        :param alpha: step size for gradient descent\n",
    "        :param batch_size: number of samples in a minibatch\n",
    "        :param print_every: no. of iterations to print debug info after\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(0, iters):\n",
    "            # get minibatch\n",
    "#                 X_mb, y_mb = get_batch(X, y, batch_size)\n",
    "            # forward prop\n",
    "            AL, cache = self.forwardPropagation(X)\n",
    "            # compute loss\n",
    "            cost, dAL = self.costFunction(AL, y)\n",
    "            # compute gradients\n",
    "            gradients = self.backPropagation(dAL, y, cache)  \n",
    "#             print(gradients)\n",
    "#             print(\"done backproping\")\n",
    "    \n",
    "            # update weights and biases based on gradient\n",
    "\n",
    "#             if i==1:\n",
    "            self.updateParameters(gradients, alpha)\n",
    "            \n",
    "\n",
    "            if i % print_every == 0:\n",
    "                # print cost, train and validation set accuracies\n",
    "#                 print(\"should print cost \"+str(i))\n",
    "                print(\"COST:\",cost)\n",
    "                \n",
    "                y_hat = NN.predict(X)\n",
    "                y_hat_hard = np.argmax(y_hat, axis=0)\n",
    "\n",
    "                print(\"y_hat:\", np.round(y_hat,2))\n",
    "                print(\"y_hard:\", y_hat_hard)\n",
    "                print(\"labels:\", y)\n",
    "                print(\"acc:\", np.mean(y_hat_hard == y))\n",
    "#                 print(gradients)\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions for each sample\n",
    "        \"\"\"\n",
    "        AL, cache = NN.forwardPropagation(X)\n",
    "        \n",
    "        y_pred = self.softmax(AL)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def get_batch(self, X, y, batch_size):\n",
    "        \"\"\"\n",
    "        Return minibatch of samples and labels\n",
    "        \n",
    "        :param X, y: samples and corresponding labels\n",
    "        :parma batch_size: minibatch size\n",
    "        :returns: (tuple) X_batch, y_batch\n",
    "        \"\"\"\n",
    "\n",
    "        return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1]\n",
      "[2, 2]\n",
      "\n",
      "[3 3]\n",
      "[2 4]\n",
      "\n",
      "[0 1 2 3 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3, 4, 0, 1, 2, 3, 4],\n",
       "       [0, 1, 2, 3, 4, 0, 1, 2, 3, 4],\n",
       "       [0, 1, 2, 3, 4, 0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [[1, 1], [2, 2]]\n",
    "\n",
    "[print(r) for r in x]\n",
    "print(\"\")\n",
    "print(np.sum(x, axis=0))\n",
    "print(np.sum(x, axis=1))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "x = np.arange(5)\n",
    "print(x)\n",
    "# np.repeat(x, [5, 1], axis=0)\n",
    "np.tile(x, (3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-231-e55015863352>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlayer_dimensions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# including the input and output layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_dimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# y_hat = NN.predict(X_train[:, :n])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-229-c6ff0b8bf7b1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, iters, alpha, batch_size, print_every)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdAL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcostFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;31m# compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackPropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;31m#             print(gradients)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;31m#             print(\"done backproping\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-229-c6ff0b8bf7b1>\u001b[0m in \u001b[0;36mbackPropagation\u001b[0;34m(self, dAL, Y, cache)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;31m#         print(\"cache shape: {:d}\".format(len(cache)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mupstream_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdAL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;31m#         print(\"upstream grad: \", upstream_grad.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get rid of aff because the last layer doesn't use relu_backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-74bfcbef3090>\u001b[0m in \u001b[0;36mone_hot\u001b[0;34m(y, num_classes)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \"\"\"\n\u001b[1;32m     19\u001b[0m     \u001b[0my_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0my_one_hot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my_one_hot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "n = 11\n",
    "\n",
    "layer_dimensions = [X_train.shape[0], 200, 10]  # including the input and output layers\n",
    "NN = NeuralNetwork(layer_dimensions)\n",
    "NN.train(X_train[:, :n],np.array([4,4,4,4,4,4,4,4,4,4,4]), iters=1000, alpha=0.01, batch_size=128, print_every=100)\n",
    "\n",
    "# y_hat = NN.predict(X_train[:, :n])\n",
    "# y_hat_hard = np.argmax(y_hat, axis=0)\n",
    "\n",
    "# print(np.round(y_hat,2))\n",
    "# print(y_hat_hard)\n",
    "# print(y_train[:n])\n",
    "# print(np.mean(y_hat_hard == y_train[:n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,) (1,)\n",
      "[[1]]\n",
      "[1]\n",
      "(1, 1) (1,)\n",
      "COST: 0.69314718056\n",
      "y_hat: [[ 0.5]\n",
      " [ 0.5]]\n",
      "y_hard: [0]\n",
      "labels: [1]\n",
      "acc: 0.0\n",
      "COST: 0.69314718056\n",
      "y_hat: [[ 0.5]\n",
      " [ 0.5]]\n",
      "y_hard: [0]\n",
      "labels: [1]\n",
      "acc: 0.0\n",
      "COST: 0.69314718056\n",
      "y_hat: [[ 0.5]\n",
      " [ 0.5]]\n",
      "y_hard: [0]\n",
      "labels: [1]\n",
      "acc: 0.0\n",
      "COST: 0.69314718056\n",
      "y_hat: [[ 0.5]\n",
      " [ 0.5]]\n",
      "y_hard: [0]\n",
      "labels: [1]\n",
      "acc: 0.0\n",
      "COST: 0.69314718056\n",
      "y_hat: [[ 0.5]\n",
      " [ 0.5]]\n",
      "y_hard: [0]\n",
      "labels: [1]\n",
      "acc: 0.0\n",
      "COST: 0.69314718056\n",
      "y_hat: [[ 0.5]\n",
      " [ 0.5]]\n",
      "y_hard: [0]\n",
      "labels: [1]\n",
      "acc: 0.0\n",
      "COST: 0.69314718056\n",
      "y_hat: [[ 0.5]\n",
      " [ 0.5]]\n",
      "y_hard: [0]\n",
      "labels: [1]\n",
      "acc: 0.0\n",
      "COST: 0.69314718056\n",
      "y_hat: [[ 0.5]\n",
      " [ 0.5]]\n",
      "y_hard: [0]\n",
      "labels: [1]\n",
      "acc: 0.0\n",
      "COST: 0.69314718056\n",
      "y_hat: [[ 0.5]\n",
      " [ 0.5]]\n",
      "y_hard: [0]\n",
      "labels: [1]\n",
      "acc: 0.0\n",
      "COST: 0.69314718056\n",
      "y_hat: [[ 0.5]\n",
      " [ 0.5]]\n",
      "y_hard: [0]\n",
      "labels: [1]\n",
      "acc: 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkhpi9CWpIUZfkhpi9CWpIUZfkhry/7/W2w2IlqtZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1694cffd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.hstack( (np.random.randn(10)+5, np.random.randn(10)-5) )\n",
    "y = np.array([0]*10 + [1]*10)\n",
    "\n",
    "X = np.array([1])\n",
    "y = np.array([1])\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "plt.plot(X, y, 'o')\n",
    "\n",
    "X = X[None, :]\n",
    "\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "layer_dimensions = [1, 2, 2]  # including the input and output layers\n",
    "NN = NeuralNetwork(layer_dimensions)\n",
    "NN.train(X,y, iters=10, alpha=0.01, batch_size=128, print_every=1)\n",
    "\n",
    "# y_hat = NN.predict(X_train[:, :n])\n",
    "# y_hat_hard = np.argmax(y_hat, axis=0)\n",
    "\n",
    "# print(np.round(y_hat,2))\n",
    "# print(y_hat_hard)\n",
    "# print(y_train[:n])\n",
    "# print(np.mean(y_hat_hard == y_train[:n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax([[0, 0, 1, 0], [1, 0, 0, 0]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Functions to load data, DO NOT change these\n",
    "\n",
    "def get_labels(folder, label2id):\n",
    "    \"\"\"\n",
    "    Returns vector of labels extracted from filenames of all files in folder\n",
    "    :param folder: path to data folder\n",
    "    :param label2id: mapping of text labels to numeric ids. (Eg: automobile -> 0)\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    y = []\n",
    "    for f in files:\n",
    "        y.append(get_label(f,label2id))\n",
    "    return np.array(y)\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts each label index in y to vector with one_hot encoding\n",
    "    \"\"\"\n",
    "    y_one_hot = np.zeros((num_classes, y.shape[0]))\n",
    "    y_one_hot[y, range(y.shape[0])] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "def get_label_mapping(label_file):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        id2label = f.readlines()\n",
    "        id2label = [l.strip() for l in id2label]\n",
    "    label2id = {}\n",
    "    count = 0\n",
    "    for label in id2label:\n",
    "        label2id[label] = count\n",
    "        count += 1\n",
    "    return id2label, label2id\n",
    "\n",
    "def get_images(folder):\n",
    "    \"\"\"\n",
    "    returns numpy array of all samples in folder\n",
    "    each column is a sample resized to 30x30 and flattened\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    images = []\n",
    "    count = 0\n",
    "    \n",
    "    for f in files:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "        img_arr = get_img_array(f)\n",
    "        img_arr = img_arr.flatten() / 255.0\n",
    "        images.append(img_arr)\n",
    "    X = np.column_stack(images)\n",
    "\n",
    "    return X\n",
    "\n",
    "def get_train_data(data_root_path):\n",
    "    \"\"\"\n",
    "    Return X and y\n",
    "    \"\"\"\n",
    "    train_data_path = data_root_path + 'train'\n",
    "    id2label, label2id = get_label_mapping(data_root_path+'labels.txt')\n",
    "    print(label2id)\n",
    "    X = get_images(train_data_path)\n",
    "    y = get_labels(train_data_path, label2id)\n",
    "    return X, y\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    \"\"\"\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(np.array([4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Helper functions, DO NOT modify this\n",
    "\n",
    "def get_img_array(path):\n",
    "    \"\"\"\n",
    "    Given path of image, returns it's numpy array\n",
    "    \"\"\"\n",
    "    return scipy.misc.imread(path)\n",
    "\n",
    "def get_files(folder):\n",
    "    \"\"\"\n",
    "    Given path to folder, returns list of files in it\n",
    "    \"\"\"\n",
    "    filenames = [file for file in glob.glob(folder+'*/*')]\n",
    "    filenames.sort()\n",
    "    return filenames\n",
    "\n",
    "def get_label(filepath, label2id):\n",
    "    \"\"\"\n",
    "    Files are assumed to be labeled as: /path/to/file/999_frog.png\n",
    "    Returns label for a filepath\n",
    "    \"\"\"\n",
    "    tokens = filepath.split('/')\n",
    "    label = tokens[-1].split('_')[1][:-4]\n",
    "    if label in label2id:\n",
    "        return label2id[label]\n",
    "    else:\n",
    "        sys.exit(\"Invalid label: \" + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel/__main__.py:7: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000/50000\n",
      "Loaded 20000/50000\n",
      "Loaded 30000/50000\n",
      "Loaded 40000/50000\n",
      "Loaded 50000/50000\n",
      "Loaded 10000/10000\n",
      "Data loading done\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_root_path = 'cifar10-hw1/'\n",
    "X_train, y_train = get_train_data(data_root_path) # this may take a few minutes\n",
    "X_test = get_images(data_root_path + 'test')\n",
    "print('Data loading done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### run some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight and bias dimentions and norms\n",
      "[['layer1', (500, 100), (500,)], ['layer2', (100, 500), (100,)], ['layer3', (30, 100), (30,)], ['layer4', (10, 30), (10,)]]\n",
      "[(166.89464867576024, 0.0), (165.33499556380119, 0.0), (47.252557570903775, 0.0), (16.241798557370469, 0.0)]\n",
      "\n",
      "test affineForward\n",
      "\t (2, 2) • (2, 2) = (2, 2)\n",
      "\t( (2, 2) + (2, 1) ).T = (2, 2)\n",
      "(array([[4, 1],\n",
      "       [2, 2]]), [array([[1, 0],\n",
      "       [0, 1]]), array([[4, 1],\n",
      "       [2, 2]]), array([[4, 1],\n",
      "       [2, 2]])])\n",
      "(2,)\n",
      "\t (2, 2) • (2,) = (2,)\n",
      "\t( (2,) + (2, 1) ).T = (2, 2)\n",
      "(array([[5, 4],\n",
      "       [5, 4]]), [array([1, 1]), array([[4, 1],\n",
      "       [2, 2]]), array([[5, 4],\n",
      "       [5, 4]])])\n",
      "\n",
      "test activation\n",
      "[0.1, 0.2, -0.2, 1, -1, -1, -1, 1, 1, 1, 2]\n",
      "[ 0.1  0.2  0.   1.   0.   0.   0.   1.   1.   1.   2. ]\n",
      "[ 0.1  0.2  0.   1.   0.   0.   0.   1.   1.   1.   2. ]\n",
      "\n",
      "\n",
      "\n",
      "test forward prop\n",
      "compute: 0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'layer0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6b8cf85c25fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\ntest forward prop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforwardPropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nAL:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-96a13901374f>\u001b[0m in \u001b[0;36mforwardPropagation\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;31m#             W = self.parameters[\"w\"+str(i)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;31m#             b = self.parameters[\"b\"+str(i)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"layer\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffineForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'layer0'"
     ]
    }
   ],
   "source": [
    "layer_dimensions = [X_train.shape[0], 100, 500, 100, 30, 10]  # including the input and output layers\n",
    "NN = NeuralNetwork(layer_dimensions)\n",
    "# NN.train(X_train, y_train, iters=10, alpha=0, batch_size=128, print_every=10)\n",
    "\n",
    "# print([k for k in NN.parameters.keys()])\n",
    "# print([v.shape for v in NN.parameters.values()])\n",
    "\n",
    "print(\"weight and bias dimentions and norms\")\n",
    "print([[k, v[0].shape, v[1].shape] for k,v in NN.parameters.items()])\n",
    "print([( np.sum(v[0] ** 2), np.sum(v[1] ** 2)) for k,v in NN.parameters.items()])\n",
    "print()\n",
    "\n",
    "print(\"test affineForward\")\n",
    "A = np.array([[1, 0], [0, 1]])\n",
    "W = np.array([[4, 1], \n",
    "     [2, 2]])\n",
    "print(NN.affineForward(A, W, np.array([0,0]).T))\n",
    "A = np.array([1,1])\n",
    "print(A.shape)\n",
    "print(NN.affineForward(A, W, np.array([0,0])))\n",
    "\n",
    "print(\"\\ntest activation\")\n",
    "z = [.1, .2, -.2, 1, -1, -1, -1, 1, 1, 1, 2]\n",
    "print(z)\n",
    "print(NN.relu(z))\n",
    "print(NN.activationForward(z))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_train[:, 0].shape)\n",
    "# print(X_train[1].shape)\n",
    "\n",
    "\n",
    "print(\"\\ntest forward prop\")\n",
    "AL, cache = NN.forwardPropagation(X_train[:, :10000])\n",
    "print(\"\\nAL:\", AL.shape)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"test cost\")\n",
    "cost, dAL = NN.costFunction(AL, y_train[:10000])\n",
    "print(\"cost: (\", cost, \"?=\", 2.3, \")  (\", np.exp(-cost), \"?=\", 0.1,\")\") # cost \\approx    2.3  \\approx    -log(1/10)     => :-)\n",
    "print(\"\\t/\\ should be about 2.3 and .1\\n\\n\")\n",
    "\n",
    "y_hat = NN.predict(X_train[:, :10000])\n",
    "\n",
    "print(\"\\n\\ntest predict\\n\")\n",
    "print(\"predict: \\n\", y_hat.shape)\n",
    "\n",
    "print(\"\\nacc: \", np.mean(np.argmax(y_hat, axis=0) == y_train[:10000]))\n",
    "print(\"\\t/\\ should be about .1\")\n",
    "\n",
    "print(\"\\n\\n\\n\", np.row_stack( (y_hat[:, :5], np.sum( y_hat[:, :5], axis=0)) ) )  # last row = 1  => :-)\n",
    "print(\"\\t/\\ should all add to 1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### check the distribution on the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_hat.shape\n",
    "p = np.argmax(y_hat, axis = 0)\n",
    "print(p.shape)\n",
    "print(\"y_\", y_hat[:, 1000])\n",
    "\n",
    "print(np.argmax(y_hat[:, 1000]))\n",
    "print(np.max(y_hat[:, 1000]))\n",
    "\n",
    "print(\"p:\", p[1000])\n",
    "plt.hist(p)\n",
    "\n",
    "print(\"mean:\", np.mean(y_hat, axis=1))\n",
    "prob_mass = np.sum(y_hat, axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(prob_mass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### tests for backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"test cost\")\n",
    "cost, dAL = NN.costFunction(AL, y_train[:10000])\n",
    "print(\"cost: {0:.2f}\".format(cost), \"dAL:\", dAL)\n",
    "\n",
    "\n",
    "# NN.affineBackward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "i = 39\n",
    "n = 5\n",
    "print(y_train[i:i+n])\n",
    "print(dAL[:, i:i+n])\n",
    "print(np.argmax(y_hat[:, i:i+n], axis=0))\n",
    "print(1*(np.equal(y_train[i:i+n], np.argmax(y_hat[:, i:i+n], axis=0))))\n",
    "\n",
    "np.sum( np.equal(y_train[:10000], np.argmax(y_hat, axis=0) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Split train into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# X_train = ...\n",
    "# y_train = ...\n",
    "\n",
    "# X_val   \n",
    "# y_val1\n",
    "\n",
    "idx = np.random.permutation(len(y_train))\n",
    "X_all = X_train[:, idx]\n",
    "y_all = y_train[idx]\n",
    "\n",
    "m_train = int(len(y_all)*0.9);\n",
    "m_val   = len(y_all)-m_train\n",
    "\n",
    "X       = X_all[:, :m_train]\n",
    "y       = y_all[:m_train]\n",
    "\n",
    "X_val   = X_all[:, m_train:]\n",
    "y_val   = y_all[m_train:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(X_all.shape)\n",
    "print(y_all.shape)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(y)\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 1\n",
    "\n",
    "#### Simple fully-connected deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "layer_dimensions = [X_train.shape[0], 100, 10]  # including the input and output layers\n",
    "NN = NeuralNetwork(layer_dimensions)\n",
    "NN.train(X_train, y_train, iters=10, alpha=0, batch_size=128, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_predicted = NN.predict(X_test)\n",
    "save_predictions('ans1-uni', y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# test if your numpy file has been saved correctly\n",
    "loaded_y = np.load('ans1-uni.npy')\n",
    "print(loaded_y.shape)\n",
    "loaded_y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 2: Regularizing the neural network\n",
    "#### Add dropout and L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "NN2 = NeuralNetwork(layer_dimensions, drop_prob=0, reg_lambda=0)\n",
    "NN2.train(X_train, y_train, iters=1000, alpha=0.00001, batch_size=1000, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_predicted2 = NN2.predict(X)\n",
    "save_predictions(y_predicted, 'ans2-uni')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

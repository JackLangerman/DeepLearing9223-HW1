{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# CS-GY 9223-E: Deep Learning Homework 1\n",
    "Due on Sunday, 11th February 2018, 11:55 PM\n",
    "\n",
    "This homework can be done in pairs. Everyone must submit on NYU Classes individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Write down the UNIs (NetIDs) of your group (if applicable)\n",
    "\n",
    "Member 1: Name, NetID\n",
    "\n",
    "Member 2: Name, NetID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import glob\n",
    "import sys\n",
    "# you shouldn't need to make any more imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    Abstraction of neural network.\n",
    "    Stores parameters, activations, cached values. \n",
    "    Provides necessary functions for training and prediction. \n",
    "    \"\"\"\n",
    "    def __init__(self, layer_dimensions, drop_prob=0.0, reg_lambda=0.0):\n",
    "        \"\"\"\n",
    "        Initializes the weights and biases for each layer\n",
    "        :param layer_dimensions: (list) number of nodes in each layer\n",
    "        :param drop_prob: drop probability for dropout layers. Only required in part 2 of the assignment\n",
    "        :param reg_lambda: regularization parameter. Only required in part 2 of the assignment\n",
    "        \"\"\"\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        self.parameters = {}\n",
    "        self.num_layers = len(layer_dimensions)\n",
    "        self.drop_prob = drop_prob\n",
    "        self.reg_lambda = []\n",
    "        \n",
    "        # init parameters\n",
    "        for i in range(self.num_layers - 1):\n",
    "            d_in  = layer_dimensions[i]\n",
    "            d_out = layer_dimensions[i+1]\n",
    "#             self.parameters[\"w\"+str(i)] = np.random.randn(d_in, d_out) / np.sqrt((d_in+d_out)/2)\n",
    "#             self.parameters[\"w\"+str(i)] = np.random.randn(d_in, d_out) / np.sqrt(d_in)\n",
    "            self.parameters[\"w\"+str(i)] = np.random.randn(d_out, d_in) * np.sqrt(2/(d_in+d_out))\n",
    "            self.parameters[\"b\"+str(i)] = np.zeros(d_out)\n",
    "        \n",
    "\n",
    "    def affineForward(self, A, W, b):\n",
    "        \"\"\"\n",
    "        Forward pass for the affine layer.\n",
    "        :param A: input matrix, shape (L, S), where L is the number of hidden units in the previous layer and S is\n",
    "        the number of samples\n",
    "        :returns: the affine product WA + b, along with the cache required for the backward pass\n",
    "        \"\"\"\n",
    "#         lin = np.matmul(A.T, W)\n",
    "#         print(\"\\t\", str(A.shape)+\".T\", \"•\", W.shape, \"=\", lin.shape)\n",
    "        \n",
    "#         aff = lin + b\n",
    "#         print(\"\\t(\", lin.shape, \"+\", b.shape, \").T =\", aff.T.shape)\n",
    "\n",
    "#         return aff.T\n",
    "        \n",
    "        lin = np.matmul(W, A)\n",
    "        print(\"\\t\", str(W.shape), \"•\", A.shape, \"=\", lin.shape)\n",
    "        \n",
    "        aff = lin + b[:, None]\n",
    "        print(\"\\t(\", lin.shape, \"+\", b[:, None].shape, \").T =\", aff.T.shape)\n",
    "\n",
    "        return aff, A  # A is returned as the cache\n",
    "    \n",
    "\n",
    "    def activationForward(self, A, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Common interface to access all activation functions.\n",
    "        :param A: input to the activation function\n",
    "        :param prob: activation funciton to apply to A. Just \"relu\" for this assignment.\n",
    "        :returns: activation(A)\n",
    "        \"\"\" \n",
    "        return getattr(self,activation)(A)\n",
    "\n",
    "\n",
    "    def relu(self, X):\n",
    "        return np.maximum(X, 0)\n",
    "    \n",
    "    \n",
    "#     def tanh(self, X):\n",
    "#         return (np.exp(X)-np.exp(-X)) / (np.exp(X)+np.exp(-X))\n",
    "\n",
    "\n",
    "    def softmax(self, X):\n",
    "        ex = np.exp(X)\n",
    "        return ex / np.sum(ex, axis=0)\n",
    "    \n",
    "\n",
    "    def dropout(self, A, prob):\n",
    "        \"\"\"\n",
    "        :param A: Activation\n",
    "        :param prob: drop prob\n",
    "        :returns: tuple (A, M) \n",
    "            WHERE\n",
    "            A is matrix after applying dropout\n",
    "            M is dropout mask, used in the backward pass\n",
    "        \"\"\"\n",
    "        \n",
    "        return A, M\n",
    "\n",
    "    def forwardPropagation(self, X):\n",
    "        \"\"\"\n",
    "        Runs an input X through the neural network to compute activations\n",
    "        for all layers. Returns the output computed at the last layer along\n",
    "        with the cache required for backpropagation.\n",
    "        :returns: (tuple) AL, cache\n",
    "            WHERE \n",
    "            AL is activation of last layer\n",
    "            cache is cached values for each layer that\n",
    "                     are needed in further steps\n",
    "        \"\"\"\n",
    "        \n",
    "        cache = []\n",
    "        A = X #A0 is the input\n",
    "        for i in range(self.num_layers-1):\n",
    "            print(\"compute:\", i)\n",
    "            cache.append(A)\n",
    "            \n",
    "            W = self.parameters[\"w\"+str(i)]\n",
    "            b = self.parameters[\"b\"+str(i)]\n",
    "            Z, cache = self.affineForward(A, W, b)\n",
    "            A = self.activationForward(Z, 'relu')\n",
    "            \n",
    "            \n",
    "        AL = self.softmax(A)\n",
    "        \n",
    "        return AL, cache\n",
    "    \n",
    "    def costFunction(self, AL, y):\n",
    "        \"\"\"\n",
    "        :param AL: Activation of last layer, shape (num_classes, S)\n",
    "        :param y: labels, shape (S)\n",
    "        :param alpha: regularization parameter\n",
    "        :returns cost, dAL: A scalar denoting cost and the gradient of cost\n",
    "        \"\"\"\n",
    "        # compute loss\n",
    "#         print(\"y.sh, AL.sh\", y.shape, AL.shape)\n",
    "#         print(AL)\n",
    "\n",
    "        Y = np.zeros_like(AL)\n",
    "#         print(\"YYY:\", Y.shape, y.shape, len(y))\n",
    "        Y[y, np.arange(len(y))] = 1\n",
    "#         print(Y)\n",
    "        \n",
    "#         y_pred = np.exp(AL) / np.sum(np.exp(AL), axis=0)\n",
    "        y_pred = AL\n",
    "\n",
    "        cost = np.sum(-Y * np.log(y_pred)) / len(y)\n",
    "#         cost = np.dot(AL, y)\n",
    "                                   # if AL is un-softmaxed we can interpret AL as \n",
    "                                   # -log(P(y | x))\n",
    "\n",
    "#         p = np.exp(AL)/np.sum(np.exp(AL))\n",
    "#         cost = np.sum(-y * np.log(p))\n",
    "        \n",
    "#         if self.reg_lambda > 0:\n",
    "#             # add regularization\n",
    "#             cost += self.reg_lambda * np.sum(paramiters ** 2)\n",
    "            \n",
    "        # gradient of cost\n",
    "        dAL = np.sum(-Y / y_pred) / len(y)\n",
    "        \n",
    "        return cost, dAL\n",
    "\n",
    "    def affineBackward(self, dA_prev, cache):\n",
    "        \"\"\"\n",
    "        Backward pass for the affine layer.\n",
    "        :param dA_prev: gradient from the next layer.\n",
    "        :param cache: cache returned in affineForward\n",
    "        :returns dA: gradient on the input to this layer\n",
    "                 dW: gradient on the weights\n",
    "                 db: gradient on the bias\n",
    "        \"\"\"\n",
    "        \n",
    "#         lin = np.matmul(W.T, A)\n",
    "#         print(\"\\t\", str(W.shape)+\".T\", \"•\", A.shape, \"=\", lin.shape)\n",
    "        \n",
    "#         aff = lin + b[:, None]\n",
    "#         print(\"\\t(\", lin.shape, \"+\", b[:, None].shape, \").T =\", aff.T.shape)\n",
    "        dA = np.sum(A.T, axis=0)\n",
    "#         print(dA.shape)\n",
    "        \n",
    "        return dA, dW, db\n",
    "\n",
    "    def activationBackward(self, dA, cache, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Interface to call backward on activation functions.\n",
    "        In this case, it's just relu. \n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "        \n",
    "    def relu_derivative(self, dx, cached_x):\n",
    "\n",
    "        return dx\n",
    "\n",
    "    def dropout_backward(self, dA, cache):\n",
    "\n",
    "        return dA\n",
    "\n",
    "    def backPropagation(self, dAL, Y, cache):\n",
    "        \"\"\"\n",
    "        Run backpropagation to compute gradients on all paramters in the model\n",
    "        :param dAL: gradient on the last layer of the network. Returned by the cost function.\n",
    "        :param Y: labels\n",
    "        :param cache: cached values during forwardprop\n",
    "        :returns gradients: dW and db for each weight/bias\n",
    "        \"\"\"\n",
    "        gradients = {}\n",
    "        \n",
    "#         for i in range(0): # fill in\n",
    "            \n",
    "            \n",
    "#             if self.drop_prob > 0:\n",
    "                #call dropout_backward\n",
    "           \n",
    "            \n",
    "#         if self.reg_lambda > 0:\n",
    "            # add gradients from L2 regularization to each dW\n",
    "        \n",
    "        return gradients\n",
    "\n",
    "\n",
    "    def updateParameters(self, gradients, alpha):\n",
    "        \"\"\"\n",
    "        :param gradients: gradients for each weight/bias\n",
    "        :param alpha: step size for gradient descent \n",
    "        \"\"\"\n",
    "\n",
    "    def train(self, X, y, iters=1000, alpha=0.0001, batch_size=100, print_every=100):\n",
    "        \"\"\"\n",
    "        :param X: input samples, each column is a sample\n",
    "        :param y: labels for input samples, y.shape[0] must equal X.shape[1]\n",
    "        :param iters: number of training iterations\n",
    "        :param alpha: step size for gradient descent\n",
    "        :param batch_size: number of samples in a minibatch\n",
    "        :param print_every: no. of iterations to print debug info after\n",
    "        \"\"\"\n",
    "        \n",
    "#         for i in range(0, iters):\n",
    "            # get minibatch\n",
    "            \n",
    "            # forward prop\n",
    "\n",
    "            # compute loss\n",
    "\n",
    "            # compute gradients\n",
    "\n",
    "            # update weights and biases based on gradient\n",
    "\n",
    "#             if i % print_every == 0:\n",
    "                # print cost, train and validation set accuracies\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions for each sample\n",
    "        \"\"\"\n",
    "        AL, cache = NN.forwardPropagation(X)\n",
    "        \n",
    "        y_pred = self.softmax(AL)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def get_batch(self, X, y, batch_size):\n",
    "        \"\"\"\n",
    "        Return minibatch of samples and labels\n",
    "        \n",
    "        :param X, y: samples and corresponding labels\n",
    "        :parma batch_size: minibatch size\n",
    "        :returns: (tuple) X_batch, y_batch\n",
    "        \"\"\"\n",
    "\n",
    "        return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Functions to load data, DO NOT change these\n",
    "\n",
    "def get_labels(folder, label2id):\n",
    "    \"\"\"\n",
    "    Returns vector of labels extracted from filenames of all files in folder\n",
    "    :param folder: path to data folder\n",
    "    :param label2id: mapping of text labels to numeric ids. (Eg: automobile -> 0)\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    y = []\n",
    "    for f in files:\n",
    "        y.append(get_label(f,label2id))\n",
    "    return np.array(y)\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts each label index in y to vector with one_hot encoding\n",
    "    One-hot encoding converts categorical labels to binary values\n",
    "    \"\"\"\n",
    "    y_one_hot = np.zeros((y.shape[0], num_classes))\n",
    "    y_one_hot[y] = 1\n",
    "    return y_one_hot.T\n",
    "\n",
    "def get_label_mapping(label_file):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        id2label = f.readlines()\n",
    "        id2label = [l.strip() for l in id2label]\n",
    "    label2id = {}\n",
    "    count = 0\n",
    "    for label in id2label:\n",
    "        label2id[label] = count\n",
    "        count += 1\n",
    "    return id2label, label2id\n",
    "\n",
    "def get_images(folder):\n",
    "    \"\"\"\n",
    "    returns numpy array of all samples in folder\n",
    "    each column is a sample resized to 30x30 and flattened\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    images = []\n",
    "    count = 0\n",
    "    \n",
    "    for f in files:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "        img_arr = get_img_array(f)\n",
    "        img_arr = img_arr.flatten() / 255.0\n",
    "        images.append(img_arr)\n",
    "    X = np.column_stack(images)\n",
    "\n",
    "    return X\n",
    "\n",
    "def get_train_data(data_root_path):\n",
    "    \"\"\"\n",
    "    Return X and y\n",
    "    \"\"\"\n",
    "    train_data_path = data_root_path + 'train'\n",
    "    id2label, label2id = get_label_mapping(data_root_path+'labels.txt')\n",
    "    print(label2id)\n",
    "    X = get_images(train_data_path)\n",
    "    y = get_labels(train_data_path, label2id)\n",
    "    return X, y\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    \"\"\"\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Helper functions, DO NOT modify this\n",
    "\n",
    "def get_img_array(path):\n",
    "    \"\"\"\n",
    "    Given path of image, returns it's numpy array\n",
    "    \"\"\"\n",
    "    return scipy.misc.imread(path)\n",
    "\n",
    "def get_files(folder):\n",
    "    \"\"\"\n",
    "    Given path to folder, returns list of files in it\n",
    "    \"\"\"\n",
    "    filenames = [file for file in glob.glob(folder+'*/*')]\n",
    "    filenames.sort()\n",
    "    return filenames\n",
    "\n",
    "def get_label(filepath, label2id):\n",
    "    \"\"\"\n",
    "    Files are assumed to be labeled as: /path/to/file/999_frog.png\n",
    "    Returns label for a filepath\n",
    "    \"\"\"\n",
    "    tokens = filepath.split('/')\n",
    "    label = tokens[-1].split('_')[1][:-4]\n",
    "    if label in label2id:\n",
    "        return label2id[label]\n",
    "    else:\n",
    "        sys.exit(\"Invalid label: \" + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel/__main__.py:7: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000/50000\n",
      "Loaded 20000/50000\n",
      "Loaded 30000/50000\n",
      "Loaded 40000/50000\n",
      "Loaded 50000/50000\n",
      "Loaded 10000/10000\n",
      "Data loading done\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_root_path = 'cifar10-hw1/'\n",
    "X_train, y_train = get_train_data(data_root_path) # this may take a few minutes\n",
    "X_test = get_images(data_root_path + 'test')\n",
    "print('Data loading done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### run some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight and bias dimentions and norms\n",
      "[['w0', (100, 3072)], ['b0', (100,)], ['w1', (500, 100)], ['b1', (500,)], ['w2', (100, 500)], ['b2', (100,)], ['w3', (30, 100)], ['b3', (30,)], ['w4', (10, 30)], ['b4', (10,)]]\n",
      "[193.41081813638363, 0.0, 167.42608344452105, 0.0, 167.03653537541445, 0.0, 46.761635217691875, 0.0, 16.579385674361468, 0.0]\n",
      "\n",
      "test affineForward\n",
      "\t (2, 2) • (2, 2) = (2, 2)\n",
      "\t( (2, 2) + (2, 1) ).T = (2, 2)\n",
      "[[4 1]\n",
      " [2 2]]\n",
      "(2,)\n",
      "\t (2, 2) • (2,) = (2,)\n",
      "\t( (2,) + (2, 1) ).T = (2, 2)\n",
      "[[5 4]\n",
      " [5 4]]\n",
      "\n",
      "test activation\n",
      "[0.1, 0.2, -0.2, 1, -1, -1, -1, 1, 1, 1, 2]\n",
      "[ 0.1  0.2  0.   1.   0.   0.   0.   1.   1.   1.   2. ]\n",
      "[ 0.1  0.2  0.   1.   0.   0.   0.   1.   1.   1.   2. ]\n",
      "\n",
      "\n",
      "\n",
      "test forward prop\n",
      "compute: 0\n",
      "\t (100, 3072) • (3072, 10000) = (100, 10000)\n",
      "\t( (100, 10000) + (100, 1) ).T = (10000, 100)\n",
      "compute: 1\n",
      "\t (500, 100) • (100, 10000) = (500, 10000)\n",
      "\t( (500, 10000) + (500, 1) ).T = (10000, 500)\n",
      "compute: 2\n",
      "\t (100, 500) • (500, 10000) = (100, 10000)\n",
      "\t( (100, 10000) + (100, 1) ).T = (10000, 100)\n",
      "compute: 3\n",
      "\t (30, 100) • (100, 10000) = (30, 10000)\n",
      "\t( (30, 10000) + (30, 1) ).T = (10000, 30)\n",
      "compute: 4\n",
      "\t (10, 30) • (30, 10000) = (10, 10000)\n",
      "\t( (10, 10000) + (10, 1) ).T = (10000, 10)\n",
      "\n",
      "AL: (10, 10000)\n",
      "\n",
      "\n",
      "\n",
      "test cost\n",
      "cost: ( 2.3111780303 ?= 2.3 )  ( 0.0991443876461 ?= 0.1 )\n",
      "\t/\\ should be about 2.3 and .1\n",
      "\n",
      "\n",
      "compute: 0\n",
      "\t (100, 3072) • (3072, 10000) = (100, 10000)\n",
      "\t( (100, 10000) + (100, 1) ).T = (10000, 100)\n",
      "compute: 1\n",
      "\t (500, 100) • (100, 10000) = (500, 10000)\n",
      "\t( (500, 10000) + (500, 1) ).T = (10000, 500)\n",
      "compute: 2\n",
      "\t (100, 500) • (500, 10000) = (100, 10000)\n",
      "\t( (100, 10000) + (100, 1) ).T = (10000, 100)\n",
      "compute: 3\n",
      "\t (30, 100) • (100, 10000) = (30, 10000)\n",
      "\t( (30, 10000) + (30, 1) ).T = (10000, 30)\n",
      "compute: 4\n",
      "\t (10, 30) • (30, 10000) = (10, 10000)\n",
      "\t( (10, 10000) + (10, 1) ).T = (10000, 10)\n",
      "\n",
      "\n",
      "test predict\n",
      "\n",
      "predict: \n",
      " (10, 10000)\n",
      "\n",
      "acc:  0.1036\n",
      "\t/\\ should be about .1\n",
      "\n",
      "\n",
      "\n",
      " [[ 0.08403392  0.08448363  0.08264527  0.08799797  0.08567329]\n",
      " [ 0.12648309  0.11511274  0.13003968  0.11478705  0.12278294]\n",
      " [ 0.0932636   0.11250977  0.10238479  0.10357661  0.1072238 ]\n",
      " [ 0.12348586  0.11933967  0.12023393  0.11352379  0.11462763]\n",
      " [ 0.10505715  0.10607284  0.11272883  0.10043404  0.10123254]\n",
      " [ 0.08403392  0.08448363  0.08264527  0.08799797  0.08567329]\n",
      " [ 0.09825707  0.08929151  0.0882691   0.10051824  0.09665382]\n",
      " [ 0.08403392  0.08448363  0.08264527  0.08799797  0.08567329]\n",
      " [ 0.09961577  0.10524427  0.09746372  0.1066646   0.10066173]\n",
      " [ 0.10173569  0.0989783   0.10094414  0.09650177  0.09979767]\n",
      " [ 1.          1.          1.          1.          1.        ]]\n",
      "\t/\\ should all add to 1\n"
     ]
    }
   ],
   "source": [
    "layer_dimensions = [X_train.shape[0], 100, 500, 100, 30, 10]  # including the input and output layers\n",
    "NN = NeuralNetwork(layer_dimensions)\n",
    "NN.train(X_train, y_train, iters=10, alpha=0, batch_size=128, print_every=10)\n",
    "\n",
    "# print([k for k in NN.parameters.keys()])\n",
    "# print([v.shape for v in NN.parameters.values()])\n",
    "\n",
    "print(\"weight and bias dimentions and norms\")\n",
    "print([[k, v.shape] for k,v in NN.parameters.items()])\n",
    "print([np.sum(v ** 2) for k,v in NN.parameters.items()])\n",
    "print()\n",
    "\n",
    "print(\"test affineForward\")\n",
    "A = np.array([[1, 0], [0, 1]])\n",
    "W = np.array([[4, 1], \n",
    "     [2, 2]])\n",
    "print(NN.affineForward(A, W, np.array([0,0]).T))\n",
    "A = np.array([1,1])\n",
    "print(A.shape)\n",
    "print(NN.affineForward(A, W, np.array([0,0])))\n",
    "\n",
    "print(\"\\ntest activation\")\n",
    "z = [.1, .2, -.2, 1, -1, -1, -1, 1, 1, 1, 2]\n",
    "print(z)\n",
    "print(NN.relu(z))\n",
    "print(NN.activationForward(z))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_train[:, 0].shape)\n",
    "# print(X_train[1].shape)\n",
    "\n",
    "\n",
    "print(\"\\ntest forward prop\")\n",
    "AL, cache = NN.forwardPropagation(X_train[:, :10000])\n",
    "print(\"\\nAL:\", AL.shape)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"test cost\")\n",
    "cost, dAL = NN.costFunction(AL, y_train[:10000])\n",
    "print(\"cost: (\", cost, \"?=\", 2.3, \")  (\", np.exp(-cost), \"?=\", 0.1,\")\") # cost \\approx    2.3  \\approx    -log(1/10)     => :-)\n",
    "print(\"\\t/\\ should be about 2.3 and .1\\n\\n\")\n",
    "\n",
    "y_hat = NN.predict(X_train[:, :10000])\n",
    "\n",
    "print(\"\\n\\ntest predict\\n\")\n",
    "print(\"predict: \\n\", y_hat.shape)\n",
    "\n",
    "print(\"\\nacc: \", np.mean(np.argmax(y_hat, axis=0) == y_train[:10000]))\n",
    "print(\"\\t/\\ should be about .1\")\n",
    "\n",
    "print(\"\\n\\n\\n\", np.row_stack( (y_hat[:, :5], np.sum( y_hat[:, :5], axis=0)) ) )  # last row = 1  => :-)\n",
    "print(\"\\t/\\ should all add to 1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### check the distribution on the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "y_ [ 0.086485    0.1161585   0.11406657  0.11236898  0.09912854  0.086485\n",
      "  0.10002502  0.086485    0.10231219  0.09648521]\n",
      "1\n",
      "0.116158496776\n",
      "p: 1\n",
      "mean: [ 0.1  0.1  0.1 ...,  0.1  0.1  0.1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([    7.,    79.,     0.,   868.,  2096.,     0.,  5212.,     0.,\n",
       "         1705.,    33.]),\n",
       " array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1112c0470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEJCAYAAACOr7BbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSJK6DAhJUpcBIUnqMiAkSV3/D0VHceyRK6dZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1111db278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_hat.shape\n",
    "p = np.argmax(y_hat, axis = 0)\n",
    "print(p.shape)\n",
    "print(\"y_\", y_hat[:, 1000])\n",
    "\n",
    "print(np.argmax(y_hat[:, 1000]))\n",
    "print(np.max(y_hat[:, 1000]))\n",
    "\n",
    "print(\"p:\", p[1000])\n",
    "plt.hist(p)\n",
    "\n",
    "print(\"mean:\", np.mean(y_hat, axis=1))\n",
    "prob_mass = np.sum(y_hat, axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(prob_mass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### tests for backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test cost\n",
      "cost: 2.31 dAL: -10.1891639758\n"
     ]
    }
   ],
   "source": [
    "print(\"test cost\")\n",
    "cost, dAL = NN.costFunction(AL, y_train[:10000])\n",
    "print(\"cost: {0:.2f}\".format(cost), \"dAL:\", dAL)\n",
    "\n",
    "\n",
    "# NN.affineBackward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Split train into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# X_train = ...\n",
    "# y_train = ...\n",
    "\n",
    "# X_val   \n",
    "# y_val1\n",
    "\n",
    "idx = np.random.permutation(len(y_train))\n",
    "X_all = X_train[:, idx]\n",
    "y_all = y_train[idx]\n",
    "\n",
    "m_train = int(len(y_all)*0.9);\n",
    "m_val   = len(y_all)-m_train\n",
    "\n",
    "X       = X_all[:, :m_train]\n",
    "y       = y_all[:m_train]\n",
    "\n",
    "X_val   = X_all[:, m_train:]\n",
    "y_val   = y_all[m_train:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(X_all.shape)\n",
    "print(y_all.shape)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(y)\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 1\n",
    "\n",
    "#### Simple fully-connected deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layer_dimensions = [X_train.shape[0], 1, 10]  # including the input and output layers\n",
    "NN = NeuralNetwork(layer_dimensions)\n",
    "NN.train(X_train, y_train, iters=10, alpha=0, batch_size=128, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_predicted = NN.predict(X_test)\n",
    "save_predictions('ans1-uni', y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# test if your numpy file has been saved correctly\n",
    "loaded_y = np.load('ans1-uni.npy')\n",
    "print(loaded_y.shape)\n",
    "loaded_y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 2: Regularizing the neural network\n",
    "#### Add dropout and L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "NN2 = NeuralNetwork(layer_dimensions, drop_prob=0, reg_lambda=0)\n",
    "NN2.train(X_train, y_train, iters=1000, alpha=0.00001, batch_size=1000, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_predicted2 = NN2.predict(X)\n",
    "save_predictions(y_predicted, 'ans2-uni')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
